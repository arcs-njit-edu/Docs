{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"High Performance Computing (HPC)","text":"<p>Welcome to HPC at New Jersey Institute of Technology (NJIT).</p> <ul> <li> <p> NJIT provides High Performance Computing resources to support scientific computing for faculty and students. These resources include CPU nodes, GPU nodes, parallel storage, high speed, low latency Infiniband networking and a fully optimized scientific software stack.</p> </li> <li> <p> Click here for a virtual tour of the Data Center! </p> </li> </ul>"},{"location":"#hpc-latest-news","title":"HPC latest News!","text":"<ul> <li> <p> Wulver Scheduled Maintenance</p> <p>Wulver will be out of service for maintenance once a month for updates, repairs, and upgrades.  The schedule is 9 a.m. to 9 p.m. the second Tuesday of every month.  During the maintenance period, the logins will be disabled and the jobs that do not end before the maintenance window begins will be held until the maintenance is complete and the cluster is returned to production.   For example, if you submit a job the day before maintenance, your job will enter a pending state (you will see job status <code>PD</code> when using <code>squeue -u $LOGNAME</code>). You can either adjust the walltime or wait until maintenance ends. Please stay informed about maintenance updates at Cluster Maintenance Updates and News.</p> </li> <li> <p> Open Office Hours</p> <p>This spring semester, we are offering drop-in office hours every Monday and Wednesday from 2:00 to 4:00 p.m starting January 21 in a New Location. Stop by to meet with our student consultants and ask any questions you have about using HPC resources. There's no need to create a ticket in advance; if follow-up is needed, the student consultants will open a ticket on your behalf, and you'll receive further instructions. </p> <ul> <li>Date: Every Monday and Wednesday</li> <li>Location: GITC 5302N</li> <li>Time: 2:00 PM - 4:00 PM</li> </ul> </li> </ul> <ul> <li> <p> HPC Spring Events, 2026</p> <p>Check out our event schedule for spring here! If you have suggestions for webinar topics, please feel free to contact us at hpc@njit.edu.</p> </li> <li> <p> Monthly HPC User Meeting</p> <p>We are currently offering a new monthly event for HPC researchers at NJIT: The HPC Monthly User Meeting. This event is open to all NJIT students, faculty, and staff who use or are interested in NJIT's HPC resources and services. No prior registration is required.</p> <ul> <li>Date: Check the spring schudule.</li> </ul> </li> </ul>"},{"location":"#hpc-highlights","title":"HPC Highlights!","text":"<p> Policies See our updated Policies for cluster resource allocation and investment. </p> <p> Getting started with Wulver If you are new to Wulver, and want to know how to get started, visit Wulver Quickstart.</p> <p> Cluster Updates To see the latest updates on NJIT cluster, please visit Cluster Maintenance Updates and News.</p> <p> Software Modules See Software Modules for list of software packages installed on our cluster.</p> <p> Running jobs on Wulver See Running jobs for differenet partitions, QoS and sample jobs scripts.</p> <p> HPC Events Check the upcoming events hosted by HPC team and register from HPC Events.</p> <p> HPC Trainings and Webinars Make sure to stay up-to-date with the latest webinar sessions on HPC by visiting the HPC Training.</p> <p> FAQs For any queries regarding the usage of our cluster, please visit the FAQs which are organized by topic.</p> <p> Contact Us To create a ticket or request for software installation, visit Contact Us.</p>"},{"location":"Courses/","title":"HPC Resources for Teaching &amp; Coursework","text":""},{"location":"Courses/#introduction","title":"Introduction","text":"<p>Instructors can utilize High-Performance Computing (HPC) resources for academic courses. Whether faculty are planning a course that involves computationally intensive tasks or introducing students to parallel computing concepts, the HPC environment can offer valuable resources.</p>"},{"location":"Courses/#course-request","title":"Course Request","text":"<p>To request HPC resources for your course, submit the following information to this    HPC Course Request Form </p>"},{"location":"Courses/#name-of-course","title":"Name of Course:","text":"<p>[Please provide the name of the course and a short description]</p>"},{"location":"Courses/#is-this-one-section-of-a-larger-course","title":"Is this one section of a larger course?","text":"<p>[Yes or No]</p>"},{"location":"Courses/#estimated-number-of-students","title":"Estimated Number of Students:","text":"<p>[Enter the estimated number of students for the course]</p>"},{"location":"Courses/#activities-on-hpc","title":"Activities on HPC:","text":"<p>[Describe the specific activities that will involve the use of HPC resources. For example, simulations, data analysis, modeling, etc.]</p>"},{"location":"Courses/#software-needed","title":"Software Needed:","text":"<p>[Specify the software required for the course. Check the list of software installed on Wulver in Software. Include any specific versions or software not already available on the HPC cluster] </p> <p>Notice Period</p> <p>A minimum of 30 days' notice is required for requesting specific software installations or substantial resource allocations.</p>"},{"location":"Courses/#when-to-use-hpc-in-a-course","title":"When to Use HPC in a Course","text":"<p>HPC resources are good for the courses if they satisfy the following requirements.</p> <ul> <li>Simulations and Modeling: Perform complex simulations and modeling exercises that require significant computational power.</li> <li>Data Analysis: Conduct large-scale data analysis projects, exploring real-world datasets with efficiency.</li> <li>Parallel Computing: Teach parallel computing concepts and applications by leveraging the cluster's parallel processing capabilities.</li> <li>Optimization Problems: Solve optimization problems that benefit from parallel processing and distributed computing.</li> <li>Scientific Research Projects: Enable students to work on scientific research projects that demand high-performance computing resources.</li> </ul>"},{"location":"Courses/#hpc-introduction-for-courses","title":"HPC Introduction for Courses","text":"<p>The HPC Facilitator is available to provide an introduction to High-Performance Computing. This introduction can be conducted in person or online based on the preferences and requirements of the course. The session covers:</p> <ul> <li>Overview of HPC concepts</li> <li>Accessing and navigating the HPC cluster</li> <li>Basic job submission and monitoring</li> <li>Filesystems on HPC cluster</li> <li>Conclusion</li> </ul> <p>By incorporating HPC resources into your course, you provide students with the opportunity to engage in hands-on, real-world applications of computational concepts. The HPC environment enhances the learning experience and prepares students for challenges in data-driven and computationally intensive fields. For specific requests or to schedule an HPC introduction session, please contact the HPC Facilitator.</p>"},{"location":"Courses/course-job-submission/","title":"Submitting Course Jobs","text":"<p>Submit scripts on Wulver must include specification of partition, account, qos, and time limit. Also the first line in the batch file must be <code>#!/bin/bash -l</code>. </p> <p>Below is the minimal example for a 10-minute test job:</p> <pre><code>#!/bin/bash -l\n#SBATCH --partition=course\n#SBATCH --account=2025-fall-ds-492-kjc59-ls565\n#SBATCH --qos=course\n#SBATCH --time=00:10:00\n</code></pre>"},{"location":"Courses/course-job-submission/#sample-job-scripts","title":"Sample Job scripts","text":"<p>Make sure to replace <code>--account</code> with your assigned course account.</p>"},{"location":"Courses/course-job-submission/#cpu-job-example","title":"CPU Job Example","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=mpi_test_job\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=course\n#SBATCH --account=2025-fall-ds-492-kjc59-ls565\n#SBATCH --qos=course\n#SBATCH --time=00:10:00\n#SBATCH --ntasks=64\n\n# Run application commands\nsrun /apps/testjobs/bin/mpihello\n</code></pre> <ul> <li>Runs an MPI job named mpi_test_job.</li> <li>Uses 64 processes across available nodes.</li> <li>Wall time: 10 minutes.</li> </ul>"},{"location":"Courses/course-job-submission/#gpu-job-example","title":"GPU Job Example","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=test_gpu_job\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=course_gpu\n#SBATCH --account=2025-fall-ds-492-kjc59-ls565\n#SBATCH --qos=course\n#SBATCH --time=00:20:00\n#SBATCH --ntasks=2\n#SBATCH --gres=gpu:a100_10g:1\n\n# Load application environment\nmodule load CUDA\n\n# Run application commands\nnvidia-smi\n</code></pre> <ul> <li>Runs a GPU job named test_gpu_job.</li> <li>Allocates 2 CPUs and single A100 10G MIG GPU.</li> <li>Wall time: 20 minutes.</li> </ul>"},{"location":"Courses/course-job-submission/#limitation-of-gpu-jobs","title":"Limitation of GPU Jobs","text":"<ul> <li> <p>You cannot request multiple MIG instances in one job.  For example: <code>--gres=gpu:a100_10g:2</code>  This will either cause an error or misinterpretation as a single GPU.</p> </li> <li> <p>Each job should request one GPU per job.  For multiple tasks, use job arrays instead of multiple MIGs.</p> </li> </ul>"},{"location":"Courses/course-job-submission/#interactive-jobs","title":"Interactive jobs","text":"<p>You can also start an interactive session instead of a batch job.</p> <pre><code>interactive -a ACCOUNT -q QOS -p PARTITION -j JOB_TYPE\n</code></pre> <p>Parameters:</p> <ul> <li><code>-a ACCOUNT</code>  \u2192 Your assigned course account</li> <li><code>-q course</code>   \u2192 QoS for course jobs</li> <li><code>-p course</code>   \u2192 Partition (use course or course_gpu)</li> <li><code>-j JOB_TYPE</code> \u2192 Type of job (e.g., cpu or gpu)</li> </ul> <p>Example</p> <pre><code>interactive -a 2025-fall-ds-492-kjc59-ls565 -q course -p course\n</code></pre> <p>This command launches a temporary compute session for hands-on work or testing.</p> <p>Learn More About Job Submission</p> <p>For more detailed examples and advanced options, visit the Running Jobs page.</p>"},{"location":"Courses/course-resource-config/","title":"Course Resource Configuration","text":""},{"location":"Courses/course-resource-config/#overview","title":"Overview","text":"<p>Before submitting a job on Wulver, it\u2019s important to understand how resources such as cores, memory, and GPUs are allocated for course users. Each job\u2019s runtime and Service Unit (SU) charge depend on how many nodes and resources are requested.</p> <p></p>"},{"location":"Courses/course-resource-config/#partition-use-partition","title":"Partition (Use <code>--partition</code>)","text":"<p>Course-related jobs must be submitted to one of the course partitions listed below:</p> Partition Nodes Cores per Node GPU Memory per Node SU Charge per Hour <code>--\u200bpartition\u200b=\u200bcourse</code> 64 32\u201348 NA 375 GB <code>MAX(CPUs, Memory/4G) SU</code> <code>--\u200bpartition\u200b=\u200bcourse_gpu</code> 4 32\u201348 A100 10G MIG 375 GB <code>2 + MAX(CPUs, Memory/4G) SU</code> <p>Courses that require GPU computing are assigned NVIDIA A100 10G MIG units for GPU-enabled coursework.</p>"},{"location":"Courses/course-resource-config/#service-units","title":"Service Units","text":"<p>Each user is allocated 2500 Service Units (SUs) in their account for the duration of their course. SUs are consumed based on how many cores, memory, and GPUs your job requests and how long it runs.</p>"},{"location":"Courses/course-resource-config/#su-calculation-examples","title":"SU Calculation Examples","text":"<p>Example 1 \u2013 CPU-only job  20 cores for 8 hours (no <code>--mem</code> specified, so default 4G per core will be allocated)  <pre><code>SU = MAX(CPUs, Memory/4G) x Hours \nSU = (20, 4/4) \u00d7 8 = 160  \n</code></pre></p> <p>Example 2 \u2013 GPU job  20 cores + 1 GPU for 8 hours with <code>--mem=128G</code> <pre><code>SU = [2 + MAX(CPUs, Memory/4G)] x Hours \nSU = [2 + MAX(20, 128/4)] \u00d7 8 \nSU = (2 + 32) \u00d7 8 = 34 \u00d7 8 = 272\n</code></pre></p> <p>To make the most of your SUs, avoid overestimating cores or memory. Over-allocation wastes SUs and delays scheduling.</p> <p>Please visit here for more info on Service Units</p>"},{"location":"Courses/course-resource-config/#account","title":"Account","text":"<p>Every course is assigned a dedicated account for SU billing. You must specify this account in all SLURM job scripts using the <code>--account</code> flag.</p> <p>Account Format <pre><code>&lt;semester&gt;-&lt;course&gt;-&lt;instructor_ucid&gt;-&lt;student_ucid&gt;\n</code></pre></p> <p>For example: <pre><code>#SBATCH --account=2025-fall-ds-492-kjc59-ls565\n\n# 2025-fall \u2192 Semester\n# ds-492 \u2192 Course code\n# kjc59 \u2192 Instructor\u2019s UCID\n# ls565 \u2192 Student\u2019s UCID\n</code></pre></p> <p>Note</p> <p>Each user\u2019s specific account information will be emailed to them when HPC resources for that course is allocated. You can also view all accounts assigned to you using the <code>quota_info</code> command.</p>"},{"location":"Courses/course-resource-config/#important-notes","title":"Important Notes:","text":"<ul> <li>Your UCID and password are used to log in to the cluster.</li> <li>Your account name is used for SU tracking and billing.</li> <li>If you are enrolled in multiple courses or research projects, ensure that you submit jobs using the correct account to avoid suspension or access issues.</li> <li>Use the <code>quota_info</code> command to view all accounts assigned to you.</li> </ul>"},{"location":"Courses/course-resource-config/#priority-use-qos","title":"Priority (Use <code>--qos</code>)","text":"<p>All course-related jobs must include the <code>--qos=course</code> flag. This ensures fair scheduling and appropriate priority for classwork jobs.</p> Qos Purpose Wall time limit (hours) Valid Users <code>--\u200bqos\u200b=\u200bcourse</code> For all course-related jobs 72 Course users only <p>Example: <pre><code>#SBATCH --qos=course\n</code></pre></p>"},{"location":"Courses/course-resource-config/#course-directory","title":"Course Directory","text":"<p>Each student is assigned a dedicated course directory, and all work must be done inside it, including job scripts and outputs.</p> <p>Directory Format <pre><code>/course/&lt;year&gt;/&lt;semester&gt;/&lt;course&gt;/&lt;instructor_ucid&gt;/&lt;student_ucid&gt;\n</code></pre></p> <p>Example: <pre><code>/course/2025/fall/ds/492/kjc59/ls565\n</code></pre></p> <p>To switch to your course directory after logging in: <pre><code>cd /course/2025/fall/ds/492/kjc59/ls565\n</code></pre></p> <p>Or add it to your <code>.bash_profile</code> for automatic navigation: <pre><code>echo 'cd /course/2025/fall/ds/492/kjc59/ls565' &gt;&gt; ~/.bash_profile\n</code></pre></p> <p>Note</p> <p>Your specific course directory path will also be emailed to you along with account information when resources are allocated.</p>"},{"location":"HPC_Events_and_Workshops/","title":"HPC Events","text":""},{"location":"HPC_Events_and_Workshops/#2026-spring","title":"2026 Spring","text":"<p>Please check our workshop schedule for this spring season. Expand each section to view more details about the event. For webinars, the links will be sent to your email once you register. The links to slides and recordings will be updated after each webinar. For the HPC User Meeting, users are encouraged to register using the form provided in the registration link; however, registration is not mandatory. If you forget or miss registering, you are still welcome to stop by the location listed in the schedule below.</p> Topic Date RegistrationLink Location Recording Slides Instructor/Facilitator Intro to Wulver: HPC Resources &amp; AllocationsThis virtual session will provide essential information about the Wulver cluster, how to get an account, and allocation details, accessing installed software. January 282:30pm - 3:30pm Online - - Abhishek Mukherjee COMPLECS: Intermediate LinuxLinux command line interface (CLI) skills are essential for advanced cyberinfrastructure (CI). This session covers filesystem hierarchy, permissions, links, wildcards, finding files, environment variables, modules, config files, aliases, history &amp; Bash scripting tips. January 292:00pm - 3:30pm Online - - External Event,hosted by SDSC Intro to Wulver: Job Scheduler &amp; Running Jobs This webinar will introduce researchers, scientists, and HPC users to the fundamentals of the SLURM (Simple Linux Utility for Resource Management) workload manager. This virtual session will provide the information on effectively utilizing HPC resources through SLURM.  February 42:30pm - 3:30pm Online - - Abhishek Mukherjee HPC User Meeting - HPC UpdatesThe details will be updated soon February 172:30pm - 3:30pm Will be updated soon Will be updated soon In-Person Event - Hui(Julia) Zhao HPC User MeetingThe details will be updated soon. March 242:30pm - 3:30pm Will be updated soon Will be updated soon In-Person Event - Hui(Julia) Zhao Introduction to ContainersThis webinar will provide an introductory understanding of container technology and its advantages in high-performance computing environments April 152:30pm - 3:30pm Will be updated soon Online - - Hui(Julia) Zhao HPC User Meeting - Containers Hands OnJoin us for this in-person and virtual session to gain the hands on experience with containers. April 212:30pm - 3:30pm Will be updated soon Will be updated soon - - Hui(Julia) Zhao <p>Archived Workshops</p> <p>Click here to review our past workshops!</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/","title":"HPC Education and Training","text":"<p>NJIT HPC provides practical training in high performance computing for students and researchers at various levels of expertise. HPC training for research professionals aims to enhance their capabilities in utilizing high-performance computing, data-intensive computing, and data analytics within their respective research fields. </p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#2025-fall","title":"2025 Fall","text":""},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#intro-to-wulver-resources-hpc","title":"Intro to Wulver: Resources &amp; HPC","text":"<p>This webinar provides essential information about the Wulver cluster, how to get an account, and allocation details, accessing installed software.</p> <p>Key Highlights:</p> <ul> <li>Introduction to HPC (High Performance Computing)</li> <li>Hardware and architecture of Wulver </li> <li>Guidance on how to obtain an account and login to the cluster </li> <li>Data Storage systems </li> <li>Understanding allocations to utilize the shared resources</li> </ul> <p></p> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#intro-to-wulver-job-scheduler-submitting-jobs","title":"Intro to Wulver: Job Scheduler &amp; Submitting Jobs","text":"<p>This webinar provides the basic information on running jobs , how to run batch processing, and submit and manage the Slurm jobs.</p> <p>Key Highlights:</p> <ul> <li>Access the software on Wulver</li> <li>Batch Processing</li> <li>Manage Slurm Jobs</li> <li>Troubleshooting Common Issues</li> <li>SlurmInteractive Jobs and Use GUI Apps</li> </ul> <p></p> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#intro-to-wulver-focus-on-job-efficiency","title":"Intro to Wulver: Focus on Job Efficiency","text":"<p>This webinar provides more in-depth features of SLURM, how to run dependency, array jobs to run efficiently on the cluster.</p> <p>Key Highlights:</p> <ul> <li>Sbatch : Some Examples</li> <li>salloc command</li> <li>Job Dependencies</li> <li>Job Arrays</li> <li>Checkpointing</li> </ul> <p></p> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#conda-for-shared-environments","title":"Conda for Shared Environments","text":"<p>This webinar provides an introductory understanding of using Python for HPC and effectively managing their Python environments.</p> <p>Key Highlights:</p> <ul> <li>Access Python on Wulver</li> <li>Introduction to Conda environments</li> <li>Install, uninstall and upgrade packages</li> <li>Best Practices for managing conda environments</li> <li>Common Python libraries for scientific computing</li> </ul> <p></p> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/#hpc-user-meeting-introduction-to-mig","title":"HPC User Meeting - Introduction to MIG","text":"<p>This in-person and virtual session provide an introductory understanding of using Multi-Instance GPUs (MIGs) on Wulver.</p> <p>Key Highlights:</p> <ul> <li>What is MIG?</li> <li>Why MIG on Wulver?</li> <li>MIG Configuration Example</li> <li>Submitting Jobs (<code>srun</code> &amp; <code>sbatch</code>)</li> <li>New Billing Model</li> </ul> <p></p> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/","title":"Archived HPC Training Recordings","text":""},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#2025-spring","title":"2025 Spring","text":""},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#open-ondemand-on-wulver","title":"Open OnDemand on Wulver","text":"<p>This webinar will introduce NJIT\u2019s Open OnDemand portal, a browser-based gateway to the Wulver cluster and shared storage. With a focus on streamlining your HPC workflows, you will explore common scenarios and tasks through interactive demos. You will gain a detailed understanding of how to manage your files on the cluster, run interactive applications like Jupyter Notebook and RStudio, launch a full Linux desktop environment in your browser, and submit and monitor SLURM jobs. Additionally, you'll learn how to track resource usage and optimize your job performance for efficient computing on the Wulver cluster.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#key-highlights","title":"Key Highlights:","text":"<ul> <li>Explore and manage your files on the cluster</li> <li>Run interactive tools like Jupyter Notebook and RStudio</li> <li>Launch a full Linux desktop environment in your browser</li> <li>Submit and monitor SLURM jobs</li> <li>Track resource usage and optimize job performance</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#introduction-to-linux","title":"Introduction to Linux","text":"<p>This is the fourth webinar of the 2025 Spring semester, introducing the basics of the Linux operating system. This session is designed to help new users become familiar with Linux, an essential skill for working in High-Performance Computing (HPC) environments.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#key-highlights_1","title":"Key Highlights:","text":"<ul> <li>Basics of the Linux operating system </li> <li>Common commands and file system navigation </li> <li>Managing files, directories, and permissions </li> <li>Introduction to shell scripting for automation </li> <li>Connecting to remote systems and working with HPC cluster</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#python-and-conda-environments-in-hpc-from-basics-to-best-practices","title":"Python and Conda Environments in HPC: From Basics to Best Practices","text":"<p>This is the third webinar of the 2025 Spring semester, focusing on an introductory understanding of using Python for HPC and effectively managing their Python environments using Conda</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#key-highlights_2","title":"Key Highlights:","text":"<ul> <li>Learn how to manage Python environments using Conda.</li> <li>How to create Conda environments in different locations and install Python packages.</li> <li>Become familiar with common tools and libraries for scientific computing in Python.</li> <li>Import Conda environment to a different location.</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#introduction-to-wulver-accessing-system-running-jobs","title":"Introduction to Wulver: Accessing System &amp; Running Jobs","text":"<p>This is the second webinar of the 2025 Spring semester, focusing on job submission, monitoring, and management on Wulver. This webinar also provides common tips for troubleshooting issues that users may encounter during job submission.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#key-highlights_3","title":"Key Highlights:","text":"<ul> <li>How to Access HPC Software</li> <li>Introduction to SLURM and Its Role in HPC Environments </li> <li>Manage Slurm Jobs </li> <li>Troubleshooting Common Issues </li> <li>Slurm Interactive Jobs and Use GUI Apps</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#introduction-to-wulver-getting-started","title":"Introduction to Wulver: Getting Started","text":"<p>This is the first webinar of the 2025 Spring semester introducing the NJIT HPC environment. This webinar provides basic information about our new High-Performance Computing (HPC) research cluster, Wulver.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#key-highlights_4","title":"Key Highlights:","text":"<ul> <li>Introduction to HPC (High Performance Computing)</li> <li>Hardware and architecture of Wulver</li> <li>Guidance on how to obtain an account and login to the cluster </li> <li>Data Storage systems</li> <li>Understanding allocations to utilize the shared resources</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#2024-fall","title":"2024 Fall","text":""},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#job-arrays-and-advanced-submission-techniques-for-hpc","title":"Job Arrays and Advanced Submission Techniques for HPC","text":"<p>This is the final in a series of three webinars in the fall semester. designed to introduce researchers, scientists, and HPC users to the fundamentals of the containers. This session aims to provide useful information on submitting SLURM jobs efficiently by covering job arrays, job dependencies, checkpointing, and addressing common SLURM job issues.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#key-highlights_5","title":"Key Highlights:","text":"<ul> <li>Understanding the concept and benefits of job arrays</li> <li>Syntax for submitting and managing job arrays</li> <li>Best practices for efficient array job design</li> <li>Dependency chains and complex workflows</li> <li>Resource optimization strategies</li> <li>Using SLURM's advanced options for improved job control</li> <li>Checkpointing the jobs and use of 3<sup>rd</sup> party checkpointing tool</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#introduction-to-containers-on-wulver","title":"Introduction to Containers on Wulver","text":"<p>This is the second in a series of three webinars in the fall semester, designed to introduce researchers, scientists, and HPC users to the fundamentals of the containers. Attendees will learn the fundamentals of Singularity, including installation, basic commands, and workflow, as well as how to create and build containers using definition files and existing Docker images. The training will cover executing containerized applications on HPC clusters and integrating with job schedulers like SLURM, while also addressing security considerations and performance optimization techniques.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#key-highlights_6","title":"Key Highlights:","text":"<ul> <li>Introduction to containers and its role in HPC environments</li> <li>Fundamentals of Singularity, including installation, basic commands, and workflow</li> <li>Create and build containers using definition files and existing Docker images</li> <li>How to execute containerized applications on HPC clusters</li> <li>Use Containers via SLURM</li> <li>Performance optimization techniques</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#slurm-batch-system-basics","title":"SLURM Batch System Basics","text":"<p>This is the first in a series of three webinars in the fall semester. designed to introduce researchers, scientists, and HPC users to the fundamentals of the SLURM (Simple Linux Utility for Resource Management) workload manager. This virtual  session will equip you with essential skills to effectively utilize HPC resources through SLURM.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#key-highlights_7","title":"Key Highlights:","text":"<ul> <li>Introduction to SLURM and its role in HPC environments </li> <li>Basic SLURM commands for job submission, monitoring, and management </li> <li>How to write effective job scripts for various application types </li> <li>Understanding SLURM partitions, quality of service, and job priorities </li> <li>Best practices for resource requests and job optimization </li> <li>Troubleshooting common issues in job submission and execution</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#2024-spring","title":"2024 Spring","text":"<p>Since Wulver is quite different from the older cluster Lochness, the HPC training programs are designed to guide both new and existing users on how to use the new cluster. The following trainings provide the basic information on  </p> <ul> <li>Introduction to HPC </li> <li>Performance Optimization</li> <li>Job Submission and Management</li> <li>Managing Conda Environment </li> </ul> <p>If you still have any questions on HPC usage, please contact the HPC Facilitator.</p> <ul> <li> </li> </ul> <ul> <li> </li> </ul> <ul> <li> </li> </ul>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#getting-started-on-wulver-session-i","title":"Getting Started on Wulver: Session I","text":"<p>This is the first in a series of three webinars introducing the NJIT HPC environment. This webinar provided the basic information in learning more about our new High Performance Computing (HPC) research cluster, Wulver.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#key-highlights_8","title":"Key Highlights:","text":"<ul> <li>HPC concepts</li> <li>Hardware and architecture of the Wulver cluster</li> <li>Guidance on how to obtain an account and receive an allocation to utilize the shared resources. </li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#getting-started-on-wulver-session-ii","title":"Getting Started on Wulver: Session II","text":"<p>This session offered an overview of the environment on the Wulver cluster, including file management, working with the batch system (SLURM), and accessing software. </p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#key-highlights_9","title":"Key Highlights:","text":"<ul> <li>HPC allocations</li> <li>Using SLURM</li> <li>Job submissions</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#introduction-to-python-and-conda","title":"Introduction to Python and Conda","text":"<p>Participants will gain an introductory understanding of using Python for HPC and effectively managing their Python environments using Conda. This knowledge will empower them to leverage the power of Python for their scientific computing needs on HPC systems.</p>"},{"location":"HPC_Events_and_Workshops/Workshop_and_Training_Videos/archived/#key-highlights_10","title":"Key Highlights:","text":"<ul> <li>Learn how to manage Python environments for HPC using Conda.</li> <li>Become familiar with common tools and libraries for scientific computing in Python.</li> </ul> <p> Download Slides</p>"},{"location":"HPC_Events_and_Workshops/archived/","title":"Archived HPC Workshops","text":""},{"location":"HPC_Events_and_Workshops/archived/#2025","title":"2025","text":"FallSummerSpring Topic Date Recording Slides Instructor/Facilitator Intro to Wulver: Resources &amp; HPCThis virtual session will provide essential information about the Wulver cluster, how to get an account, and allocation details, accessing installed software. September 172:30pm - 3:30pm Abhishek Mukherjee HPC User Meeting - MIG &amp; SUsPlease join us for this in-person session, where we will provide information on the new SU policies and how to use MIG. September 24 2:30pm - 3:30pm In-Person Event Hui(Julia) Zhao Intro to Wulver: Job Scheduler &amp; Submitting Jobs This webinar will introduce researchers, scientists, and HPC users to the fundamentals of the SLURM (Simple Linux Utility for Resource Management) workload manager. This virtual session will provide the information on effectively utilizing HPC resources through SLURM.  October 12:30pm - 3:30pm Abhishek Mukherjee Intro to Wulver: Focus on Job EfficiencyThis virtual session will provide an overview of best practices for running jobs efficiently on the cluster. Topics will include the use of job arrays, dependency jobs, and advanced tools to monitor and analyze job efficiency, helping users optimize their workflows and resource usage. October 82:30pm - 3:30pm Abhishek Mukherjee Machine Learning and Big DataThis workshop will focus on topics including  big data analytics and machine learning with Spark, and deep learning using Tensorflow. Hands-on exercises are included to give attendees practice with the concepts presented October 14-1511am - 5pm In-Person Event Pittsburgh Supercomputing Center HPC User Meeting - Cluster Tools &amp; MonitoringJoin us for this in-person session to learn the easiest and most user-friendly ways to monitor key statistics, including job status, SU consumption, and account allocation details on Wulver. October 222:30pm - 3:30pm In-Person Event Hui(Julia) Zhao Conda for Shared EnvironmentsThis webinar will provide an introductory understanding of using Python for HPC and effectively managing their Python environments using Conda. This knowledge will empower them to leverage the power of Python for their scientific computing needs on HPC systems. November 52:30pm - 3:30pm Hui(Julia) Zhao HPC User Meeting - Introduction to MIGJoin us for this in-person and virtual session to learn more about Multi-Instance GPUs (MIGs). December 32:30pm - 3:30pm Hui(Julia) Zhao Topic Date Recording/Slides Instructor Machine Learning and Big Data WorkshopThis workshop This workshop focuses on topics including big data analytics and machine learning with Spark, and deep  learning using Tensorflow. July 29-30, 2025 Pittsburgh Supercomputing Center MATLAB Parallel Computing Hands-On Using WulverThis interactive webinar guides participants through practical techniques for accelerating code and workflows using MATLAB\u2019s parallel computing tools. Through live demonstrations and guided examples, this webinar provides a solid understanding of how to parallelize MATLAB code, overcome common challenges, and optimize performance across distributed computing environments. June 12<sup>th</sup> 2025 Check the materials here Evan Cosgrove (MATLAB) Topic Date Recording/Slides Instructor Open OnDemand on WulverProvides information on NJIT\u2019s Open OnDemand portal, a browser-based gateway to the Wulver cluster and shared storage. With a focus on streamlining your HPC workflows, you will explore common scenarios and tasks through interactive demos. This webinar provides a detailed understanding of how to manage your files on the cluster, run interactive applications like Jupyter Notebook and RStudio, launch a full Linux desktop environment in a browser, and submit and monitor SLURM jobs. Additionally, it provides information on how to track resource usage and optimize your job performance for efficient computing on the Wulver cluster. April 30<sup>th</sup> 2025 Hui(Julia) Zhao Parallel Computing with MATLAB: Hands on workshopThis hands-on workshop introduces parallel and distributed computing in MATLAB with a focus on speeding up application codes and offloading computers. By working through common scenarios and workflows using hands-on demos, this webinar provides a detailed understanding of the parallel constructs in MATLAB, their capabilities, and some of the common hurdles that users encounter when using them. April 16<sup>th</sup> 2025 Evan Cosgrove Introduction to Linux This webinar introducing the basics of Linux, essential for working in High-Performance Computing (HPC) environments.  March 26<sup>th</sup> 2025 Abhishek Mukherjee Python and Conda Environments in HPC: From Basics to Best PracticesThis workshop offers a basic concept of using Python for High-Performance Computing (HPC) and effectively managing Python environments with Conda. This webinar empowers participants to leverage the power of Python for their scientific computing needs on HPC systems. Feb 26<sup>th</sup> 2025 Hui(Julia) Zhao Introduction to Wulver: Accessing System &amp; Running JobsThe HPC training event focuses on providing the fundamentals of SLURM (Simple Linux Utility for Resource Management), a workload manager. This virtual session will equip you with the essential skills needed to effectively utilize HPC resources using SLURM. Jan 29<sup>th</sup> 2025 Abhishek Mukherjee Introduction to Wulver: Getting Started This webinar introduces NJIT's HPC environment, Wulver. This virtual session will provide essential information about the Wulver cluster, how to get an account, and allocation details. Jan 22<sup>nd</sup> 2025 Abhishek Mukherjee"},{"location":"HPC_Events_and_Workshops/archived/#2024","title":"2024","text":"FallSummer Topic Date Recording/Slides Instructor Intro to MPI WorkshopThis workshop is intended to give C and Fortran programmers a hands-on introduction to MPI programming. Both days are compact, to accommodate multiple time zones, but packed with useful information and lab exercises. This workshop provides working knowledge of how to write scalable codes using MPI \u2013 the standard programming tool of scalable parallel computing. December 10-11, 2024 Pittsburgh Supercomputing Center Job Arrays and Advanced Submission Techniques for HPCThis session is designed for HPC users who are familiar with basic SLURM commands and are ready to dive into more sophisticated job management techniques. Nov 20<sup>th</sup> 2024 Abhishek Mukherjee Introduction to Containers on Wulver The HPC training event on using Singularity containers provides participants with a comprehensive introduction to container technology and its advantages in high-performance computing environments. This workshop provides the fundamentals of Singularity, including installation, basic commands, and workflow, as well as how to create and build containers using definition files and existing Docker images.  Oct 16<sup>th</sup> 2024 Hui(Julia) Zhao SLURM Batch System BasicsThis workshop introduces researchers, scientists, and HPC users to the fundamentals of the SLURM (Simple Linux Utility for Resource Management) workload manager. This virtual session  provides the information on effectively utilizing HPC resources through SLURM. Sep 18<sup>th</sup> 2024 Abhishek Mukherjee Topic Date Recording/Slides Instructor SLURM Workload Manager WorkshopThis workshop helps in identifying commonalities between previously used resources and schedulers, offering increased understanding and adoption of SLURM job scheduling, resource management, and troubleshooting techniques. August 13-14, 2024 In-person Workshop SchedMD HPC Research SymposiumThe Symposium features a keynote from Anthony Dina, Global Field CTO for Unstructured Data Solutions at Dell Technologies, along with invited talks by Dibakar Datta from NJIT\u2019s Department of Mechanical and Industrial Engineering, and Jose Alvarez from Cambridge Computer Services. It also includes several lightning talks by NJIT researchers showcasing their use of High Performance Computing resources in their work. July 16<sup>th</sup> 2024 In-person - NVIDIA Workshop This workshop provides the information on GPU-accelerated resources to analyze data  July 15<sup>th</sup> 2024 NVIDIA"},{"location":"HPC_Events_and_Workshops/archived/2024/1_slurm/","title":"SLURM Batch System Basics","text":"<p>Join us for an informative webinar designed to introduce researchers, scientists, and HPC users to the fundamentals of the SLURM (Simple Linux Utility for Resource Management) workload manager. This virtual  session will equip you with essential skills to effectively utilize HPC resources through SLURM.</p> <ul> <li>Date: Sep 18<sup>th</sup> 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2024/1_slurm/#topics-covered","title":"Topics Covered","text":"<ul> <li>Introduction to SLURM and its role in HPC environments</li> <li>Basic SLURM commands for job submission, monitoring, and management</li> <li>How to write effective job scripts for various application types</li> <li>Understanding SLURM partitions, quality of service, and job priorities</li> <li>Best practices for resource requests and job optimization</li> <li>Troubleshooting common issues in job submission and execution</li> </ul> <p>Our experienced HPC specialists will guide you through practical examples and provide tips for efficient use of SLURM in your research workflows. Whether you're new to HPC or looking to refine your SLURM skills, this webinar will help you maximize your productivity on SLURM-based clusters.</p>"},{"location":"HPC_Events_and_Workshops/archived/2024/1_slurm/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/archived/2024/2_containers/","title":"Introduction to Containers on Wulver","text":"<p>The HPC training event on using Singularity containers provides participants with a comprehensive introduction to container technology and its advantages in high-performance computing environments.</p> <ul> <li>Date: Oct 16<sup>th</sup> 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2024/2_containers/#topics-covered","title":"Topics Covered","text":"<ul> <li>Introduction to containers and its role in HPC environments</li> <li>Fundamentals of Singularity, including installation, basic commands, and workflow</li> <li>Create and build containers using definition files and existing Docker images</li> <li>How to execute containerized applications on HPC clusters</li> <li>Use Containers via SLURM</li> <li>Performance optimization techniques</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2024/2_containers/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/archived/2024/3_slurm_advanced/","title":"Job Arrays and Advanced Submission Techniques for HPC","text":"<p>Elevate your High-Performance Computing skills with our advanced SLURM webinar! This session is designed for HPC users who are familiar with basic SLURM commands and are ready to dive into more sophisticated job management techniques.</p> <ul> <li>Date: Nov 20<sup>th</sup> 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2024/3_slurm_advanced/#topics-covered","title":"Topics Covered","text":""},{"location":"HPC_Events_and_Workshops/archived/2024/3_slurm_advanced/#job-arrays","title":"Job Arrays","text":"<ul> <li>Understanding the concept and benefits of job arrays</li> <li>Syntax for submitting and managing job arrays</li> <li>Best practices for efficient array job design</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2024/3_slurm_advanced/#advanced-job-submission-techniques","title":"Advanced Job Submission Techniques","text":"<ul> <li>Dependency chains and complex workflows</li> <li>Resource optimization strategies</li> <li>Using SLURM's advanced options for improved job control</li> <li>Techniques for balancing resource requests and job efficiency</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2024/3_slurm_advanced/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/archived/2024/4_nvidia/","title":"NVIDIA Workshop","text":""},{"location":"HPC_Events_and_Workshops/archived/2024/4_nvidia/#fundamentals-of-accelerated-data-science","title":"Fundamentals of Accelerated Data Science","text":"<p>Learn to use GPU-accelerated resources to analyze data. This is an intermediate level workshop that is intended for those who have some familiarity with Python, especially NumPy and SciPy libraries.</p> <ul> <li>Date: July 15<sup>th</sup> 2024</li> <li>Location: GITC 3700</li> <li>Time: 9 AM - 5 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2024/4_nvidia/#schedule","title":"Schedule","text":"Time Topic 9:00 AM - 9:15 AM Introduction 9:15 AM - 11:30 AM GPU-Accelerated Data Manipulation 11:30 AM - 12:00 PM GPU-Accelerated Machine Learning 12:00 PM - 1:00PM Lunch 1:00 PM - 2:30 PM GPU-Accelerated Machine Learning (contd) 2:45 PM - 4:45 PM Project: Data Analysis to Save the UK 4:45 PM - 5:00 PM Assessment and Q&amp;A <p>Coffee and Lunch will be provided. See more detail about the workshop here.</p> <p>Registration is now closed.</p>"},{"location":"HPC_Events_and_Workshops/archived/2024/5_symposium/","title":"HPC Research Symposium","text":""},{"location":"HPC_Events_and_Workshops/archived/2024/5_symposium/#hpc-research-symposium","title":"HPC Research Symposium","text":"<p>This past year has been transformative for HPC research at NJIT. The introduction of our new shared HPC cluster, Wulver, has expanded our computational capacity and made research into vital areas more accessible to our faculty. The Advanced Research Computing Services group of Information Services and Technology, in collaboration with Dell Technologies, invites you to a symposium on July 16, 2024, to celebrate the launch of Wulver.</p> <p>The Symposium will feature a keynote from Anthony Dina, Global Field CTO for Unstructured Data Solutions at Dell Technologies, and invited speakers from NJIT, Dibakar Datta from the Department of Mechanical and Industrial Engineering, and Cambridge Computer Services, Jose Alvarez. The Symposium will also feature several lightning talks from NJIT researchers highlighting the use of High Performance Computing resources in their research.</p> <p>Please join us to learn how our researchers use HPC resources and connect with the NJIT HPC community.</p> <ul> <li>Date: July 16<sup>th</sup>, 2024</li> <li>Location: Campus Center Atrium</li> <li>Time: 9AM - 5PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2024/5_symposium/#agenda","title":"Agenda","text":"Time Session Title 9:00 - 9:15 Welcome Remarks Ed Wozencroft, Vice President for Digital Strategy &amp; CIO 9:15 - 9:35 Research Computing @ NJIT Overview of Research Computing Services Gedaliah Wolosh, Director High Performance Research Computing 9:40 - 10:30 Keynote It\u2019s Time Research Behaves More Like Formula 1 Anthony Dina, Global Field CTO for the Unstructured Data Solutions at Dell Technologies 10:30 - 10:40 Break 10:40 - 11:20 Invited Speaker Electro-Chemo-Mechanics of Multiscale Active Materials for Next-Generation Energy Storage Dibakar Datta, Associate Professor, Mechanical &amp; Industrial Engineering 11:20 - 12:00 Lightning Talks I Hemodynamics and Cancer Cell Transport in a Tortuous in vivo Microvessel Ali Kazempour, Peter Balogh Research Group, Mechanical &amp; Industrial Engineering Running Two-phase Flows on Wulver: Introduction to Basilisk Matthew Cho, Shahriar Afkhami Research Group, Mathematical Sciences Temporal Super-Resolution of Solar Images with Generative AI Jialing Li, Jason Wang Research Group, Computer Science Numerical study of Thermo-Marangoni flow induced by a warm plate Shivam Verma, Pushpendra Singh Research Group, Mechanical &amp; Industrial Engineering 12:10 - 13:00 Lunch 13:00 - 13:30 Invited Speaker Introduction to Grace Hopper and ARM Technology in Higher Education Jose Alvarez, Vice President Research Computing HPC/AI, Cambridge Computer Services 13:30- 14:20 Lightning Talks II Inference of Nullability Annotations using Machine Learning Kazi Amanul Islam Siddiqui, Martin Kellogg Research Group, Computer Science Deep Learning for Spatial Image Super-Resolution of Solar Observations Chunhui Xu, Jason Wang Research Group, Computer Science Volume Integral Method for Electromagnetic Equations Matthew Cassini, Thi Phong Nguyen Research Group, Mathematical Sciences Enhancing Region-based Image Captioning with Contextual Feature Exploitation Al Shahriar Rubel, Fadi Deek Research Group, Informatics Instability between the two-layer Poiseuille flow with the VOF method Nastaran Rezaei, Shahriar Afkhami Research Group, Mathematical Sciences 14:20 - 14:30 Break 14:30 - 15:00 Research Computing @ NJIT Introducing Open OnDemand web portal: New Technologies on Wulver Kate Cahill, Associate Director High Performance Research Computing 15:00 - 15:50 Lightning Talks III Red Blood Cell Modeling Reveals 3D Angiogenic Wall Shear Stress Patterns Mir Md Nasim Hossain, Peter Balogh Research Group, Mechanical &amp; Industrial Engineering Understanding and Forecasting Space Weather with Artificial Intelligence Hongyang Zhang, Jason Wang Research Group, Computer Science Entropic pressure on fluctuating solid membranes Rubayet Hassan, Fatemeh Ahmadpoor Research Group, Mechanical &amp; Industrial Engineering Investigation of PFAS Adsorption on Functionalized COF-300 Daniel Mottern, Joshua Young Research Group, Chemical &amp; Materials Engineering Large Language models (LLM) for hardware Deepak Vungarala, Shaahin Angizi Research Group, Electrical and Computer Engineering 15:50 - 16:00 Break 16:00 - 17:00 Poster Session Molecular Dynamic study on Ar nano-bubble stability in water Targol Teymourian, Jay Meegoda Research Group, Civil &amp; Environmental Engineering Possible mechanism for sonolytic degradation of PFAS Laura Nwanebu, Jay Meegoda Research Group, Civil &amp; Environmental Engineering Entropic Force Near Fluctuating Surface Rubayet Hassan, Fatemeh Ahmadpoor Research Group, Mechanical &amp; Industrial Engineering DFT investigations of the enantioselective phase-transfer-catalyzed aza-Michael cyclization of ureas Diana Marlen Castaneda Bagatella, Pier Champagne Research Group, Chemistry and Environmental Science"},{"location":"HPC_Events_and_Workshops/archived/2024/6_slurm_workshop/","title":"SLURM Workload Manager Workshop","text":"<p>Advanced Research Computing Services in collaboration with SchedMD is pleased to announce a two-day workshop on SLURM Workload Manager on August 13-14, 2024. This immersive 2-day experience will take you through comprehensive technical scenarios with lectures, demos, and workshop lab environments. The Slurm trainer will assist in identifying commonalities between previously used resources and schedulers, offering increased understanding and adoption of SLURM job scheduling, resource management, and troubleshooting techniques.</p> <p>Registration is now closed.</p>"},{"location":"HPC_Events_and_Workshops/archived/2024/7_Intro_to_MPI_Workshop/","title":"Intro to MPI Workshop","text":"<p>NJIT is an in-person, satellite location for a two-day HPC workshop hosted by the Pittsburgh Supercomputing Center (PSC) on December 10 &amp; 11. This is a great introduction to using MPI programming to scale up your computational research. Attendees will leave with a working knowledge of how to write scalable codes using MPI \u2013 the standard programming tool of scalable parallel computing.</p> <p>Registration is now closed.</p> <p>To download the training materials, check MPI Workshop Agenda.</p>"},{"location":"HPC_Events_and_Workshops/archived/2025/1_intro_to_Wulver_I/","title":"Introduction to Wulver: Getting Started","text":"<p>This is the first webinar of the 2025 Spring semester introducing the NJIT HPC environment. This webinar provides basic information about our new High-Performance Computing (HPC) research cluster, Wulver.</p> <ul> <li>Date: Jan 22<sup>nd</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/1_intro_to_Wulver_I/#topics-covered","title":"Topics Covered","text":"<ul> <li>Introduction to HPC (High Performance Computing)</li> <li>Hardware and architecture of Wulver</li> <li>Guidance on how to obtain an account and login to the cluster</li> <li>Understanding allocations to utilize the shared resources</li> </ul> <p>Our experienced HPC specialists will guide you through practical examples and provide tips for proper use of cluster in your research workflows.</p>"},{"location":"HPC_Events_and_Workshops/archived/2025/1_intro_to_Wulver_I/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/archived/2025/2_intro_to_Wulver_II/","title":"Introduction to Wulver: Accessing System &amp; Running Jobs","text":"<p>This is the second webinar of the 2025 Spring semester introducing the NJIT HPC environment. This webinar provided the basic information in learning more about our new High Performance Computing (HPC) research cluster, Wulver.</p> <ul> <li>Date: Jan 29<sup>th</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/2_intro_to_Wulver_II/#topics-covered","title":"Topics Covered","text":"<ul> <li>HPC allocations</li> <li>How to access HPC software</li> <li>Introduction to SLURM and its role in HPC environments</li> <li>Basic SLURM commands for job submission, monitoring, and management</li> <li>Troubleshooting common issues in job submission and execution</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/2_intro_to_Wulver_II/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/archived/2025/3_conda_training/","title":"Python and Conda Environments in HPC: From Basics to Best Practices","text":"<p>This is the third webinar of the 2025 Spring semester introducing the NJIT HPC environment. Participants will gain an introductory understanding of using Python for HPC and effectively managing their Python environments using Conda. This knowledge will empower them to leverage the power of Python for their scientific computing needs on HPC systems.</p> <ul> <li>Date: Feb 26<sup>th</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/3_conda_training/#topics-covered","title":"Topics Covered","text":"<ul> <li>Learn how to manage Python environments using Conda.</li> <li>How to create Conda environments in different locations and install Python packages.</li> <li>Become familiar with common tools and libraries for scientific computing in Python.</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/3_conda_training/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/archived/2025/4_intro_to_linux/","title":"Introduction to Linux","text":"<p>This is the fourth webinar of the 2025 Spring semester, introducing the basics of the Linux operating system. This session is designed to help new users become familiar with Linux, an essential skill for working in High-Performance Computing (HPC) environments.</p> <ul> <li>Date: March 26<sup>th</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/4_intro_to_linux/#topics-covered","title":"Topics Covered","text":"<ul> <li>Basics of the Linux operating system </li> <li>Common commands and file system navigation </li> <li>Managing files, directories, and permissions </li> <li>Introduction to shell scripting for automation </li> <li>Connecting to remote systems and working with HPC cluster</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/4_intro_to_linux/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/archived/2025/5_parallel_computing_with_matlab/","title":"Parallel Computing with MATLAB: Hands on workshop","text":"<p>During this hands-on workshop, we will introduce parallel and distributed computing in MATLAB with a focus on speeding up application codes and offloading computers. By working through common scenarios and workflows using hands-on demos, you will gain a detailed understanding of the parallel constructs in MATLAB, their capabilities, and some of the common hurdles that you'll encounter when using them.</p> <ul> <li>Date: April 16<sup>th</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 1:00 PM - 4:00 PM</li> <li>Hosted by Mathworks - A MATLAB account is required to participate..</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/5_parallel_computing_with_matlab/#topics-covered","title":"Topics Covered","text":"<ul> <li>Multithreading vs multiprocessing</li> <li>When to use parfor vs parfeval constructs</li> <li>Creating data queues for data transfer</li> <li>Leveraging NVIDIA GPUs</li> <li>Parallelizing Simulink models</li> <li>Working with large data</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/5_parallel_computing_with_matlab/#registration","title":"Registration","text":"<p>Registration is now closed.</p>"},{"location":"HPC_Events_and_Workshops/archived/2025/6_intro_to_OnDemand/","title":"Open OnDemand on Wulver","text":"<p>Open OnDemand is a browser-based gateway to NJIT's Wulver cluster and shared storage.</p> <ul> <li>Date: April 30<sup>th</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/6_intro_to_OnDemand/#topics-covered","title":"Topics Covered","text":"<ul> <li>Explore and manage your files on the cluster</li> <li>Run interactive tools like Jupyter Notebook and RStudio</li> <li>Launch a full Linux desktop environment in your browser</li> <li>Submit and monitor SLURM jobs</li> <li>Track resource usage and optimize job performance</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/6_intro_to_OnDemand/#registration","title":"Registration","text":"<p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"HPC_Events_and_Workshops/archived/2025/7_MATLAB_on_Wulver/","title":"HPC Summer Workshop: MATLAB Parallel Computing Hands-On Using Wulver","text":""},{"location":"HPC_Events_and_Workshops/archived/2025/7_MATLAB_on_Wulver/#overview","title":"Overview","text":"<p>Join us for an interactive webinar hosted at NJIT's HPC facility, where MATLAB expert (Evan Cosgrov) will guide participants through practical techniques for accelerating code and workflows using MATLAB\u2019s parallel computing tools. Through live demonstrations and guided examples, you\u2019ll gain a solid understanding of how to parallelize MATLAB code, overcome common challenges, and optimize performance across distributed computing environments. </p> <p>Each participant will have access to the OnDemand Matlab server running on Wulver.</p>"},{"location":"HPC_Events_and_Workshops/archived/2025/7_MATLAB_on_Wulver/#guide-to-access-matlab-via-ondemand","title":"Guide to access Matlab via OnDemand:","text":"<p>Users need use <code>Jupyter Matlab Proxy</code> to use MATLAB on OnDemand. </p>"},{"location":"HPC_Events_and_Workshops/archived/2025/7_MATLAB_on_Wulver/#date-and-location","title":"Date and Location:","text":"<ul> <li>Date: June 12<sup>th</sup> 2025</li> <li>Location: Campus Center 235</li> <li>Time: 1:00 PM - 4.00 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/7_MATLAB_on_Wulver/#topics-covered","title":"Topics Covered","text":"<ul> <li>How to identify bottlenecks in serial MATLAB code and convert them to run in parallel.</li> <li>Practical differences between <code>parfor</code> and <code>parfeval</code>, and how to choose the right one.</li> <li>Creating asynchronous tasks and managing outputs with DataQueue.</li> <li>Running Simulink models in parallel and accelerating simulation tasks.</li> <li>Leveraging GPU resources to boost performance in compute-intensive operations.</li> <li>Working with large datasets using tall arrays and distributed arrays.</li> <li>Best practices for launching and managing MATLAB parallel pools.</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/7_MATLAB_on_Wulver/#registration","title":"Registration","text":"<p>Registration is now closed.</p>"},{"location":"HPC_Events_and_Workshops/archived/2025/8_PSC_Machine_Learning_workshop/","title":"HPC Summer Workshop: PSC Machine Learning and BIG DATA Workshop","text":""},{"location":"HPC_Events_and_Workshops/archived/2025/8_PSC_Machine_Learning_workshop/#overview","title":"Overview","text":"<p>The Pittsburgh Supercomputing Center is pleased to present a Machine Learning and Big Data workshop. This workshop will focus on topics including big data analytics and machine learning with Spark, and deep  learning using Tensorflow. This will be an IN PERSON event hosted by various satellite sites, there WILL NOT be a direct to desktop option for this event. </p>"},{"location":"HPC_Events_and_Workshops/archived/2025/8_PSC_Machine_Learning_workshop/#date-and-location","title":"Date and Location:","text":"<ul> <li>Date: July 29-30, 2025</li> <li>Location: Room 2315A GITC</li> <li>Time: 11:00 AM - 5.00 PM</li> </ul>"},{"location":"HPC_Events_and_Workshops/archived/2025/8_PSC_Machine_Learning_workshop/#topics-covered","title":"Topics Covered","text":"<p>Check details at PSC Machine Learning and BIG DATA Workshop</p>"},{"location":"HPC_Events_and_Workshops/archived/2025/8_PSC_Machine_Learning_workshop/#registration","title":"Registration","text":"<p>Registration is now closed.</p>"},{"location":"MIG/","title":"MIG Overview","text":"<p>MIG (Multi-Instance GPU) is a technology introduced by NVIDIA starting with its Ampere architecture (e.g., A100). It enables a single physical GPU to be partitioned into multiple smaller, isolated GPU instances \u2014 each with dedicated compute cores, memory, cache, and bandwidth. These instances function independently and appear to software as discrete GPUs.</p> <p>This allows multiple users or processes to simultaneously run GPU workloads without interfering with one another, improving resource utilization, reducing wait times in job queues, and increasing throughput in shared computing environments like Wulver.</p>"},{"location":"MIG/#why-mig-on-wulver","title":"Why MIG on Wulver","text":"<p>The plots above show cluster-wide usage of the A100 GPUs over the last 3 months. In short: GPUs were heavily allocated but lightly utilized, which means lots of capacity sat idle behind single jobs.</p> <ul> <li> <p>Allocated vs. used: GPUs were reserved ~85\u201395% of the time, while average compute utilization was ~25\u201350% and memory utilization stayed mostly under 20%.</p> </li> <li> <p>Impact: Long queue times and poor overall throughput when full GPUs are booked for workloads that only need a fraction of the device.</p> </li> <li> <p>MIG rationale: Partitioning A100s into 10/20/40 GB slices lets multiple right-sized jobs run concurrently, improving SU efficiency and time-to-results without requiring more hardware.</p> </li> </ul>"},{"location":"MIG/#mig-implementation-in-wulver","title":"MIG Implementation in Wulver","text":"<p>MIG is implemented on selected NVIDIA A100 80GB GPUs. Wulver currently supports the following MIG configurations:</p> <ul> <li>10gb</li> <li>20gb</li> <li>40gb</li> </ul> <p>These profiles correspond to different partitions of compute and memory resources from the A100 80GB GPU. You can view a full comparison in the Profile Comparison section.</p> <p>Each profile maps to a Service Unit (SU) usage factor that reflects its computational weight \u2014 ranging from 2 SU/hour for a 10gb instance up to 16 SU/hour for a full GPU. You can check full SU overview of MIG here.</p> <p>MIGs address key challenges in shared environments:</p> <ul> <li> <p>Fair resource sharing: MIG enables multiple users to share a single GPU without stepping on each other\u2019s performance.</p> </li> <li> <p>Right-sizing workloads: Users can request GPU capacity that matches their actual workload requirements.</p> </li> <li> <p>Improved scheduling efficiency: MIG instances are smaller and easier to schedule than full GPUs.</p> </li> <li> <p>Reduced idle time: MIG reduces GPU underutilization by splitting large resources into usable chunks.</p> </li> </ul>"},{"location":"MIG/#why-you-should-use-mig-instances","title":"Why You Should Use MIG Instances","text":"<p>Lower SU Cost for Smaller Workloads:  MIG lets you select a GPU slice that meets your job\u2019s needs without paying for more power than you use. For example:</p> <ul> <li>A small training job or inference script may only require <code>10\u201320</code> GB of GPU memory.</li> <li>Running such a job on a <code>10gb</code> or <code>20gb</code> MIG instance will consume only a fraction of the SUs compared to using a full GPU.</li> </ul> <p>No Need to Reserve a Full GPU:  Some jobs \u2014 like unit testing, data preprocessing with GPU acceleration, or light model inference \u2014 simply don\u2019t need the full resources of an 80GB A100. MIG allows users to avoid bottlenecks and free up resources by choosing an appropriately sized instance.This is especially valuable when:</p> <ul> <li>Cluster demand is high</li> <li>Your jobs don\u2019t need massive memory or compute</li> <li>You're trying to run multiple parallel tasks independently</li> </ul> <p>This is especially important if you\u2019re working within a research group\u2019s annual SU allocation on Wulver.</p> <p>Isolated and Predictable Performance:  Each MIG instance has dedicated resources \u2014 memory, L2 cache, compute cores \u2014 and is logically isolated from others on the same GPU. That means:</p> <ul> <li>No performance interference from noisy neighbors</li> <li>Consistent and reliable job behavior</li> <li>Easier debugging and performance tuning</li> </ul> <p>MIG is compatible with CUDA, cuDNN, PyTorch, TensorFlow, and most GPU-accelerated libraries \u2014 no code changes are typically required.</p> <p>Info</p> <p>MIG is not implemented on all GPUs in Wulver. </p>"},{"location":"MIG/job-submission-and-su-charges/","title":"MIG Job Submission and SU Charges","text":"<p>When submitting jobs on Wulver's MIG-enabled A100 GPUs, you must explicitly request the desired MIG profile using the <code>--gres</code> directive in your SLURM script.</p> GPU MIG Slurm Directive 10G MIG <code>--gres=gpu:a100_10g:1</code> 20G MIG <code>--gres=gpu:a100_20g:1</code> 40G MIG <code>--gres=gpu:a100_40g:1</code> <p>Note</p> <p>If you want to see a job script example of requesting a full GPU, please refer to the sample GPU job scripts.</p>"},{"location":"MIG/job-submission-and-su-charges/#running-jobs-with-mig","title":"Running Jobs with MIG","text":"Sample SLURM Script for a MIG JobInteractive session with MIG <pre><code>#!/bin/bash -l\n#SBATCH --job-name=gpu_job\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=gpu\n#SBATCH --qos=standard\n#SBATCH --account=$PI_ucid         # Replace with PI's UCID\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --gres=gpu:a100_10g:1      # Change to 20g or 40g as needed\n#SBATCH --time=59:00\n#SBATCH --mem-per-cpu=4000M\n\nsrun ./myexe &lt;input/output options&gt;\n</code></pre> <pre><code>$srun --partition=gpu \\\n--account=$PI_ucid \\\n--qos=standard \\\n--gres=gpu:a100_10g:1 \\\n--time=00:59:00 \\\n--pty bash\n</code></pre> <p>Warning</p> <p>You cannot run your job using multiple MIG instances. For example, <code>--gres=gpu:a100_10g:2</code> will allocate two instances of the 10G MIG, but it will either raise an error or some jobs may assume it as a single MIG, even if multiple instances are requested.</p>"},{"location":"MIG/job-submission-and-su-charges/#understanding-su-charges","title":"Understanding SU Charges","text":"<p>Wulver uses a Service Unit (SU) model to track computing usage. Your job's SU cost is based on:</p> <ul> <li>CPU usage</li> <li>Memory request</li> <li>GPU memory allocation (via MIG)</li> </ul> <p>Each component contributes to the SU calculation. The SU cost is charged per hour using the formula:</p> <pre><code>SU = MAX(#CPUs, Memory (in GB) / 4) + 16 \u00d7 (GPU memory requested / 80GB)\n</code></pre> <p>Info</p> <p>GPU memory requested is based on the MIG profile, not your actual memory usage during the job.</p> SLURM Directive SU Explaination 4 CPUs + 10MIG MAX(4, 4*4G/4G) + 16 * (10G/80G) = 6 Since no memory requireemnt is specified, SU is charged based on the same number of CPUs and 10G of GPU memory 4 CPUs + 20MIG MAX(4, 4*4G/4G) + 16 * (20G/80G) = 8 SU is charged based on the same number of CPUs and 20G of GPU memory 4 CPUs + 40MIG MAX(4, 4*4G/4G) + 16 * (40G/80G) = 12 SU is charged based on the same number of CPUs and 40G of GPU memory 4 CPUs + Full GPU MAX(4, 4*4G/4G) + 16 * (80G/80G) = 20 SU is charged based on the same number of CPUs and 80G of GPU (A full GPU) memory 4 CPUs + <code>--mem=64G</code> + Full GPU MAX(4, 64G/4G) + 16 * (80G/80G) = 32 The MAX function the evaluates the maximum of 4 SUs (from CPUs), and 64G/4G= 16 SUs (from memory). In addition, 16 SUs are charged from 80G of GPU (A full GPU) memory, bringing the total SU charge to 32 SUs 4 CPUs + <code>--mem-per-cpu=8G</code> + Full GPU MAX(4, 4*8G/4G) + 16 * (80G/80G) = 24 The MAX function the evaluates the maximum of 4 SUs (from CPUs), and 4*8G/4G= 8 SUs (from memory). In addition, 16 SUs are charged from 80G of GPU (A full GPU) memory, bringing the total SU charge to 24 SUs"},{"location":"MIG/job-submission-and-su-charges/#tips-for-efficient-job-submission-think-fit-not-power","title":"Tips for Efficient Job Submission (Think Fit, Not Power)","text":"<ul> <li>Choose the profile that fits your workload, not the biggest one available. You\u2019ll save SUs, get scheduled faster, and help the cluster stay responsive for everyone.</li> <li>Avoid requesting a full GPU unless your job cannot run on a MIG.</li> <li>Combine small jobs using job arrays or batching when possible.</li> <li>Need help estimating SUs? Try submitting test jobs with <code>--time=10:00</code> and reviewing the actual SU usage via the job summary.</li> <li>By default, each CPU is allocated <code>4 GB</code> of memory. If your job requires more, you can request additional memory using <code>--mem</code>. Please ensure you request only as much as your job needs, as over-allocating memory will increase your SU usage. Refer to the formula above for details.</li> <li>MIG is designed to make high-performance GPUs accessible and efficient \u2014 take advantage of it wisely.</li> </ul>"},{"location":"MIG/performance_testing/","title":"Performance Testing Overview","text":"<p>To help users select the appropriate MIG profile for their workloads, we conducted benchmark tests using LLM fine-tuning, PyTorch training, and GROMACS molecular dynamics simulations. Tests were run on the NVIDIA A100 GPUs (full 80 GB and MIG profiles) available on Wulver.</p> <p>The results below show differences in runtime, accuracy, memory usage, and service unit (SU) cost across profiles. Observations and notes are included to explain results.</p>"},{"location":"MIG/performance_testing/#list-of-benchmark-tests","title":"List of Benchmark Tests","text":"GROMACSLLM Fine-TuningMatrix Multiplication Benchmarks <p>The GROMACS benchmark test was conducted for different MIG profiles and a full GPU. The results suggest that the 40G MIG shows better performance compared to a full 80G GPU. We also calculated the cost/performance parameter, which is obtained by taking the ratio of SU consumption to performance. Based on this parameter, the 20G MIG is the best choice. However, if users prefer performance over cost/performance, then the 40G MIG would be the recommended option.</p> GPU Profile Nodes Cores Threads Performance (ns/day) Walltime (s) SU Consumption Cost/Performance A100_10gb MIG 1 1 4 43.28 499.131 6 3.32717 A100_20gb MIG 1 1 4 81.205 265.995 8 2.36439 A100_40gb MIG 1 1 4 115.507 187.003 12 2.49336 A100_80gb full GPU 1 1 4 118.59 182.142 20 4.04756 <p>The benchmark script fine-tunes the Qwen 1.5B Instruct model on the Alpaca-cleaned dataset using QLoRA. Training is done with 4-bit quantization to save memory and LoRA adapters so that only a small set of parameters are updated. The Hugging Face TRL <code>SFTTrainer</code> handles training, while the script also logs runtime, GPU/CPU memory, and tokens processed per second. The setup runs consistently on both full NVIDIA's A100 80GB GPU and different MIG slices (10 GB, 20 GB, 40 GB), making it useful for comparing speed and cost across profiles.</p> GPU Profile Walltime (h) SU Total Tokens Processed Tokens/s Peak Allocated (GB) Peak Reserved (GB) SU per 1M Tokens A100_10gb MIG 1.092 3.28 166327 42.3 5.68 8.97 19.05 A100_20gb MIG 0.556 2.78 166327 83 5.68 18.3 16.39 A100_40gb MIG 0.353 3.18 166327 130.9 5.68 23.55 18.88 A100_80gb full GPU 0.267 4.53 166327 173.2 5.68 23.55 27.04 <ul> <li> <p>Peak Allocated \u2248 5.7 GB across all runs: The model + LoRA fine-tune has a fixed memory demand, regardless of MIG size.</p> </li> <li> <p>Peak Reserved varies (8.9 \u2192 23.5 GB): PyTorch\u2019s caching allocator grabs bigger chunks when more GPU memory is available, but this doesn\u2019t change training feasibility.</p> </li> <li> <p>Efficiency vs. Speed: Smaller MIGs (e.g., 10 GB, 20 GB) can be more cost-efficient per token, while larger MIGs or the full 80 GB GPU finish training faster.</p> </li> <li> <p>Choosing a profile: The right option depends on priorities \u2014 use smaller MIGs to save SUs on long jobs, or larger MIGs when wall-time (speed) is more important.</p> </li> </ul> <p>Info</p> <p>SU values are calculated as: <code>SU = (max(#CPUs, #RAM/4GB) + 16 \u00d7 (GPU_mem/80)) \u00d7 hours</code> </p> <p>Example (A100_20GB, 0.556 hr walltime, 1 CPU, 4 GB RAM, 20 GB GPU): <pre><code>SU = (max(1, 4/4) + 16 \u00d7 (20/80)) \u00d7 0.556\n   = (1 + 4) * 0.556 = 2.78\n</code></pre></p> <p>We ran a matrix multiplication benchmark on different NVIDIA A100 MIG profiles and the full GPU. The test multiplies large square matrices (sizes like 4096\u00d74096 up to 49k\u00d749k) using PyTorch and CUDA.</p> <p>Matrix multiplication is the core operation in deep learning \u2014 it\u2019s what neural networks spend most of their time doing. Measuring how many TFLOPs (trillion floating point operations per second) each MIG slice achieves gives a good picture of its raw compute power.</p> GPU Profile SMs Memory (GB) Peak FP16 TFLOPs Peak FP32 (TF32) TFLOPs Peak Matrix Size (n) Peak GPU Mem Used (GB) SU Usage Factor A100_10gb MIG 14 9.5 38.694 18.985 12288 (FP16), 22528 (FP32) 7.57 2 A100_20gb MIG 28 19.5 79.304 37.887 20480 (FP16), 32256 (FP32) 15.52 4 A100_40gb MIG 42 39.5 118.924 55.576 49152 (FP16), 32768 (FP32) 18.01 8 A100_80gb full GPU 108 79.3 286.185 135.676 16384 (FP16), 16384 (FP32) 18.01 16 <ul> <li>Peak FP16 performance (fast half-precision mode used in AI training).</li> <li>Peak FP32 performance (single precision with TF32 tensor cores, higher accuracy but slower).</li> <li>Largest tested matrix size (n) where peak performance was observed.</li> <li>Peak GPU memory usage, to see whether memory or compute was the bottleneck.</li> <li>SU usage factor, to tie performance back to billing.</li> </ul> <p>The results show that performance scales almost linearly with MIG size (number of SMs), while memory never became the limiting factor. This means compute capacity is the main driver of speed, and users can choose between smaller slices (cheaper, slower) or larger slices (faster, higher SU rate) depending on their workload needs.</p>"},{"location":"MIG/profile-comparison/","title":"MIG Profile Comparison","text":"<p>MIG profiles represent different partitions of a physical NVIDIA A100 80GB GPU. Each profile gives users a slice of compute and memory resources while maintaining full isolation from other workloads running on the same GPU.</p> <p>On Wulver, the following MIG profiles are supported:</p> <ul> <li><code>10gb</code> \u2013 10 GB memory</li> <li><code>20gb</code> \u2013 20 GB memory</li> <li><code>40gb</code> \u2013 40 GB memory</li> </ul> <p>The table below summarizes the hardware characteristics of each MIG profile available on Wulver, alongside the full NVIDIA A100 80 GB GPU. It lists memory capacity, compute resources, and other architectural limits so users can quickly compare performance and capability across profiles.</p> Specification 10gb 20gb 40gb Full 80GB Notes Device Name A100 MIG 10gb A100 MIG 20gb A100 MIG 40gb A100-SXM4-80GB GPU Profile SU Usage factor 2 4 8 16 Service units Global Memory 10.2 GB 20.9 GB 42.4 GB 85.2 GB Raw hardware memory Usable Memory ~9.5 GB ~20 GB ~40 GB ~80 GB Available for applications Multiprocessors (SMs) 14 28 42 108 Parallel compute units Relative Compute Power 1x 2x 3x 7.7x Performance scaling Total Parallel Threads 28672 57344 86016 221184 SMs \u00d7 threads/SMP Memory Bus Width 640 bits 1280 bits 2560 bits 5120 bits Memory bandwidth Memory Bandwidth limits ~1.3 TB/s ~2.6 TB/s ~5.1 TB/s ~10.2 TB/s Theoretical peak Bandwidth\u00a0(B/s)=Memory\u00a0Clock\u00a0(Hz)\u00d7Bus\u00a0Width\u00a0(bits)\u00f78 L2 Cache Size 5 MB 10 MB 20 MB 41 MB Fast memory cache Async Engines 1 2 3 5 Concurrent operations Max Threads per Block 1024 1024 1024 1024 CUDA block limit Max Threads per SMP 2048 2048 2048 2048 Per multiprocessor Memory Clock 1593 MHz 1593 MHz 1593 MHz 1593 MHz Memory frequency Clock Rate 1410 MHz 1410 MHz 1410 MHz 1410 MHz GPU core frequency Shared Memory/Block 49 KB 49 KB 49 KB 49 KB Per CUDA block Registers per Block 65536 65536 65536 65536 Per CUDA block Constant Memory 64 KB 64 KB 64 KB 64 KB Read-only memory Warp Size 32 32 32 32 SIMD execution width ECC Support Yes Yes Yes Yes Error correction Unified Memory Yes Yes Yes Yes CPU-GPU memory sharing Concurrent Kernels Yes Yes Yes Yes Multiple kernel execution Max Grid Dimensions 2\u00b3\u00b9-1 \u00d7 65535 \u00d7 65535 2\u00b3\u00b9-1 \u00d7 65535 \u00d7 65535 2\u00b3\u00b9-1 \u00d7 65535 \u00d7 65535 2\u00b3\u00b9-1 \u00d7 65535 \u00d7 65535 CUDA grid limits"},{"location":"OnDemand/","title":"Open OnDemand","text":""},{"location":"OnDemand/#overview","title":"Overview","text":"<p>Open OnDemand is now available for NJIT HPC access at ondemand.njit.edu!</p> <p>Open OnDemand is a browser-based gateway to NJIT's Wulver cluster and shared storage. It offers a graphical interface allowing users to view, edit, download, and upload files. Users can manage and create job templates for the cluster and access interactive applications such as remote desktops to cluster nodes. Additionally, Open OnDemand supports GUI-based software like Jupyter Lab/Notebook, Matlab, and RStudio, all accessible through a web browser on virtually any device. No additional software installation is necessary, and users can operate with minimal Linux and job scheduler command knowledge.</p> <p>This is an open source project developed through NSF funding.</p>"},{"location":"OnDemand/#features","title":"Features","text":"<ul> <li>Easy to use</li> <li>Great for researchers and students new to HPC</li> <li>Convenient for experienced users as well</li> </ul>"},{"location":"OnDemand/#using-ondemand","title":"Using OnDemand","text":""},{"location":"OnDemand/#logging-into-ondemand","title":"Logging into OnDemand","text":"<p>If you have access to the Wulver cluster, you can use OnDemand. Open any browser and go to ondemand.njit.edu. Use your UCID and password to log in. If you are off campus, you will need to set up VPN to access the platform.</p>"},{"location":"OnDemand/#dashboard","title":"Dashboard","text":"<p>Once you log in, you will see the OnDemand Dashboard. You will see the menu bar on the top where you can access all the tools available including Files Manager, Shell Access, Job Composer, and Interactive Apps. You will also see several pinned apps highlighted on the Dashboard.</p>"},{"location":"OnDemand/2_files/","title":"Files","text":""},{"location":"OnDemand/2_files/#overview","title":"Overview","text":"<p>Files provide you UI based access to your <code>/home</code>, <code>/project</code> and <code>/scratch</code> directory. </p> <p></p>"},{"location":"OnDemand/2_files/#guide","title":"Guide","text":"<p>The File Manager tool is available under Files from the Dashboard. Here you can view, edit, and transfer files between your local computer and the cluster. You can access any of the shared filesystems on Wulver including your <code>$HOME</code> directory as well as Project, Research, and Scratch. This graphical interface makes it easy to navigate your directories and transfer files to the cluster. (This transfer is only for small files such as job scripts or input scripts, please use command line tools, such as rsync for larger datasets).</p> <p>Use the Upload and Download buttons to transfer files between your local computer and the cluster. You can navigate to any of your directories through the Change Directory button where you can enter the path for your desired location. You can also create new folders with the New Directory button.</p>"},{"location":"OnDemand/3_clusters/","title":"Clusters","text":""},{"location":"OnDemand/3_clusters/#overview","title":"Overview","text":"<p>Cluster shell provides you CLI access to Wulver through browser based terminal. </p> <p></p> <p></p> <p>Tip</p> <p>If you are on windows and don't have shell access, then this feature is very helpful.</p>"},{"location":"OnDemand/4_tools/","title":"Tools","text":""},{"location":"OnDemand/4_tools/#overview","title":"Overview","text":"<p>The passenger apps (Tools) on OnDemand are some of the easiest and user-friendly ways to monitor key stats related to your account and Wulver in general.</p> <p></p>"},{"location":"OnDemand/4_tools/#joblist","title":"Joblist","text":"<p>This is a simple tool to monitor your past jobs as well as the service units you have consumed in a given time period. To use this tool:</p> <ul> <li> <p>Click on the Joblist option in the tools drop down menu.</p> </li> <li> <p>Enter the start date and end date between which you want to monitor your past jobs</p> </li> </ul> <p></p> <ul> <li>You will be presented with a table of your jobs with a few job details atop which you find information like service units consumed, your specified date range and the qos.</li> </ul> <p></p>"},{"location":"OnDemand/4_tools/#quota-info","title":"Quota Info","text":"<p>Quota Info is a tool similar to the command \u201cquota_info\u201d on wulver. This tool allows you to see your account information. This information includes:</p> <ul> <li>All your accounts</li> <li>The total available SUs for each account</li> <li>The SUs consumed for each account</li> <li>Your storage information for each account for each partition <code>/project</code>, <code>/scratch</code></li> <li>Your <code>/home</code> storage information</li> </ul> <p></p> <p>This is a very good tool for a quick look at your basic account information and is recommended to be used at least once or twice a month to check your SUs as well as <code>/home</code> storage. If you find your <code>/home</code>is reaching its upper limit you can use another tool called homespace described below, to have a detailed look at your <code>/home</code> usage.</p>"},{"location":"OnDemand/4_tools/#checkload","title":"Checkload","text":"<p>This is a tool which can be used to monitor the CPU load, state, and other parameters on compute nodes across the entire wulver cluster. This tool is not specific to your account.</p> <ul> <li>It can be used to check for idle nodes.</li> <li>It can also be used to cross check your requested configuration.</li> <li>It can also be crudely used to check the load on the cluster</li> </ul> <p></p>"},{"location":"OnDemand/4_tools/#homespace","title":"Homespace","text":"<p>This is one of the most useful tool to monitor your <code>/home</code> partition in detail. This gives you a list of subdirectories and their respective sizes (in MB). You should monitor your <code>/home</code> directory size at least twice a month depending on your usage to be sure you are below the upper limit of 50GB. Ideally you should clean your <code>/home</code> directory if it reaches 40GB or above. If your <code>/home</code> exceeds this limit it severely impacts some operations like creating new conda envs, installing new packages, etc.</p> <p>Also an easy way to keep your <code>/home</code> below this limit is to move your conda environment and package default directory to your <code>/project</code> directory, the details for this can be found here</p> <p>Note</p> <p>Since it calculates the storage used by each subdirectory and file in your <code>/home</code> directory, it may take some time to load the results.</p> <p></p>"},{"location":"OnDemand/4_tools/#ps","title":"PS","text":"<p>This is also a very important tool to monitor processes running on your login node usage. </p> <p></p> <p>Warning</p> <p>It is strictly advised to not run any resource consuming process on the login node, instead use compute node. The login node is shared between all the wulver users. Using login node for any such process like creating conda env, installing conda packages, running your jobs/programs is strictly not advised. Repeatedly using the login node for such processes will result in strict action. If you find there is high activity by any process on your login node and you are unsure of the cause please email to hpc@njit.edu with the screenshot of the output of the PS tool.</p>"},{"location":"OnDemand/4_tools/#qoslist","title":"Qoslist","text":"<p>QOS is an important flag for any type of job submission. You can check all the available QOS for each of your accounts using this tool. Click here for details about QOS.</p> <p></p>"},{"location":"OnDemand/5_My_Interactive_Sessions/","title":"My Interactive Sessions","text":""},{"location":"OnDemand/5_My_Interactive_Sessions/#overview","title":"Overview","text":"<p>All of your active OnDemand sessions will be shown. </p> <p></p> <p></p> <p>Warning</p> <p>Please make sure to delete/close all of your running session after use otherwise it will keep on consuming SU unless low priority is used.</p>"},{"location":"OnDemand/6_jobs/","title":"Jobs","text":""},{"location":"OnDemand/6_jobs/#overview","title":"Overview","text":"<p>The Jobs menu on the menu bar includes both the Job Composer and the Active Jobs tools. The Job Composer assists you to set up and submit jobs to the cluster through a graphical interface using file management tools and access to job templates. </p> <p></p>"},{"location":"OnDemand/6_jobs/#job-composer","title":"Job Composer","text":""},{"location":"OnDemand/6_jobs/#creating-a-new-job","title":"Creating a New Job","text":"From Default TemplateFrom Existing JobFrom Specified Path <ul> <li>Click New Job &gt; From Default Template</li> </ul> <ul> <li> <p>A new job (e.g., Simple Sequential Job) will appear in your job list.</p> </li> <li> <p>Click Open Editor to open the directory in the File Editor.</p> </li> </ul> <p></p> <ul> <li>Modify the main_job.sh script as needed.</li> </ul> <p></p> <ul> <li>Return to the Job Composer and click Job Options to adjust:<ul> <li>Job Name</li> <li>Cluster (ensure it\u2019s set to wulver)</li> <li>Account (It will take the default account. If you have or ever had multiple accounts then you have to specify your account)</li> </ul> </li> </ul> <p></p> <ul> <li>Click Save to apply changes.    </li> </ul> <ul> <li>Select an existing job from your list.</li> <li>Click New Job &gt; From Selected Job.</li> <li>This duplicates the job, allowing you to modify it without altering the original.</li> </ul> <ul> <li>Click New Job &gt; From Specified Path.</li> <li>Enter the full path to a directory containing your job script and necessary files.</li> <li>This is useful for jobs prepared outside the Job Composer.</li> </ul>"},{"location":"OnDemand/6_jobs/#editing-the-job-script","title":"Editing the Job Script","text":"<ol> <li>In the Job Composer, locate your job and click Open Editor under the Submit Script section.</li> <li>Modify the main_job.sh script with your desired commands and SLURM directives. For example: <pre><code>#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --time=01:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4000M\n</code></pre></li> <li>Ensure you adjust the SLURM directives (#SBATCH lines) according to your job\u2019s requirements.</li> <li>Click Save after editing.</li> </ol>"},{"location":"OnDemand/6_jobs/#submitting-the-job","title":"Submitting the Job","text":"<ol> <li>In the Job Composer, select your job.</li> <li>Click Submit.</li> <li>Monitor the job\u2019s status under the Active Jobs tab.<ul> <li>Queued: Waiting for resources.</li> <li>Running: Currently executing.</li> <li>Completed: Finished execution.</li> </ul> </li> </ol>"},{"location":"OnDemand/6_jobs/#outputs","title":"Outputs","text":"<ul> <li>You can check the output/error in folder contents</li> </ul> <ul> <li>Check both files to confirm your program ran successfully.</li> </ul> <p>Note</p> <p>Even if an error might have occurred, your job status will still show complete.</p>"},{"location":"OnDemand/6_jobs/#active-jobs","title":"Active Jobs","text":"<p>The Active Jobs tool will allow you to view all the jobs you\u2019ve submitted that are currently in the queue, via OnDemand or not, so you can check on their status.  </p> <p></p> <p>You can expand each job to check more details. You can also open the current working directory of job in file manager or terminal by clicking <code>Open in File Manger</code> or <code>Open in Terminal</code> respectively.</p> <p></p>"},{"location":"OnDemand/Interactive_Apps/","title":"Interactive Apps","text":""},{"location":"OnDemand/Interactive_Apps/#overview","title":"Overview","text":"<p>We have a lot of interactive apps which you can use for UI interface. They run as jobs in similar way as you would do using shell, except these are interactive. For example, you can start Jupyter Notebook app and work on it. Your files will be saved in directory created by the ondemand unless you specify a different directory during the configurations. </p> <p></p> <p>Warning</p> <p>Please make sure to close your app session once you have completed your work otherwise it will keep consuming SUs.</p>"},{"location":"OnDemand/Interactive_Apps/Matlab/","title":"MATLAB","text":""},{"location":"OnDemand/Interactive_Apps/Matlab/#overview","title":"Overview","text":"<p>We have three different ways to connect Matlab: </p> <ul> <li>Matlab VNC</li> <li>Matlab Server</li> <li>Jupyter Matlab Proxy</li> </ul> <p></p>"},{"location":"OnDemand/Interactive_Apps/Matlab/#guide","title":"Guide","text":"Matlab VNCMatlab ServerJupyter Matlab Proxy <ul> <li> <p>Select Matlab Server from the interactive apps dropdown menu. </p> </li> <li> <p>Fill in your configurations based on your job requirements</p> </li> </ul> <p></p> <ul> <li>Wait for the job to start and then click on Connect to Matlab</li> </ul> <p></p> <ul> <li>Select existing License</li> </ul> <p></p> <ul> <li>Click Start Matlab </li> </ul> <p></p> <ul> <li>Wait for couple minutes</li> </ul> <p></p> <ul> <li>Start working!!</li> </ul> <p></p> <p></p> <ul> <li> <p>Select Jupyter-matlab-proxy from the dropdown menu.</p> </li> <li> <p>Choose matlab-proxy as conda env</p> </li> </ul> <p></p> <ul> <li> <p>Fill the rest of the form based on your desired configurations and click Launch. Wait for couple of seconds (same way as other matlab servers)</p> </li> <li> <p>Once the Jupyter opens, click on Open Matlab</p> </li> </ul> <p></p> <ul> <li>Select existing License</li> </ul> <p></p> <ul> <li>Click start Matlab</li> </ul> <p></p> <ul> <li>Wait for couple minutes</li> </ul> <p></p> <ul> <li>Start working!!</li> </ul> <p></p>"},{"location":"OnDemand/Interactive_Apps/Matlab/#launching-matlab","title":"Launching MATLAB","text":"<ul> <li>Navigate to the Interactive Apps section.</li> <li>Select <code>MATLAB</code> from the list of available applications.</li> </ul>"},{"location":"OnDemand/Interactive_Apps/Matlab/#loading-the-matlab-version","title":"Loading the Matlab Version","text":"<ul> <li>Select the dropdown option in <code>MATLAB Version</code>. The current versions installed on Wulver are <code>2023a</code> and <code>2024a</code></li> </ul>"},{"location":"OnDemand/Interactive_Apps/Matlab/#configuring-resources","title":"Configuring Resources","text":"<ul> <li>Specify your Slurm account/partition/qos.</li> <li>Set the maximum wall time requested.</li> <li>Choose the number of cores you need.</li> <li>If required, specify the number of GPUs.</li> </ul>"},{"location":"OnDemand/Interactive_Apps/Matlab/#launching-the-session","title":"Launching the Session","text":"<ul> <li>Select the <code>Launch</code> option after finalizing the resources. Once clicking Launch, the request will be queued, and when resources have been allocated, you will be presented with the option to connect to the session by clicking on the blue <code>Launch MATLAB</code> option.</li> </ul> <ul> <li>You might see <code>Unable to contact settings server</code> message. It does not mean the Matlab session is terminated. You need to wait a few minutes to see the Matlab popup window.</li> </ul>"},{"location":"OnDemand/Interactive_Apps/Notebook/","title":"Jupyter","text":""},{"location":"OnDemand/Interactive_Apps/Notebook/#launching-jupyter-session","title":"Launching Jupyter Session","text":"<ul> <li>Navigate to the Interactive Apps section.</li> <li>Select <code>Jupyter</code> from the list of available applications.</li> </ul>"},{"location":"OnDemand/Interactive_Apps/Notebook/#loading-the-environment","title":"Loading the Environment","text":"<ul> <li> <p>Choose the <code>Mode</code> option, where you can select the interface:</p> <ul> <li>Jupyterlab </li> <li>Jupyter Notebook</li> </ul> </li> <li> <p>In the <code>Conda Environment</code> section, you can see all your conda environments automatically detected, please select the one which you want to work with. </p> </li> </ul> <p>Note</p> <p>Please make sure that Jupyter package is installed in your conda env otherwise you will get an error. If you are unsure how to install Jupyter Notebook or Jupyterlab in the Conda environment, check Conda Documentation and Jupyter Installation.</p> <ul> <li>Choose the path where you want to start the Jupyter session in <code>Enter the full path of the case directory</code>. For session in <code>$HOME</code> directory keep this blank. </li> </ul> <p></p>"},{"location":"OnDemand/Interactive_Apps/Notebook/#configuring-resources","title":"Configuring Resources","text":"<ul> <li>Specify your Slurm account/partition/qos.</li> <li>Set the maximum wall time requested.</li> <li>Choose the number of cores you need.</li> <li>If required, specify the number of GPUs.</li> </ul>"},{"location":"OnDemand/Interactive_Apps/Notebook/#launching-the-session","title":"Launching the Session","text":"<p>Select the <code>Launch</code> option after finalizing the resources. Once clicking Launch, the request will be queued, and when resources have been allocated, you will be presented with the option to connect to the session by clicking on the blue <code>Connect to Jupyter</code> option.</p> <p></p> <p></p>"},{"location":"OnDemand/Interactive_Apps/Rstudio/","title":"RStudio","text":""},{"location":"OnDemand/Interactive_Apps/Rstudio/#launching-rstudio","title":"Launching RStudio","text":"<ul> <li>Navigate to the Interactive Apps section.</li> <li>Select <code>RStudio</code> from the list of available applications.</li> </ul>"},{"location":"OnDemand/Interactive_Apps/Rstudio/#loading-the-r-environment","title":"Loading the R Environment","text":"<ul> <li>Please select your desired R version from the drop down. </li> <li>To use a different version or environment, please select \"custom\" and enter the necessary commands: For example, to use a Conda environment, enter: <code>module load Anaconda3/2023.09-0; source conda.sh; conda activate my_conda_r_env</code></li> </ul>"},{"location":"OnDemand/Interactive_Apps/Rstudio/#configuring-resources","title":"Configuring Resources","text":"<ul> <li>Specify your Slurm account/partition/qos.</li> <li>Set the maximum wall time requested.</li> <li>Choose the number of cores you need.</li> <li>If required, specify the number of GPUs.</li> </ul>"},{"location":"OnDemand/Interactive_Apps/Rstudio/#launching-the-session","title":"Launching the Session","text":"<p>Once clicking Launch, the request will be queued, and when resources have been allocated, you will be presented with the option to connect to the session by clicking on the blue Connect to R Studio Server button. </p> <p></p> <p></p> <p>Once connected, the familiar R Studio interface is presented, and you will be able to use the allocated resources, and access your research data located on Wulver. Installing packages It's likely your scripts will require additional R libraries; these can be installed using the <code>install.packages()</code> command in the console window of R Studio. </p>"},{"location":"OnDemand/Interactive_Apps/Rstudio/#exiting-the-session","title":"Exiting the session","text":"<p>If a session exceeds the requested running time, it will be killed. You may receive a message \"The previous R session was abnormally terminated...\". Click OK to acknowledge the message and continue. To avoid this message, it's good practice to exit the session cleanly when you have finished. To cleanly exit R Studio, click <code>File -&gt; Quit Session...</code> and then release resources back to the cluster queues by clicking the red Delete button for the relevant session on the My Interactive Sessions page.</p>"},{"location":"Policies/","title":"HPC Policies","text":"<p>All users must be associated with a Principal Investigator's (PI's) allocation. Course-based allocations expire at the end of the semester. If you have accessed Wulver as part of a course, you will need to be associated with a PI's research allocation to retain access after the semester concludes.</p> <p>See the following details on Wulver Usage, Condo and Allocation Policies</p> <ul> <li>Wulver Polices</li> <li>Condo Polices</li> <li>Allocation Policies</li> </ul>"},{"location":"Policies/condo_policies/","title":"Shared Condo Partnership","text":""},{"location":"Policies/condo_policies/#cluster-resource-investment-and-priority-access-policy","title":"Cluster Resource Investment and Priority Access Policy","text":"<p>Faculty members who regularly require more resources than the standard allocation may choose to invest in additional resources\u2014either partial or full nodes\u2014thereby contributing to the growth of the cluster. A catalog of available partial node and GPU investment options is provided below. Principal Investigators (PIs) and their associated users will receive higher job priority, up to the amount of resources they have contributed. Please note that jobs may not necessarily run on the specific nodes purchased; instead, the contributed resources will be made available through a floating reservation equivalent to the purchased capacity.</p> <p>Contributors can also submit jobs using standard SUs and at lower priority (See Job QoS) beyond their reserved allocation. The university will cover all infrastructure-related costs for these contributed nodes. This floating reservation will remain in effect for five years. If the hardware is upgraded or replaced before the end of this period, the job priority will transfer to the closest equivalent resources on the new hardware.</p>"},{"location":"Policies/condo_policies/#full-node-investment","title":"Full node investment","text":"<p>Please contact hpc@njit.edu to discuss your specific computational needs.</p>"},{"location":"Policies/condo_policies/#partial-node-investment","title":"Partial Node Investment","text":"<p>You can invest in partial nodes, either on a per-CPU or per-MIG GPU basis. This flexible model allows you to customize and build resources tailored to your research requirements. Rates for HPC resources are provided below to help you plan your investment.</p> Resource Mem (GB)/Core Cost 1 CPU 4 $150 1 CPU 8 $250 1 CPU 16 $300 A100 GPU (80GB) + 1 CPU 4 $10,000 40GB MIG + 1 CPU 4 $5,000 20GB MIG + 1 CPU 4 $2,500 10GB MIG + 1 CPU 4 $1,250 <p>Info</p> <p>MIG (Multi-Instance GPU) allows a single NVIDIA GPU (like the A100) to be split into multiple independent instances, each with dedicated compute and memory resources. This enables multiple users to share a GPU efficiently. It\u2019s ideal for running smaller workloads without needing a full GPU.</p> <p>Example: If your research workflow requires 128 CPU cores with 4 GB RAM per core, one 40 GB MIG, and one 20 GB MIG, you can invest in 128 CPUs at 150 per core (19,200), plus the MIGs ($5,000 for 40 GB and $2,500 for 20 GB), for a total cost of $26,700.</p> <p>Tips</p> <p>Please contact hpc@njit.edu with an index to invest in partial nodes. Note that all invoices get billed to HPC Services account and not the Equipment account.</p>"},{"location":"Policies/condo_policies/#dedicated-pool","title":"Dedicated Pool","text":"<p>If the shared condo module does not satisfy the needs of the PI, a dedicated pool may be set up. In addition to the nodes, the PI will be charged for all infrastructure costs, including but not limited to electricity, HVAC, system administration, etc. It is strongly recommended to first try the shared condo model. If the shared condo model does not work, the nodes can be converted to a dedicated pool.</p> <p>Warning</p> <p>A dedicated pool of partial node or gpus is not available, only full node investments are allowed.</p>"},{"location":"Policies/wulver_policies/","title":"Wulver Policies","text":""},{"location":"Policies/wulver_policies/#faculty-computing-allowance","title":"Faculty Computing Allowance","text":"<p>Faculty PIs are allocated 300,000 Service Units (SU) per year on request at no cost. An SU is equal to 1 core hour on a standard node. For more details on calculating SUs for GPUs, see Service Units. All users working as part of the PIs project will use this allocation. Multiple PIs working on the same project may pool SUs. The SUs can be renewed annually by providing a brief report describing how the SUs were used and a list of publications, presentations, and awards generated from research conducted. Additional SUs may be purchased at a cost of $0.005/SU. The minimum purchase is 50,000 SU (250 USD). </p> <p>Note</p> <p>The 300,000 SUs are available on <code>--qos=standard</code> only. If PI does not want to buy more SUs, PI's group members can use <code>--qos=low</code> which does not have any SU charges. For more details, see SLURM QOS.</p>"},{"location":"Policies/wulver_policies/#user-storage-allowance","title":"User Storage Allowance","text":"<p>Users will be provided with 50GB of <code>$HOME</code> directories. Home directories are backed up. PIs are additionally provided 2TB project directories. These project directories are backed up. Very fast NVME scratch is available to users. This scratch space is for temporary files generated during a run and will be deleted after 30 days. Additional project storage can be purchased if needed. This additional project space will also be backed up. Users need to manage data so that backed-up data fits in the project directory space. Transient, or rapidly changing data should be stored in the scratch directory. Long-term storage with backups or archival storage for research data will be stored in a yet to be determined campus wide storage resource. See Wulver Filesystems for details.</p>"},{"location":"Policies/wulver_policies/#job-priorities","title":"Job Priorities","text":"<ul> <li> <p>Standard Priority</p> <ul> <li>Faculty PIs are allocated 300,000 Service Units (SU) per year on request at no cost</li> <li>Wall time maximum - 72 hours</li> <li>Additional SUs may be purchased at a cost of $0.005/SU</li> <li>The minimum purchase is 50,000 SU ($250)</li> <li>Jobs can be superseded by those with higher priority jobs</li> </ul> </li> <li> <p>Low Priority</p> <ul> <li>Not charged against SU allocation</li> <li>Wall time maximum - 72 hours</li> <li>Jobs can be preempted by those with higher and standard priority jobs when they are in the queue</li> </ul> </li> <li> <p>High Priority</p> <ul> <li>Not charged against SU allocation</li> <li>Wall time: 72 hours (default), PI can request longer walltimes up to 30 days. ARCS HPC reserves the right to reboot nodes once a month for maintenance \u2014 the second Tuesday of each month. See Cluster Maintenance Updates and News for details</li> <li>Only available to contributors</li> </ul> </li> </ul>"},{"location":"Policies/allocation_policies/","title":"Allocation Policies","text":""},{"location":"Policies/allocation_policies/#introduction","title":"Introduction","text":"<p>This page outlines the policy and guidelines for requesting and obtaining allocations of computing time on the High-Performance Computing (HPC) cluster at New Jersey Institute of Technology (NJIT). The goal is to ensure fair, transparent, and efficient access to this valuable resource while supporting high-quality research at the university</p>"},{"location":"Policies/allocation_policies/#eligibility","title":"Eligibility","text":"<p>The HPC cluster is primarily intended for research conducted by NJIT faculty, and their collaborators. Any NJIT Tenure/Tenure Track Faculty member can serve as a PI on a research or education allocation. Adjunct and Visiting Faculty members are not eligible for research allocations. Any instructor - Lecturer, Adjunct or Visiting Faculty can serve as a PI on an education allocation in the term that the course will be offered. Emeritus faculty can serve as a PI provided they are actively supervising students who need access to the cluster or are teaching a registrar scheduled course.</p>"},{"location":"Policies/allocation_policies/#proposal-deadlines","title":"Proposal Deadlines","text":"<p>The Research Computing Advisory Board (RCAB) will review and award allocation proposals exceeding the annual 300K SUs up to 4 times a year. A call for proposals will be issued to the HPC PIs mailing list. The table below shows the submission deadline and award start for the four terms. The details for the proposal can be found at HPC Allocation Request.</p> Quarter Submit Date Allocation Start Date Winter      Nov 30      Jan 1 Spring  Feb 28  Apr 1 Summer      May 31      Jul 1 Fall      Aug 31      Oct 1"},{"location":"Policies/allocation_policies/1.allocation_categories/","title":"Allocation Categories","text":"<p>The HPC cluster offers different allocation categories to cater to diverse research needs:</p> <ul> <li>Annual: Suitable for small to medium-scale projects requiring moderate resource usage. This is provided by default to all HPC PI\u2019s, 300K SU annually</li> <li>Research: Designed for computationally intensive projects requiring significant resources. Max 5M per year.</li> <li>Education: Designed for use in courses that incorporate HPC in their curriculum. Recommended 2500 SU/student</li> </ul>"},{"location":"Policies/allocation_policies/1.allocation_categories/#allocation-limits-and-request-process","title":"Allocation Limits and Request Process","text":""},{"location":"Policies/allocation_policies/1.allocation_categories/#annual-allocation-300k-sus","title":"Annual Allocation (300K SUs)","text":"<ul> <li>Who may apply: Any qualified PI may request a Startup allocation for up to 300,000 SU's. The intent is to support low intensity projects, such as small analysis efforts or program benchmarking and characterization work in preparation for applying for a Research allocation.</li> <li>How to apply: An Annual allocation is provided by default to any qualified PI by requesting access to HPC resources.</li> <li>Review and Awards: Annual allocations may be made at any time throughout the year, but their start time is set to the beginning of the fiscal year. All annual allocations will be renewed automatically at the beginning of the fiscal year. PI\u2019s should submit a progress report within 3 months of the new fiscal year. Failure to submit a progress report will impact the automatic renewal in the next cycle.</li> </ul>"},{"location":"Policies/allocation_policies/1.allocation_categories/#research-allocation-300k-and-5m-sus","title":"Research Allocation (&gt;300K and &lt; 5M SUs)","text":"<ul> <li>Who may apply: Any qualified PI may request a Research allocation for more than 300,000 SU's, up to the maximum 5,000,000 SUs. Research allocations are for a well-defined, compute-intensive project that demonstrably requires resources beyond the Startup limit. </li> <li>How to apply: A request for an allocation requires that a formal proposal be submitted via InfoReady to the Research Computing Advisory Board (RCAB). The formal proposal must be in PDF format (check at tex template), and should include a technical merit section within a 4-page limit. Proposals that fail to do so will NOT be reviewed by the committee.</li> </ul>"},{"location":"Policies/allocation_policies/2.review/","title":"Review and Evaluation","text":""},{"location":"Policies/allocation_policies/2.review/#review-process","title":"Review Process","text":"<p>All allocation requests must be submitted through InfoReady. Applications will be reviewed by the Research Computing Advisory Board (RCAB) four times a year.</p>"},{"location":"Policies/allocation_policies/2.review/#evaluation-criteria-for-research-allocations-300k-sus","title":"Evaluation Criteria for Research Allocations (&gt;300K SUs)","text":"<p>The RCAB will evaluate proposals based on the following criteria, focusing on the necessity, technical viability, and efficient utilization of the requested large-scale resources. The committee reviews for computational merit and resource justification, not for the general scientific merit of the underlying research.</p>"},{"location":"Policies/allocation_policies/2.review/#necessity-and-justification-why-this-allocation-is-needed","title":"Necessity and Justification (Why this Allocation is Needed)","text":"<ol> <li> <p>Necessity of Scale and Exhaustion of Default Allocation:</p> <ol> <li>Is the request for more than 300K SUs clearly justified by the scale of the problem (e.g., large data volume, high-resolution runs, extensive parameter space)?</li> <li>Has the PI demonstrated that the work cannot be feasibly completed within the standard 300K SU annual allocation?</li> </ol> </li> <li> <p>Alignment and Institutional Return:</p> <ol> <li>Does the project align with NJIT's strategic research goals?</li> <li>Does the project provide a high return on investment, particularly by supporting or generating new external grant funding?</li> </ol> </li> </ol>"},{"location":"Policies/allocation_policies/2.review/#technical-viability-and-high-efficiency-scaling","title":"Technical Viability and High-Efficiency Scaling","text":"<ol> <li> <p>Appropriateness of Methodology:</p> <ol> <li>Is the proposed methodology a suitable and efficient use of the parallel HPC cluster architecture (i.e., does the problem require concurrent processing)?</li> </ol> </li> <li> <p>Code Readiness and Demonstrated Performance:</p> <ol> <li>Has the PI provided quantitative evidence (e.g., benchmark data, scaling plots) that the code is ready to run and will perform efficiently when utilizing a large number of cores/nodes?</li> <li>Is the requested amount of compute time logically justified based on problem size and demonstrated scaling efficiency?</li> </ol> </li> <li> <p>Optimal Resource Utilization:</p> <ol> <li>Are the requested ancillary resources (e.g., high-memory nodes, GPU nodes, specific storage) precisely matched to the technical needs of the application, avoiding waste or over-provisioning?</li> </ol> </li> </ol>"},{"location":"Policies/allocation_policies/2.review/#project-management-outcomes-and-stewardship-crucial-for-renewals","title":"Project Management, Outcomes, and Stewardship (Crucial for Renewals)","text":"<ol> <li> <p>Coherence of Computational Plan:</p> <ol> <li>Does the proposal present a clear, executable, and well-phased plan for utilizing the substantial allocation within the one-year timeframe?</li> <li>Does the PI or team possess the necessary technical expertise to manage large-scale, parallel computing jobs?</li> </ol> </li> <li> <p>History of Use and Demonstrated Stewardship (For Renewals Only):</p> <ol> <li>Was the previous allocation time utilized efficiently and responsibly?</li> <li>Did the PI meet all reporting requirements, and is the progress reported in the mandatory Progress Report (Section 7) consistent with the resources consumed and the project goals?</li> </ol> </li> <li> <p>Deliverables and Data Management:</p> <ol> <li>Are the specific deliverables (e.g., manuscripts, thesis chapters) that will result from this allocation clearly defined?</li> <li>Is there a feasible and detailed plan for managing the large data I/O, transfer, and archiving/cleanup upon project completion?</li> </ol> </li> </ol>"},{"location":"Policies/allocation_policies/3.proposal_guidelines/","title":"Proposal Guideline Sections","text":""},{"location":"Policies/allocation_policies/3.proposal_guidelines/#sections-within-4-page-limit","title":"Sections (within 4-page limit):","text":"<ul> <li>Background: Section describing how the resources will be used to address the problem (specific models, theory, etc.).</li> <li>Problem Statement: Section describing the desired high-level scientific/engineering outcomes of the project.</li> <li>Methodology: Section describing the computational methodology that will be used. This should include the applications required and specific cluster architecture needs (e.g., GPU, high-memory).</li> <li>Software Characteristics &amp; Benchmarking: Section describing the performance, scaling, and other characteristics of the software application that will be used, ideally based on small-scale test-runs on the resources of interest.</li> <li>Research Schedule &amp; Resource Plan: Section describing the research schedule, including the anticipated expenditure of granted resources (estimation of computational time). Allocations are assumed to be uniformly consumed over their lifetime. If this is not the case, an estimate of expenditure by quarters is strongly recommended.</li> </ul>"},{"location":"Policies/allocation_policies/3.proposal_guidelines/#sections-not-subject-to-the-4-page-limit","title":"Sections (Not Subject to the 4-Page Limit):","text":"<ul> <li>Impacts and Outcomes of Previous Allocations: Describe impacts and outcomes of any previous allocations. Must refer to the required Progress Report (Section 7) if seeking renewal.</li> <li>List of Publications/Reports: List reports, presentations, publications that were enabled by previous allocations.</li> <li>External Funding: Describe or list related external research funding.</li> </ul> <p>Tip</p> <p>Please bear in mind that the Research Computing Advisory Board (RCAB) is composed of faculty members with a very wide range of scientific backgrounds, and any of them may be assigned to review your proposal. When describing the goals of your research, as well as the tools and approaches you plan to use, write your proposal in such a way that it is informative to people working in the same or related fields, and understandable to a scientifically literate lay reader.</p>"},{"location":"Policies/allocation_policies/4.acknowledgement/","title":"Acknowledgement and Reporting","text":""},{"location":"Policies/allocation_policies/4.acknowledgement/#acknowledgement","title":"Acknowledgement","text":"<p>All users should acknowledge NJIT ARCS in papers, posters, presentations and other published documents. Suggested text</p> <p><code>Portions of this work was performed on High Performance Computing resources provided by the Advanced Research Computing Services team at NJIT</code></p>"},{"location":"Policies/allocation_policies/4.acknowledgement/#reporting-requirements","title":"Reporting Requirements","text":"<p>Recipients of all allocations (Annual and Research) are required to submit an annual progress report detailing their resource usage and research progress. These reports are crucial for monitoring cluster utilization, ensuring responsible use, and informing future allocation decisions.</p>"},{"location":"Policies/allocation_policies/4.acknowledgement/#timing","title":"Timing","text":"<p>Reports are due within 3 months of the start of the new fiscal year. Failure to submit a report will jeopardize future automatic renewals (Startup) or delay the processing of new proposals (Research).</p>"},{"location":"Policies/allocation_policies/4.acknowledgement/#pi-responsibilities-in-the-progress-report-focus-for-renewals","title":"PI Responsibilities in the Progress Report (Focus for Renewals)","text":"<p>To be considered a good steward of HPC resources, the PI must address the following points in their report:</p> <ul> <li>SU Utilization Rate: Provide the percentage of the total awarded SUs that were used.</li> <li>Job Performance Metrics: Summarize the average job size, average run time, and Job Efficiency (e.g., scaling achieved in production runs), demonstrating the efficient use of parallel resources.</li> <li>Computational Progress: Briefly summarize the computational milestones completed using the SUs, referencing the original plan.</li> <li>Key Findings and Deliverables: List the scientific/engineering findings, publications, presentations, or external grant proposals submitted/awarded that were enabled by the allocated time.</li> <li>Data Management: Detail the final data storage status and the plan for archiving/removing large data volumes from the cluster's shared directories.</li> <li>Rationale for Renewal: A clear statement linking the progress made to the remaining work required for a new allocation request (for Research proposals).</li> </ul>"},{"location":"Policies/allocation_policies/5.data_policies/","title":"Data policies and HPC software","text":""},{"location":"Policies/allocation_policies/5.data_policies/#data-policies","title":"Data Policies","text":"<p>All users must adhere to the university's data storage, security, and sharing policies. This includes implementing appropriate security measures to protect sensitive data and complying with regulations governing data privacy and confidentiality.</p>"},{"location":"Policies/allocation_policies/5.data_policies/#software-compatibility","title":"Software Compatibility","text":"<p>The HPC cluster supports a range of commonly used scientific software packages. Users are encouraged to utilize these supported software to ensure compatibility and minimize technical support requirements. The HPC team does not purchase or procure software. It is the responsibility of the faculty requesting the allocations to purchase or procure software. The HPC team can assist with installing the software. Check out the Software section for instructions on how to request software installation and to view the list of software packages already available on Wulver.</p>"},{"location":"Policies/allocation_policies/6.training/","title":"Training and Periodic Policy Review","text":""},{"location":"Policies/allocation_policies/6.training/#training-and-support","title":"Training and Support","text":"<p>The HPC center offers training workshops and technical support services to assist users with accessing and utilizing the cluster resources effectively. Users are encouraged to take advantage of these resources to ensure optimal use of the HPC cluster. Check out the HPC training sessions at HPC Events.</p>"},{"location":"Policies/allocation_policies/6.training/#periodic-policy-review","title":"Periodic Policy Review","text":"<p>This policy and its associated guidelines will be reviewed periodically by the HPC Committee to ensure they remain relevant and effective in supporting the evolving research needs of the university community.</p>"},{"location":"Policies/allocation_policies/6.training/#additional-notes","title":"Additional Notes","text":"<ul> <li>The HPC Committee reserves the right to modify allocations based on cluster availability, historical usage and unforeseen circumstances.</li> <li>Users are expected to use the allocated resources responsibly and ethically. Misuse of resources may result in suspension of access privileges.</li> <li>This policy and guidelines are subject to change.</li> </ul>"},{"location":"Running_jobs/","title":"Overview","text":"<p>Wulver is a shared resource among researchers, faculty and students. It is important to use it efficiently so that everyone can complete their tasks without delay. Therefore, running jobs on Wulver, you should follow certain norms which ensures that your work is done on time and also lets others run their task without any conflict. We use Slurm on Wulver to schedule and manage jobs.</p>"},{"location":"Running_jobs/#what-is-slurm","title":"What is SLURM?","text":"<p>Slurm (Simple Linux Utility for Resource Management) is an open-source workload manager and job scheduler designed for high-performance computing clusters. It is widely used in research, academia, and industry to efficiently manage and allocate computing resources such as CPUs, GPUs, memory, and storage for running various types of jobs and tasks. Slurm helps optimize resource utilization, minimizes job conflicts, and provides a flexible framework for distributing workloads across a cluster of machines. It offers features like job prioritization, fair sharing of resources, job dependencies, and real-time monitoring, making it an essential tool for orchestrating complex computational workflows in diverse fields.</p>"},{"location":"Running_jobs/#some-best-practices-to-follow","title":"Some best practices to follow:","text":"<ul> <li> <p>Request Only the Resources You Need :     Be precise when requesting CPUs, memory, GPUs, and runtime.      Avoid overestimating job time (<code>--time</code>) and memory (<code>--mem</code>) as it reduces scheduler efficiency.     Use monitoring tools to understand your typical usage patterns and adjust accordingly.</p> </li> <li> <p>Do not run jobs on Login Node :     Login node is the entry point for Wulver and has limited memory and resources.     Please avoid directly running jobs on the login node as it can slow down the system for everyone.      Always submit jobs to compute nodes via slurm script or start an interactive session.</p> </li> <li> <p>Use Appropriate Partitions :     Submit jobs to the correct partition based on resource needs (e.g., GPU, high-memory).</p> </li> <li> <p>Test and Debug with Small Jobs First :     Use short test runs or dedicated debug partitions for code testing or troubleshooting.     This helps prevent long-running failures and wasted compute hours.</p> </li> <li> <p>Monitor Your Jobs :     Please use commands like <code>squeue</code>, <code>slurm_jobid $jobid</code>, <code>seff $jobid</code> to check your job status      You can also use our Ondemand Tools.</p> </li> <li> <p>Respect Fair Usage Policies :     Do not monopolize shared resources by submitting excessive large jobs.     Be mindful of Wulver's usage policy.</p> </li> <li> <p>Leverage MIGs for Efficient GPU Utilization :     Our Nvidia A100 GPUs have MIG implementation which allows a single GPU to be split into multiple isolated instances.     Use MIG-compatible partitions when your task doesn\u2019t require the full GPU power     More info about MIG.</p> </li> </ul>"},{"location":"Running_jobs/array-jobs/","title":"Array Jobs","text":""},{"location":"Running_jobs/array-jobs/#overview","title":"Overview","text":"<p>Array jobs allow you to submit many similar jobs (like simulations or data processing tasks) with a single sbatch command. Each job in the array runs the same script but can process different input parameters.</p> <p>Analogy: Imagine you're baking cookies in multiple batches. Each tray (array job) uses the same recipe (your script), but maybe with a different flavor (input). Instead of submitting each tray separately, you give the oven a list and it bakes them one by one or in parallel!</p>"},{"location":"Running_jobs/array-jobs/#why-use-array-jobs","title":"Why use Array jobs?","text":"<p>Array jobs are powerful and efficient for batch-processing many similar tasks. They save time, simplify management, and optimize cluster usage.</p> <p>If you\u2019re running <code>50</code> experiments with the same script \u2014 don\u2019t submit <code>50</code> jobs. Use an array job instead!</p> <ul> <li>Simplifies the submission of multiple similar jobs with a single script</li> <li>Reduces scheduler overhead by bundling related tasks into one job</li> <li>Keeps your job queue cleaner and more organized</li> <li>Makes it easier to monitor, debug, and manage large-scale workflows</li> <li>Ideal for training machine learning models on multiple datasets</li> <li>Useful for running simulations across a range of input parameters</li> <li>Efficient for processing large datasets by splitting them into manageable chunks</li> </ul>"},{"location":"Running_jobs/array-jobs/#special-variables-in-array-jobs","title":"Special Variables in Array Jobs","text":"Variable Description <code>SLURM_JOB_ID</code> Unique ID for each job element (individual task) <code>SLURM_ARRAY_JOB_ID</code> Shared job ID for the entire job array <code>SLURM_ARRAY_TASK_ID</code> The task ID for the current job in the array <code>SLURM_ARRAY_TASK_MIN</code> The lowest task ID in this job array <code>SLURM_ARRAY_TASK_MAX</code> The highest task ID in this job array <code>SLURM_ARRAY_TASK_COUNT</code> Total number of tasks in the job array"},{"location":"Running_jobs/array-jobs/#array-job-examples","title":"Array Job Examples","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J myprogram\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --array=1-30\n#SBATCH --output=myprogram%A_%a.out\n#SBATCH --error=myprogram%A_%a.err  \n#SBATCH --time=71:59:59\n\n./myprogram input$SLURM_ARRAY_TASK_ID.dat\nsleep 10\n</code></pre> <p>This example demonstrates how to use SLURM job arrays to run the same program multiple times with varying inputs. Here's what each part of the script does:</p> <ul> <li> <p><code>#SBATCH --array=1-30</code>: This line creates a job array with 30 tasks. Each task in the array gets a unique <code>SLURM_ARRAY_TASK_ID</code> from 1 to 30.</p> </li> <li> <p><code>#SBATCH --output=myprogram%A_%a.out</code> <code>#SBATCH --error=myprogram%A_%a.err</code>: These lines set up output and error file names for each array task. %A is replaced with the array job ID. %a is replaced with the task index (from 1 to 30). This prevents files from different tasks from overwriting each other.</p> </li> <li> <p><code>./myprogram input$SLURM_ARRAY_TASK_ID.dat</code>: This runs the program using different input files for each task. For example:</p> <ul> <li>Task 1 runs: <code>./myprogram input1.dat</code></li> <li>Task 2 runs: <code>./myprogram input2.dat</code></li> <li>\u2026and so on up to <code>input30.dat</code></li> </ul> </li> <li> <p><code>sleep 10</code>: This is just a placeholder command to simulate a small wait time. You can remove or replace it as needed.</p> </li> </ul>"},{"location":"Running_jobs/array-jobs/#job-array-use-case","title":"Job Array Use Case","text":"<p>I have an application, app, that needs to be run against every line of my dataset. Every line changes how app runs slightly, but I need to compare the runs against each other.</p> <p>Older, slower way of homogenous batch submission:</p> <pre><code>#!/bin/bash\nDATASET=dataset.txt\nscriptnum = 0\nwhile read LINE; do\necho \"app $LINE\" &gt; ${scriptnum}.sh\nsbatch ${scriptnum}.sh\nscriptnum=$(( scriptnum + 1 ))\ndone &lt; $DATASET\n</code></pre> <p>Not only is this needlessly complex, it is also slow, as sbatch has to verify each job as it is submitted. This can be done easily with array jobs, as long as you know the number of lines in the dataset. This number can be obtained like so: <code>wc -l</code> dataset.txt in this case lets call it <code>100</code>.</p> <p>Better way:</p> <pre><code>#!/bin/bash\n#SBATCH - - array=1-100\nsrun app `sed - n \"${SLURM_ARRAY_TASK_ID}\"` dataset.txt\n</code></pre>"},{"location":"Running_jobs/batch-jobs/","title":"Batch Jobs","text":""},{"location":"Running_jobs/batch-jobs/#overview","title":"Overview","text":"<p>Batch jobs are like pre-written instructions handed over to the cluster to be executed when resources become available.</p> <p>Unlike your personal laptop where you run commands interactively, HPC jobs are queued and run asynchronously using a job script \u2014 a text file that tells Slurm:</p> <ul> <li>What resources do you need?</li> <li>What is your PI account?</li> <li>What programs or commands are needed to run job?</li> <li>How long your job may take?</li> </ul>"},{"location":"Running_jobs/batch-jobs/#example-of-batch-job-slurm-script","title":"Example of batch job slurm script","text":"CPU NodesGPU NodesDebug NodeBigmem Node Sample Job Script to use: submit.sh Using 1 coreUsing multiple coresUsing multiple threadsUsing multiple cores and threads <pre><code>#!/bin/bash -l\n#SBATCH --job-name=job_name\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --account=$PI_ucid # Replace $PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=59:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4000M\n\n./myexe &lt;input/output options&gt; # myexe is the executable in this example.\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=job_name\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --account=$PI_ucid # Replace $PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=59:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4000M\n\nsrun ./myexe &lt;input/output options&gt; # myexe is the executable in this example.\n</code></pre> <p><pre><code>#!/bin/bash -l\n#SBATCH --job-name=job_name\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --account=$PI_ucid # Replace $PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=59:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4000M\n\nOMP_NUM_THREADS=$SLURM_NTASKS ./myexe &lt;input/output options&gt;\n</code></pre> Use this script, if your code relies on threads instead of cores. </p> <p><pre><code>#!/bin/bash -l\n#SBATCH --job-name=job_name\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --account=$PI_ucid # Replace $PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks=64\n#SBATCH --cpus-per-task=2\n#SBATCH --time=59:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4000M\n\nsrun gmx_mpi mdrun ... -ntomp $SLURM_CPUS_PER_TASK ...\n</code></pre> This is the example script of GROAMCS which uses both CPUs and threads.      </p> <p>Warning</p> <p>Do not request multiple cores unless your code is parallelized. Before using multiple cores, ensure that your code is capable of parallelizing tasks; otherwise, it will unnecessarily consume service units (SUs) and may negatively impact performance. Please review the code's documentation thoroughly and use a single core if it does not support parallel execution.  </p> <ul> <li>Here, the job requests 1 node on the <code>general</code> partition with <code>qos=standard</code>. Please note that the memory relies on the number of cores you are requesting. </li> <li>As per the policy, users can request up to 4GB memory per core, therefore the flag  <code>--mem-per-cpu</code> is used for memory requirement. If you are using 1 core and need more memory, use <code>--mem</code> instead. </li> <li>In this above script <code>--time</code> indicates the wall time which is used to specify the maximum amount of time that a job is allowed to run. The maximum allowable wall time depends on SLURM QoS, which you can find in QoS). </li> <li>To submit the job, use <code>sbatch submit.sh</code> where the <code>submit.sh</code> is the job script. Once the job has been submitted, the jobs will be in the queue, which will be executed based on priority-based scheduling. </li> <li>To check the status of the job use <code>squeue -u $LOGNAME</code> and you should see the following  <pre><code>JOBID PARTITION     NAME     USER  ST    TIME    NODES  NODELIST(REASON)\n635   general     job_nme   ucid   R   00:02:19    1      n0088\n</code></pre> Here, the <code>ST</code> stands for the status of the job. You may see the status of the job <code>ST</code> as <code>PD</code> which means the job is pending and has not been assigned yet. The status change depends upon the number of users using the partition and resources requested in the job. Once the job starts, you will see the output file with an extension of <code>.out</code>. If the job causes any errors, you can check the details of the error in the file with the <code>.err</code> extension.</li> </ul> <p>In case of submitting the jobs on GPU, you can use the following SLURM script </p> Sample Job Script to use: gpu_submit.sh Using 1 core, 1 GPUUsing multiple cores, 1 GPUUsing multiple cores, GPUsUsing MIGs <pre><code>#!/bin/bash -l\n#SBATCH --job-name=gpu_job\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=gpu\n#SBATCH --qos=standard\n#SBATCH --account=$PI_ucid # Replace $PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:1\n#SBATCH --time=59:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4000M\n\n./myexe &lt;input/output options&gt; # myexe is the executable in this example.\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=gpu_job\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=gpu\n#SBATCH --qos=standard\n#SBATCH --account=$PI_ucid # Replace $PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --gres=gpu:1\n#SBATCH --time=59:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4000M\n\nsrun ./myexe &lt;input/output options&gt; # myexe is the executable in this example.\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=gpu_job\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=gpu\n#SBATCH --qos=standard\n#SBATCH --account=$PI_ucid # Replace $PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --gres=gpu:2\n#SBATCH --time=59:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4000M\n\nsrun ./myexe &lt;input/output options&gt; # myexe is the executable in this example.\n</code></pre> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=gpu_job\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=gpu\n#SBATCH --qos=standard\n#SBATCH --account=$PI_ucid # Replace $PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --gres=gpu:a100_10g:1  # This uses 10G MIG, to use 20G or 40G MIG, modify 10g to 20g or 40g \n#SBATCH --time=59:00  # D-HH:MM:SS\n#SBATCH --mem-per-cpu=4000M\n\nsrun ./myexe &lt;input/output options&gt; # myexe is the executable in this example.\n</code></pre> <p>Warning</p> <p>Do not use multiple GPUs unless you are certain that your job's performance will benefit from them. Most GPU jobs do not require multiple CPUs either. Please remember that unnecessarily requesting additional resources can negatively impact job performance and will also consume more service units (SUs).  </p> <p>The <code>debug</code> QoS in Slurm is intended for debugging and testing jobs. It usually provides a shorter queue wait time and quicker job turnaround. Jobs submitted with the <code>debug</code> QoS have access to a limited set of resources (Only 4 CPUS on Wulver), making it suitable for rapid testing and debugging of applications without tying up cluster resources for extended periods. </p> Sample Job Script to use: debug_submit.sh <pre><code>#!/bin/bash -l\n#SBATCH --job-name=debug\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=debug\n#SBATCH --qos=debug\n#SBATCH --account=$PI_ucid # Replace $PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --time=7:59:00  # D-HH:MM:SS, Maximum allowable Wall Time 8 hours\n#SBATCH --mem-per-cpu=4000M\n\n./myexe &lt;input/output options&gt;\n</code></pre> <p>The bigmem nodes provide 2 TB of RAM in total. By default, each CPU core is allocated 16 GB of memory, but you can request additional memory if your job requires it.</p> Sample Job Script to use: bigmem_submit.sh <pre><code>#!/bin/bash -l\n#SBATCH --job-name=bigmem_job\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=bigmem\n#SBATCH --qos=bigmem\n#SBATCH --account=$PI_ucid # Replace $PI_ucid which the NJIT UCID of PI\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --time=59:00  # D-HH:MM:SS, Maximum allowable Wall Time 8 hours\n#SBATCH --mem-per-cpu=16000M\n\n./myexe &lt;input/output options&gt;\n</code></pre> <p>To submit the jobs, <code>sbatch</code> command.</p>"},{"location":"Running_jobs/checkpointing/","title":"Checkpointing","text":""},{"location":"Running_jobs/checkpointing/#overview","title":"Overview","text":"<p>Checkpointing is the process of saving the current state of a running job at regular intervals so that it can be resumed later from that state, rather than starting from scratch. This is especially useful in long-running or resource-intensive tasks on HPC systems like Wulver, where interruptions or failures may occur.</p> <p>Checkpointing typically involves:</p> <ul> <li>Periodic saving of application state (memory, variables, file handles, etc.)</li> <li>Resuming computation from the last saved state</li> <li>Integration with SLURM job re-submission or recovery workflows</li> </ul>"},{"location":"Running_jobs/checkpointing/#why-use-checkpointing","title":"Why Use Checkpointing?","text":"Benefit Description Failure Recovery Resume jobs from the last checkpoint after a node crash or time expiration. Efficient Resource Use Prevents waste of computation time on long jobs that are interrupted. Preemption Tolerance Helps tolerate job preemption on shared clusters or spot instances. Job Time Limit Bypass Breaks large jobs into smaller chunks to fit within SLURM time limits."},{"location":"Running_jobs/checkpointing/#examples-for-checkpointing","title":"Examples for checkpointing","text":"PythonPytorchTensorflowC/C++GROMACSLAMMPSOpenFOAM (CFD) <p>Save intermediate state using Python\u2019s built-in <code>pickle</code> module \u2014 ideal for lightweight scripts.</p> <pre><code>import pickle\nimport time\n\ndef save_checkpoint(data, filename=\"checkpoint.pkl\"):\n    with open(filename, \"wb\") as f:\n        pickle.dump(data, f)\n\ndef load_checkpoint(filename=\"checkpoint.pkl\"):\n    try:\n        with open(filename, \"rb\") as f:\n            return pickle.load(f)\n    except FileNotFoundError:\n        return {\"iteration\": 0}\n\nstate = load_checkpoint()\nfor i in range(state[\"iteration\"], 1000):\n    # Do some work\n    time.sleep(1)\n    print(f\"Running step {i}\")\n\n    # Save progress every 100 steps\n    if i % 100 == 0:\n        save_checkpoint({\"iteration\": i})\n</code></pre> <p>A common practice in PyTorch to checkpoint model weights, optimizer state, and epoch index \u2014 useful for training recovery.    </p> <pre><code>import torch\n\n# Save checkpoint\ntorch.save({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict()\n}, 'checkpoint.pth')\n\n# Load checkpoint\ncheckpoint = torch.load('checkpoint.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\n</code></pre> <p>Using Keras callbacks, checkpoints are saved automatically during training. Only model weights are saved to keep storage efficient.</p> <pre><code>import tensorflow as tf\n\nmodel = tf.keras.models.Sequential([...])\ncheckpoint_path = \"checkpoints/model.ckpt\"\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                save_weights_only=True,\n                                                save_freq='epoch')\n\nmodel.fit(data, labels, epochs=10, callbacks=[checkpoint_cb])\n</code></pre> <p>In C/C++, you can implement basic checkpointing by writing a loop index or state to a file and loading it at the next run. Ideal for simple simulations or compute-intensive loops.</p> <pre><code>int main() {\n    int current_step = load_checkpoint(\"state.dat\"); // Custom function\n    for (int i = current_step; i &lt; MAX; i++) {\n        // Work\n        if (i % 1000 == 0) {\n            save_checkpoint(i, \"state.dat\");\n        }\n    }\n}\n</code></pre> <p>GROMACS supports checkpointing with <code>.cpt</code> files during molecular simulations:</p> <pre><code>gmx mdrun -deffnm simulation -cpt 15  # Saves checkpoints every 15 minutes\n\n# Restart from checkpoint\ngmx mdrun -s topol.tpr -cpi simulation.cpt\n</code></pre> <p>LAMMPS checkpointing is usually done using <code>write_restart</code> and <code>read_restart</code>:</p> <pre><code>write_restart restart.equilibration\n\n# In a new job\nread_restart restart.equilibration\n</code></pre> <p>Checkpointing is done by writing time steps to disk and restarting from a previous time directory:</p> <pre><code># Set in controlDict\nwriteInterval   100;\n\n# Restart\nstartFrom       latestTime;\n</code></pre>"},{"location":"Running_jobs/dependency-jobs/","title":"Dependency Jobs","text":""},{"location":"Running_jobs/dependency-jobs/#overview","title":"Overview","text":"<p>In many workflows, one job must start only after another has successfully completed. For example, you might want to:</p> <ul> <li>Preprocess data in one job and then analyze it in another</li> <li>Run a simulation, then run a visualization job</li> <li>Compile a program, and then run the executable</li> </ul> <p>Slurm allows you to chain jobs together using job dependencies, so that one job begins only when the specified condition on another job is met.</p> <p>This avoids manual tracking and reduces errors in job sequencing.</p>"},{"location":"Running_jobs/dependency-jobs/#job-dependency-options","title":"Job Dependency Options","text":"Dependency Type Description <code>after:job_id[:job_id...]</code> This job can begin execution after the specified jobs have started. <code>afterany:job_id[:job_id...]</code> This job can begin execution after the specified jobs have terminated, regardless of state. <code>afterburstbuffer:job_id[:job_id...]</code> This job starts after the specified jobs have terminated and any associated burst buffer stage-out operations are complete. <code>afternotok:job_id[:job_id...]</code> This job starts only if the specified jobs fail (non-zero exit code, node failure, timeout, etc). <code>afterok:job_id[:job_id...]</code> This job starts only if the specified jobs succeed (exit code 0). <code>aftercorr:job_id</code> Each task in this job array will start after the corresponding task ID in the specified job has completed successfully. <code>singleton</code> This job will only start after any previous jobs with the same name and user have finished."},{"location":"Running_jobs/dependency-jobs/#job-dependency-examples","title":"Job Dependency Examples","text":"<p>Run Second Job After First Job Completes Successfully</p> <pre><code># Submit first job\n$ sbatch preprocess.sh\nSubmitted batch job 12345\n</code></pre> <pre><code># Submit dependent job (afterok = only if first job succeeds)\n$ sbatch --dependency=afterok:12345 analyze.sh\nSubmitted batch job 12346\n</code></pre> <p>In this example:</p> <ul> <li><code>12345</code> is the job ID of the first job.</li> <li>The second job will start only if preprocess.sh exits with code 0 (success).</li> </ul> <p>Chaining Multiple Jobs</p> <pre><code># Submit step 1\n$ sbatch step1.sh          # returns JobID 11111\n\n# Submit step 2 to run after step 1\n$ sbatch --dependency=afterok:11111 step2.sh   # returns JobID 11112\n\n# Submit step 3 to run after step 2\n$ sbatch --dependency=afterok:11112 step3.sh\n</code></pre>"},{"location":"Running_jobs/interactive-jobs/","title":"Interactive Jobs","text":""},{"location":"Running_jobs/interactive-jobs/#overview","title":"Overview","text":"<p>Interactive jobs allow users to directly access a compute node in real time \u2014 as if you were working on your personal computer, but with the power of HPC behind it.</p> <p>Unlike batch jobs, which are queued and run in the background, interactive jobs open a live terminal session on a compute node. This is useful for:</p> <ul> <li>Testing code or software modules</li> <li>Debugging runtime issues</li> <li>Running Jupyter notebooks</li> <li>Exploring the system environment or dependencies</li> </ul> <p>You still need to request resources via Slurm, but instead of submitting a script with sbatch, you request an interactive shell using our <code>interative</code> command.</p>"},{"location":"Running_jobs/interactive-jobs/#the-interactive-command","title":"The <code>interactive</code> Command","text":"<p>We provide a built-in shortcut command, <code>interactive</code>, that allows you to quickly and easily request a session in compute node.</p> <p>The <code>interactive</code> command acts as a convenient wrapper for Slurm\u2019s salloc command. Similar to sbatch, which is used for batch jobs, <code>salloc</code> is specifically designed for interactive jobs. </p> <pre><code>$ interactive -h\nUsage: interactive -a ACCOUNT -q QOS -j JOB_TYPE\nStarts an interactive SLURM job with the required account and QoS settings.\n\nRequired options:\n  -a ACCOUNT       Specify the account to use.\n  -q QOS           Specify the quality of service (QoS).\n  -j JOB_TYPE      Specify the type of job: 'cpu' for CPU jobs or 'gpu' for GPU jobs.\n\nExample: Run an interactive GPU job with the 'test' account and 'test' QoS:\n  /apps/site/bin/interactive -a test -q test -j gpu\n\nThis will launch an interactive job on the 'gpu' partition with the 'test' account and QoS 'test',\nusing 1 GPU, 1 CPU, and a walltime of 1 hour by default.\n\nOptional parameters to modify resources:\n  -n NTASKS        Specify the number of CPU tasks (Default: 1).\n  -t WALLTIME      Specify the walltime in hours (Default: 1).\n  -g GPU           Specify the number of GPUs (Only for GPU jobs, Default: 1).\n  -p PARTITION     Specify the SLURM partition (Default: 'general' for CPU jobs, 'gpu' for GPU jobs).\n\nUse '-h' to display this help message.\n</code></pre> CPU NodesGPU NodesDebug Nodes <pre><code>$ interactive -a $PI_UCID -q standard -j cpu\nJob Type: cpu\nStarting an interactive session with the general partition and 1 core for 01:00:00 of walltime in standard priority\nsrun: job 584280 queued and waiting for resources\nsrun: job 584280 has been allocated resources\n</code></pre> <pre><code>$ interactive -a $PI_UCID -q standard -j gpu\nJob Type: gpu\nStarting an interactive session with the GPU partition, 1 core and 1 GPU for 01:00:00 of walltime in standard priority\nsrun: job 584279 queued and waiting for resources\nsrun: job 584279 has been allocated resources\n</code></pre> <pre><code>$ interactive -a $PI_UCID -q debug -j cpu -p debug\nJob Type: cpu\nStarting an interactive session with the debug partition and 1 core for 01:00:00 of walltime in debug priority\nsrun: job 584281 queued and waiting for resources\nsrun: job 584281 has been allocated resources\n</code></pre> <p>Replace <code>$PI_UCID</code> with PI's NJIT UCID.  Now, once you get the confirmation of job allocation, you will be assigned to a compute node.</p>"},{"location":"Running_jobs/interactive-jobs/#customizing-your-resources","title":"Customizing Your Resources","text":"<p>Please note that, by default, this interactive session will request 1 core (for CPU jobs), 1 GPU (for GPU jobs), with a 1-hour walltime. To customize the resources, use the <code>-h</code> option for help. Run <code>interactive -h</code> for more details. Here is an explanation of each flag given below.</p> Flag Explanation Example <code>-a</code> Mandatory option. This is followed by your group's name. Use <code>quota_info</code> to check the account/group name. <code>-a $PI_UCID</code> <code>-q</code> Mandatory option. Used to access the priority <code>-q standard</code> <code>-j</code> Mandatory option. Specify whether you want CPU or GPU node. <code>-j cpu</code> <code>-n</code> Optional. The total number of CPUs. Default is 1 core unless specified. <code>-n 1</code> <code>-t</code> Optional. The amount of walltime to reserve for your job in hours. Default is 1 hour unless specified. <code>-t 1</code> <code>-g</code> Optional. The total number of GPUs. Default is 1 GPU unless specified. <code>-g 1</code> <code>-p</code> Optional. Specify the name of the partition. (Default: <code>general</code> for CPU jobs, <code>gpu</code> for GPU jobs). <code>-p debug</code> <p>Warning</p> <p>Login nodes are not designed for running computationally intensive jobs. You can use the head node to edit and manage your files, or to run small-scale interactive jobs. The CPU usage is limited per user on the head node. Therefore, for serious computing either submit the job using <code>sbatch</code> command or start an interactive session on the compute node.</p> <p>Note</p> <p>Please note that if you are using GPUs, check whether your script is parallelized. If your script is not parallelized and only depends on GPU, then you don't need to request more cores per node. In that case, do not use <code>-n</code> while executing the <code>interactive</code> command, as the default option will request 1 CPU per GPU. It's important to keep in mind that using multiple cores on GPU nodes may result in unnecessary CPU hour charges. Additionally, implementing this practice can make service unit accounting significantly easier.</p>"},{"location":"Running_jobs/job_limitation/","title":"Job Limitations","text":""},{"location":"Running_jobs/job_limitation/#overview","title":"Overview","text":"<p>Wulver, like most shared HPC clusters, enforces certain job limitations to ensure fair and efficient resource usage among all users. These limitations are configured through Slurm and can affect the number of jobs, runtime, memory, GPU usage, and priority of execution. Understanding these limits can help you plan better, reduce job failures, and avoid unintentional misuse.</p>"},{"location":"Running_jobs/job_limitation/#general-limitations-of-job-scheduling","title":"General limitations of job scheduling","text":"<ul> <li> <p>Walltime Limits: <code>standard</code>, <code>low</code>, and <code>high</code> QoS: maximum 72 hours (3 days); <code>debug</code> partition: maximum 8 hours. More info</p> </li> <li> <p>SUs exhausted: Once your Service Units are exhausted, you can no longer run your jobs on <code>standard</code> or <code>high</code> priority but you can still use <code>low</code>.</p> </li> <li> <p>Job preemption: Jobs running on <code>low</code> priority can be preempted by <code>standard</code> or <code>high</code> priority jobs.</p> </li> <li> <p>Maintenance: During the maintenance downtime, logins will be disabled and all the jobs will be held in scheduler. If you submit your job before maintenance with a walltime overlapping the maintenance period then your job will also be held by scheduler. More info</p> </li> </ul>"},{"location":"Running_jobs/managing-jobs/","title":"Managing and Monitoring Jobs","text":""},{"location":"Running_jobs/managing-jobs/#overview","title":"Overview","text":"<p>Managing and monitoring your jobs effectively helps ensure efficient use of resources and enables quicker debugging when things go wrong. Slurm provides several built-in commands to track job status, usage, and troubleshoot issues.</p> <p>SLURM has numerous tools for monitoring jobs. Below are a few to get started. More documentation is available on the SLURM website.</p> <p>The most common commands are: </p> <ul> <li>List all current jobs: <code>squeue</code></li> <li>Job deletion: <code>scancel [job_id]</code></li> <li>Run a job: <code>sbatch [submit script]</code></li> <li>Run a command: <code>srun &lt;slurm options&gt; &lt;command name&gt;</code></li> </ul>"},{"location":"Running_jobs/managing-jobs/#slurm-user-commands","title":"SLURM User Commands","text":"Task Command Job submission: <code>sbatch [script_file]</code> Job deletion: <code>scancel [job_id]</code> Job status by job: <code>squeue [job_id]</code> Job status by user: <code>squeue -u [user_name]</code> Job hold: <code>scontrol hold [job_id]</code> Job release: <code>scontrol release [job_id]</code> List enqueued jobs: <code>squeue</code> List nodes: <code>sinfo -N OR scontrol show nodes</code> Cluster status: <code>sinfo</code>"},{"location":"Running_jobs/managing-jobs/#use-these-commands-to-manage-and-monitor-your-jobs","title":"Use these commands to manage and monitor your jobs","text":"seffsqueuescancelquota_infocheckqcheckloadslurm_jobid <ul> <li>The <code>seff</code> command is a handy tool to assess how efficiently your job used the requested resources after it has completed. </li> <li>Using <code>seff</code> can help you adjust your future job scripts to request only as much memory/time as truly needed, improving scheduler fairness and reducing wasted resources.</li> </ul> <p><pre><code>$ seff\nUsage: seff [Options] &lt;Jobid&gt;\n       Options:\n       -h    Help menu\n       -v    Version\n       -d    Debug mode: display raw Slurm data\n</code></pre> Example output</p> <pre><code>$ seff 575079\n\nJob ID: 575079\nCluster: wulver\nUser/Group: ls565/ls565\nState: COMPLETED (exit code 0)\nCores: 1\nCPU Utilized: 00:00:19\nCPU Efficiency: 67.86% of 00:00:28 core-walltime\nJob Wall-clock time: 00:00:28\nMemory Utilized: 4.21 MB\nMemory Efficiency: 0.11% of 3.91 GB\n</code></pre> <ul> <li>The <code>squeue</code> command lets you check all the jobs currently running/pending/queued in the Wulver.</li> <li>You can use <code>squeue -u $LOGNAME</code> to check your your jobs in the queue.</li> </ul> <p>Example Output </p> <pre><code>$ squeue\n\nJOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n586871   general start_se    bb474 PD       0:00      1 (ReqNodeNotAvail, Reserved for maintenance)\n587618   general NF2300_G    km876  R      40:02      1 n0072\n587004   general     ens1    sh784  R   18:20:27      1 n0012\n586894   general 13a+H-E1   ab2757  R   23:57:52      1 n0042\n586893   general 13a+H-E1   ab2757  R   23:57:59      1 n0042\n586892   general 13a+H-E1   ab2757  R   23:58:04      1 n0042\n586891   general 13a+H-E1   ab2757  R   23:58:10      1 n0042\n586638   general TS_CPC-c   ab2757  R 1-19:40:55      1 n0097\n585445   general SN4A6CL0   sm3557  R 5-14:51:47      1 n0055\n585442   general SN4A6CL0   sm3557  R 5-14:51:56      1 n0055\n587554   general  dpptest    ea442  R    1:18:59      1 n0094\n587619   general Metionin     zr76  R      40:01      7 n[0071-0076,0094]\n586867   general  AU_mod1     zr76  R 1-00:20:18      9 n[0012,0030,0032,0034,0037,0040,0042,0060-0061]\n586713   general Au_mod3_     zr76  R 1-15:39:10      8 n[0093,0099,0102-0103,0109,0113,0118-0119]\n586615   general Au_mod3_     zr76  R 1-20:57:38     14 n[0022,0034,0044,0050-0051,0055,0084,0096,0113,0116-0119,0121]\n586493   general Au_mod2_     zr76  R 2-00:56:11      5 n[0013-0014,0018-0020]\n586492   general Au_mod2_     zr76  R 2-00:56:41      5 n[0006,0013,0114,0120-0121]\n586432   general     DlPC     pst4  R 2-02:16:21      9 n[0087,0094,0097,0101,0110,0113,0116-0118]\n586290   general     DsPE     pst4  R 2-17:36:10      9 n[0015,0017,0022,0030-0031,0034,0037,0041,0043]\n587612   general     DsPC     pst4  R      50:32      5 n[0065,0070-0071,0094,0113]\n...\n</code></pre> <ul> <li>The <code>scancel</code> command lets you cancel your job. </li> <li>It take your JobID as argument <code>scancel [job_id]</code> </li> </ul> <ul> <li>The <code>quota_info</code> command lets you check your storage space and SUs consumed. More info</li> </ul> <p>Example Output </p> <pre><code>$ quota_info\n\nUsage for account: 2025-summer-wksp-612-kjc59-ls565\n       SLURM Service Units: 42 CPU Hours (of 2500 CPU Hour quota)\n              User ls565 Usage: 42 CPU Hours (of 42 CPU Hours)\n       PROJECT Storage: 0 GB (No quota)\n              User ls565 Usage: 0 GB (No quota)\n       SCRATCH Storage: 0 GB (No quota) \n              User ls565 Usage: 0 GB (No quota)\nUsage for account: kjc59\n       SLURM Service Units: 4162 CPU Hours (of 304719 CPU Hour quota)\n              User ls565 Usage: 2802 CPU Hours (of 4162 CPU Hours)\n       PROJECT Storage: 223 GB (of 2048 GB quota)\n              User ls565 Usage: 19 GB (No quota)\n       SCRATCH Storage: 0 GB (of 10240 GB quota) \n              User ls565 Usage: 0 GB (No quota)\nUsage for account: walsh\n       SLURM Service Units: 1917 CPU Hours (of 302709 CPU Hour quota)\n              User ls565 Usage: 0 CPU Hours (of 1917 CPU Hours)\n       PROJECT Storage: 81 GB (of 2048 GB quota)\n              User ls565 Usage: 0 GB (No quota)\n       SCRATCH Storage: 0 GB (of 10240 GB quota) \n              User ls565 Usage: 0 GB (No quota)\nHOME Storage ls565 Usage: 37 GB (of 50 GB quota)\n</code></pre> <p>The <code>checkq</code> command gives same output as <code>squeue</code> with extra details.</p> <p>Example Output </p> <pre><code>$ checkq\n\nJOBID    PARTITION     NAME     USER         ACCOUNT      STATE       TIME           START_TIME          SUBMIT_TIME   TIME_LIMIT CPUS NODE     NODELIST(REASON)     PRIORITY                  QOS\n586637      general TS_CPC-t   ab2757        champagn    PENDING       0:00                  N/A  2025-08-25T16:47:24   5-00:00:00   16    1 (ReqNodeNotAvail, Re        11419        high_champagn\n586636      general TS_CPC-c   ab2757        champagn    PENDING       0:00                  N/A  2025-08-25T16:47:19   5-00:00:00   16    1 (ReqNodeNotAvail, Re        11419        high_champagn\n587559      general   DLPE_8     pst4             cld    RUNNING 1-03:34:02  2025-08-27T11:11:51  2025-08-27T11:11:35   1-18:00:00  128    2         n[0020-0021]        10205                  low\n587560      general   DLPE_9     pst4             cld    RUNNING 1-03:34:02  2025-08-27T11:11:51  2025-08-27T11:11:49   1-18:00:00  128    4 n[0021,0031-0032,0035]        10205                  low\n587557      general   DLPE_6     pst4             cld    RUNNING 1-03:34:32  2025-08-27T11:11:21  2025-08-27T11:11:02   1-18:00:00  128    4    n[0057,0060-0062]        10205                  low\n587558      general   DLPE_7     pst4             cld    RUNNING 1-03:34:32  2025-08-27T11:11:21  2025-08-27T11:11:14   1-18:00:00  128    6 n[0007,0012,0015,0017,0020,0062]        10205                  low\n587556      general   DLPE_5     pst4             cld    RUNNING 1-03:35:02  2025-08-27T11:10:51  2025-08-27T11:10:34   1-18:00:00  128    3    n[0051,0055,0057]        10205                  low\n587555      general   DLPE_4     pst4             cld    RUNNING 1-03:35:32  2025-08-27T11:10:21  2025-08-27T11:10:19   1-18:00:00  128    2         n[0050-0051]        10205                  low\n587533      general   DLPE_3     pst4             cld    RUNNING 1-03:57:47  2025-08-27T10:48:06  2025-08-27T10:47:49   1-18:00:00  128    2         n[0012,0015]        10205                  low\n587532      general   DLPE_2     pst4             cld    RUNNING 1-03:58:17  2025-08-27T10:47:36  2025-08-27T10:47:34   1-18:00:00  128    2         n[0011-0012]        10205                  low\n587528      general   DLPE_1     pst4             cld    RUNNING 1-04:00:19  2025-08-27T10:45:34  2025-08-27T10:45:23   1-18:00:00  128    2         n[0007,0011]        10205                  low\n586819      general   DSPC_c     pst4             cld    RUNNING 2-04:38:34  2025-08-26T10:07:19  2025-08-26T10:07:18   2-20:00:00  128    6 n[0087,0096-0097,0100-0102]        10205                  low\n586720      general  Rstudio     hf78          zhiwei    RUNNING 2-17:32:06  2025-08-25T21:13:47  2025-08-25T21:08:18   3-00:00:00  128    1                n0033        10205                  low\n587945      general  vs-code    au252          amr239    RUNNING    3:25:42  2025-08-28T11:20:11  2025-08-28T11:19:41     13:00:00   32    1                n0062        10203                  low\n586248          gpu      dif   jl2356           wangj    PENDING       0:00                  N/A  2025-08-24T17:04:29  10-00:00:00   64    1 (ReqNodeNotAvail, Re        11561           high_wangj\n586839          gpu    sim_t     zw37           wangj    PENDING       0:00                  N/A  2025-08-26T10:46:29   3-00:00:00    1    1 (ReqNodeNotAvail, Re        10911             standard\n587851          gpu  gmx2023   ks2297             vak    PENDING       0:00                  N/A  2025-08-27T21:00:03   3-00:00:00    2    1 (ReqNodeNotAvail, Re        10708             standard\n587856          gpu  gmx2023   ks2297             vak    PENDING       0:00                  N/A  2025-08-27T21:43:31   3-00:00:00    2    1 (ReqNodeNotAvail, Re        10703             standard\n587912          gpu ::app=Ju    aad94           tyson    PENDING       0:00                  N/A  2025-08-28T02:04:23   3-00:00:00    4    1 (ReqNodeNotAvail, Re        10678             standard\n587964          gpu ::app=Ju    tb439          geller    PENDING       0:00                  N/A  2025-08-28T12:34:36   1-00:00:00   16    1 (ReqNodeNotAvail, Re        10615             standard\n588002          gpu ::app=Sp   ap2934             mx6    PENDING       0:00                  N/A  2025-08-28T14:12:43     20:00:00   10    1 (ReqNodeNotAvail, Re        10605             standard\n586249          gpu      dif   jl2356           wangj    RUNNING 3-21:41:09  2025-08-24T17:04:44  2025-08-24T17:04:44   4-04:00:00   66    1                n0046        11004           high_wangj\n587881          gpu ::app=Ju   jc2687           bs644    RUNNING   15:40:15  2025-08-27T23:05:38  2025-08-27T23:05:24   1-00:00:00   48    1                n0089        11003           high_bs644\n587985          gpu     bash    nk569            phan    RUNNING      42:50  2025-08-28T14:03:03  2025-08-28T14:03:03     10:00:00    5    1                n0048        11002            high_phan\n...\n</code></pre> <p>The <code>checkload</code> command gives you the cpu load on each node. </p> <p>Example Output </p> <pre><code>$ checkload\n\nNODELIST   NODES PARTITION       STATE CPUS    S:C:T   MEMORY   CPU_LOAD    TIMELIMIT\n   n0001       1       gpu       mixed  128   2:64:1   514000       6.08     infinite\n   n0002       1       gpu   allocated  128   2:64:1   514000       3.97     infinite\n   n0003       1       gpu       idle~  128   2:64:1   514000       0.00     infinite\n   n0004       1       gpu       idle~  128   2:64:1   514000       0.00     infinite\n   n0005       1       gpu       idle~  128   2:64:1   514000       0.00     infinite\n   n0006       1  general*       idle~  128   2:64:1   514000       0.00     infinite\n   n0007       1  general*   allocated  128   2:64:1   514000     128.08     infinite\n   n0008       1  general*   allocated  128   2:64:1   514000     128.02     infinite\n   n0009       1  general*   allocated  128   2:64:1   514000     128.06     infinite\n   n0010       1  general*   allocated  128   2:64:1   514000     128.01     infinite\n   n0011       1  general*   allocated  128   2:64:1   514000     128.06     infinite\n   ...\n</code></pre> <p>The <code>slurm_jobid [job_id]</code> command lets you check detailed info about your job. It requires your job_id as parameter.</p> <p>Example Output </p> <pre><code>$ slurm_jobid 588032\n\n\n********************************************************************************************************************************************************************************************************************\n                                                               _   _      _  ___  _____      _     ____    ____  ____    _   _  ____    ____ \n                                                               | \\ | |    | ||_ _||_   _|    / \\   |  _ \\  / ___|/ ___|  | | | ||  _ \\  / ___|\n                                                               |  \\| | _  | | | |   | |     / _ \\  | |_) || |    \\___ \\  | |_| || |_) || |    \n                                                               | |\\  || |_| | | |   | |    / ___ \\ |  _ &lt; | |___  ___) | |  _  ||  __/ | |___ \n                                                               |_| \\_| \\___/ |___|  |_|   /_/   \\_\\|_| \\_\\ \\____||____/  |_| |_||_|     \\____|\n\n\n********************************************************************************************************************************************************************************************************************\n\nJob ID is: 588032\n\nTotal SU consumed: 0.0\n\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n\u2502 User  \u2502 Account \u2502 Partition \u2502   QOS    \u2502 Elapsed  \u2502  State  \u2502 SU  \u2502 NodeList \u2502 NCPUS \u2502     Start      \u2502   End   \u2502 Timelimit \u2502    ReqTRES     \u2502    WorkDir     \u2502   SubmitLine   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ls565 \u2502  kjc59  \u2502  general  \u2502 standard \u2502 00:00:05 \u2502 RUNNING \u2502 0.0 \u2502  n0061   \u2502   1   \u2502 2025-08-28T14: \u2502 Unknown \u2502 00:59:00  \u2502 billing=1,cpu= \u2502 /mmfs1/home/ls \u2502 sbatch test.sh \u2502\n\u2502       \u2502         \u2502           \u2502          \u2502          \u2502         \u2502     \u2502          \u2502       \u2502     41:07      \u2502         \u2502           \u2502 1,mem=4000M,no \u2502      565       \u2502                \u2502\n\u2502       \u2502         \u2502           \u2502          \u2502          \u2502         \u2502     \u2502          \u2502       \u2502                \u2502         \u2502           \u2502      de=1      \u2502                \u2502                \u2502\n\u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b\n</code></pre> <p>Info</p> <p>Please keep checking your job's status using <code>squeue -u $LOGNAME</code> so that it doesn't stay pending due to incorrect job submission parameters.</p> <p>Warning</p> <p>You can only cancel your jobs. Please don't try to cancel other users jobs.</p>"},{"location":"Running_jobs/node-memory-config/","title":"Nodes and Memory","text":""},{"location":"Running_jobs/node-memory-config/#overview","title":"Overview","text":"<p>Before submitting your job to the scheduler, it's important to know how many cores and memory your task requires and all this will be assigned based on the number of nodes you request.</p>"},{"location":"Running_jobs/node-memory-config/#partition-use-partition","title":"Partition (Use <code>--partition</code>)","text":"<p>Wulver has three partitions, differing in GPUs or RAM available:</p> Partition Nodes Cores per Node CPU GPU Memory <code>--partition=general</code> 100 128 2.5G GHz AMD EPYC 7763 (2) NA 512 GB <code>--partition=debug</code> 1 4 2.5G GHz AMD EPYC 7763 (2) NA 16 GB <code>--partition=debug_gpu</code> 1 4 2.0 GHz AMD EPYC 7713 (2) MIG (10g, 20g, 40g) 16 GB <code>--partition=gpu</code> 25 128 2.0 GHz AMD EPYC 7713 (2) NVIDIA\u00a0A100\u00a0GPUs\u00a0(4);MIG (10g, 20g, 40g) 512 GB <code>--partition=bigmem</code> 2 128 2.5G GHz AMD EPYC 7763 (2) NA 2 TB"},{"location":"Running_jobs/node-memory-config/#priority-use-qos","title":"Priority (Use <code>--qos</code>)","text":"<p>Wulver has three levels of \u201cpriority\u201d, utilized under SLURM as Quality of Service (QoS):  Qos Purpose Rules Wall time limit (hours) Valid Users <code>--qos=standard</code> Normal jobs. Faculty PIs are allocated 300,000 Service Units (SU) per year SU charges based on node type (see SU, jobs can be preempted by high QoS enqueued jobs 72 Everyone <code>--qos=low</code> Free access, no SU charge Jobs can be preempted by high or standard QoS enqueued jobs 72 Everyone <code>--qos=high_$PI</code> Replace <code>$PI</code> with the UCID of PI, only available to owners/investors Highest Priority Jobs, no SU Charges. 72 owner/investor PI Groups <code>--qos=debug</code> Intended for debugging and testing jobs No SU Charges, maximum 4 CPUs and 16G Mem is allowed, must be used with <code>--partition=debug</code> or <code>debug_gpu</code> 8 Everyone </p>"},{"location":"Running_jobs/node-memory-config/#how-many-cores-and-memory-do-i-need","title":"How many cores and memory do I need?","text":"<p>There is no deterministic method of finding the exact amount of memory needed by a job in advance. A good practice is to overestimate it slightly and then scale down based on previous runs. Significant overestimation, however, can lead to inefficiency of system resources and unnecessary expenditure of CPU time allocations.</p> <p>We have tool <code>seff</code> in Slurm which you can use to check how much resources your job consumes and based on that re-adjust the configurations.</p> <p>Understanding where your code is spending time or memory is key to efficient resource usage. Profiling helps you answer questions like:</p> <ul> <li>Am I using too many CPU cores without benefit?</li> <li>Is my job memory-bound or I/O-bound?</li> <li>Are there inefficient loops, repeated operations, or unused computations?</li> </ul>"},{"location":"Running_jobs/node-memory-config/#tips-for-optimization","title":"Tips for optimization","text":"<ul> <li>Use multi-threaded or parallel libraries (OpenMP, MPI, NumPy with MKL).</li> <li>Avoid unnecessary data copying or large in-memory objects.</li> <li>Stream large files instead of loading entire datasets into memory.</li> <li>Use job arrays for independent jobs instead of looping in one script.</li> </ul>"},{"location":"Running_jobs/node-memory-config/#be-careful-about-invalid-configuration","title":"Be careful about invalid configuration","text":"<p>Misconfigured job submissions can lead to job failures, wasted compute time, or inefficient resource usage.  Below are some common mistakes and conflicts to watch out for when submitting jobs to SLURM:</p> <ul> <li>Asking for more CPUs, memory, or GPUs than any node in the cluster can offer. Job stays in pending state indefinitely with reason like <code>ReqNodeNotAvail or Resources</code></li> <li>Mismatch Between CPUs and Tasks. For example: Using <code>--ntasks=4</code> and <code>--cpus-per-task=8</code> but your script is single-threaded. You're blocking 32 cores but using only 1 effectively \u2014 leads to very low CPU efficiency.</li> <li>Specifying walltime of more then 3 days is not allowed. </li> <li>Submitting to a partition that doesn\u2019t match your job type. For eg. Requesting a GPU with a non-GPU partition: <code>--partition=standard --gres=gpu:1</code>. Job will fail immediately or be held forever.</li> </ul>"},{"location":"Running_jobs/ondemand-jobs/","title":"Jobs on OnDemand","text":"<p>In addition to submitting jobs via batch scripts and interactive sessions, Wulver also supports Open OnDemand, a browser-based portal for job management.</p>"},{"location":"Running_jobs/ondemand-jobs/#with-ondemand-users-can","title":"With OnDemand, users can:","text":"<ul> <li>Submit and monitor jobs through a graphical interface</li> <li>Create and manage job templates without writing SLURM scripts manually</li> <li>Access interactive applications such as JupyterLab, Matlab, RStudio and many more directly in the browser</li> <li>Launch remote desktops on cluster nodes for GUI-based workflows</li> <li>Get access to browser based shell terminal to access Wulver</li> </ul>"},{"location":"Running_jobs/ondemand-jobs/#ondemand-is-ideal-for","title":"OnDemand is ideal for:","text":"<ul> <li>New users who are unfamiliar with Linux and SLURM commands</li> <li>Researchers who prefer a graphical interface</li> <li>Anyone who wants quick access to interactive HPC applications</li> </ul> <p>Info</p> <p>See the Open OnDemand page for full documentation.</p>"},{"location":"Running_jobs/problems-and-misconceptions/","title":"Overview","text":"<p>New HPC users often assume that requesting more resources (CPUs, GPUs, memory) will automatically make their jobs run faster. In reality, performance depends on how the software is written and configured. Submitting jobs with incorrect resource requests can result in wasted allocations, slower performance, and unnecessary load on shared compute nodes. Below are some common mistakes and their solutions.</p>"},{"location":"Running_jobs/problems-and-misconceptions/#misconception-if-i-allocate-more-cpus-my-software-will-automatically-use-them","title":"Misconception: \u201cIf I allocate more CPUs, my software will automatically use them\u201d","text":"<p>Many applications are not parallelized by default. Requesting multiple CPUs (--ntasks &gt; 1) will not speed up execution unless your software is explicitly written to take advantage of parallelism (e.g., via MPI, OpenMP, or job arrays). Otherwise, the job may simply run the program multiple times in parallel instead of speeding it up.</p> <p>Example of incorrect job script:</p> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=python\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=general\n#SBATCH --ntasks=4\n#SBATCH --qos=standard\n#SBATCH --time=30:00\n#################################################################################\nmodule load foss/2024a Python\nsrun python test.py\n</code></pre> <p>Problem: This script launches test.py 4 times since no parallelism is enabled in the code.</p> <p>Solution: If your code is serial, request only 1 task: <code>srun -n1 python test.py</code> or <code>python test.py</code></p> <p>To truly leverage multiple CPUs, use parallel programming libraries such as mpi4py and Parsl</p>"},{"location":"Running_jobs/problems-and-misconceptions/#misconception-my-jobs-run-slower-when-i-request-more-resources","title":"Misconception: \u201cMy jobs run slower when I request more resources\u201d","text":"<p>Requesting excessive resources can actually degrade performance. For example, oversubscribing CPUs (assigning more threads than available cores) leads to CPU contention, slowing down computations.</p> <p>Example of problematic job script:</p> <pre><code>#!/bin/bash -l\n#SBATCH -J gmx-test\n#SBATCH -o %x.%j.out\n#SBATCH -e %x.%j.err\n#SBATCH --partition=gpu\n#SBATCH --qos=standard\n#SBATCH --time 72:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128\n#SBATCH --gres=gpu:4\n#################################################################################\nmodule purge\nmodule load wulver\nmodule load foss/2025a GROMACS\ngmx grompp -f run.mdp -c npt2.gro -r npt2.gro -p topol.top -o run.tpr\nsrun gmx_mpi mdrun -deffnm run -cpi run.cpt -v -ntomp 2 -pin on -tunepme -dlb yes -noappend\n</code></pre> <p>Problem: With <code>--ntasks-per-node=128</code> and <code>-ntomp 2</code>, the job requests 256 CPUs, but the node only has 128. This overloads the node and slows down execution.</p> <p>Solution: Match resource requests to the available hardware. For example:</p> <p>This job will launch using 64 cores with 2 threads per core.</p> <pre><code>#!/bin/bash -l\n#SBATCH -J gmx-test\n#SBATCH -o %x.%j.out\n#SBATCH -e %x.%j.err\n#SBATCH --partition=gpu\n#SBATCH --qos=standard\n#SBATCH --time=72:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64\n#SBATCH --cpus-per-task=2\n#SBATCH --gres=gpu:4\n#################################################################################\nmodule purge\nmodule load wulver\nmodule load foss/2025a GROMACS\ngmx grompp -f run.mdp -c npt2.gro -r npt2.gro -p topol.top -o run.tpr\nsrun gmx_mpi mdrun -deffnm run -cpi run.cpt -v -ntomp 2 -pin on -tunepme -dlb yes -noappend\n</code></pre> <p>Tips</p> <ul> <li>Use the checkload command to monitor CPU usage.</li> <li>Cancel jobs that overload nodes and adjust scripts accordingly.</li> <li>Align --ntasks, --cpus-per-task, and application threading flags (-ntomp, OMP_NUM_THREADS, etc.) with actual node architecture.</li> </ul>"},{"location":"Running_jobs/service-units/","title":"Service Units (SU)","text":""},{"location":"Running_jobs/service-units/#overview","title":"Overview","text":"<p>Service Units (SUs) are the core accounting mechanism used to track and allocate compute usage on Wulver. Each job you run consumes a certain number of SUs based on the resources you request and the duration of your job.</p> <p>SUs help us ensure fair usage of the HPC system and monitor consumption across different users, departments, or projects.</p> <p>Since resources are limited, each PI's research account is allocated 300,000 Service Units (SUs) per year upon request at no cost. These SUs can be used via the standard priority on the SLURM job scheduler.  One SU is defined as 1 CPU hour or 4 GB of RAM per hour. </p> <p>Important information on SU</p> <ul> <li>The SU allocation is per PI account, not per individual student.</li> <li>The allocation resets each fiscal year.</li> <li>Students are expected to use SUs efficiently, as excessive usage may deplete their group's SU balance quickly.</li> </ul> <p>If a group exhausts its SU allocation early, the PI has the option to purchase additional SUs or leverage higher priority queues through investment. For more details, refer to the  Wulver Policies and Condo Policies. Check the table below to see how SU will be charged for different partitions. </p> Partition Service Unit (SU) Charge <code>--partition=general</code> MAX(CPUs, RAM/4G) SU <code>--partition=gpu</code> 16 * (GPU Memory requested)/80G + MAX(CPUs, RAM/4G) SU <code>--partition=bigmem</code> MAX(1.5 *CPUs, 1.5 *RAM/16G) SU <code>--partition=debug</code> No charges, must be used with <code>--qos=debug</code> <code>--partition=debug_gpu</code> No charges, must be used with <code>--qos=debug</code> <p>Memory request via job scheduler</p> <p>Please note that in the above SU calculation, MAX(CPUs, RAM/4GB) in the <code>general</code> and <code>gpu</code> partition \u2014 this represents the maximum of the number of CPUs requested and the memory requested divided by 4GB. in <code>bigmem</code> it's 16 GB.</p> <ul> <li>If you do not specify <code>--mem</code> in your SLURM job script, the job will be allocated the default 4GB of memory per core (16GB/core in <code>bigmem</code>), and you will be charged based on the number of CPUs requested.</li> <li>If you do specify <code>--mem</code> to request more memory, the SU charge will be based on the maximum of CPU count and memory/4GB (memory/16GB in <code>bigmem</code>). Requesting more memory than the default will result in higher SU charges than if you had only specified CPUs.</li> </ul> <p>Therefore, please be mindful of the <code>--mem</code> setting. Requesting more memory than necessary can significantly increase your SU usage.</p> Example of SU Charges <code>general</code> Partition<code>bigmem</code> Partition<code>gpu</code> Partition SLURM Directive SU Explanation 4 CPUs MAX(4, 4*4G/4G) = 4 Since no memory requiremnt is specified, SU is charged based on the same number of CPUs 4 CPUs + <code>--mem=64G</code> MAX(4, 64G/4G) = 16 Since 64G memory is specified, the MAX function the evaluates the maximum of 4 CPUS, and 64G/4G= 16, resulting in a charge of 16 SUs 4 CPUs + <code>--mem=4G</code> MAX(4, 4G/4G) = 4 MAX function the evaluates the maximum of 4 CPUS, and 4G/4G= 1, resulting in a charge of 4 SUs 4 CPUs + <code>--mem-per-cpu=8G</code> MAX(4, 8G*4/4G) = 8 MAX function the evaluates the maximum of 4 CPUS, and 8G*4CPUs/4G = 8 , resulting in a charge of 8 SUs SLURM Directive SU Explanation 4 CPUs MAX(4*1.5, 1.5*4*16G/16G) = 6 On <code>bigmem</code> partition the usage factor is 1.5 4 CPUs + <code>--mem=64G</code> MAX(4*1.5, 1.5*64G/16G) = 6 Since 64G memory is specified, the MAX function the evaluates the maximum of 4*1.5= 6 SUs, and 1.5*64G/16G= 6 SUs, resulting in a charge of 6 SUs 4 CPUs + <code>--mem=128G</code> MAX(4*1.5, 1.5*128G/16G) = 12 MAX function the evaluates the maximum of 4*1.5= 6 SUs, and 1.5*128G/16G= 12 SU, resulting in a charge of 12 SUs 4 CPUs + <code>--mem-per-cpu=8G</code> MAX(4*1.5, 1.5*8G*4/16G) = 6 MAX function the evaluates the maximum of 4*1.5= 6 SUs, and 1.5*8G*4CPUs/16G = 3 SUs , resulting in a charge of 6 SUs SLURM Directive SU Explanation 4 CPUs + 10MIG MAX(4, 4*4G/4G) + 16 * (10G/80G) = 6 Since no memory requiremnt is specified, SU is charged based on the same number of CPUs and 10G of GPU memory 4 CPUs + 20MIG MAX(4, 4*4G/4G) + 16 * (20G/80G) = 8 SU is charged based on the same number of CPUs and 20G of GPU memory 4 CPUs + 40MIG MAX(4, 4*4G/4G) + 16 * (40G/80G) = 12 SU is charged based on the same number of CPUs and 40G of GPU memory 4 CPUs + Full GPU MAX(4, 4*4G/4G) + 16 * (80G/80G) = 20 SU is charged based on the same number of CPUs and 80G of GPU (A full GPU) memory 4 CPUs + <code>--mem=64G</code> + Full GPU MAX(4, 64G/4G) + 16 * (80G/80G) = 32 The MAX function evaluates the maximum of 4 SUs (from CPUs), and 64G/4G= 16 SUs (from memory). In addition, 16 SUs are charged from 80G of GPU (A full GPU) memory, bringing the total SU charge to 32 SUs 4 CPUs + <code>--mem-per-cpu=8G</code> + Full GPU MAX(4, 4*8G/4G) + 16 * (80G/80G) = 24 The MAX function the evaluates the maximum of 4 SUs (from CPUs), and 4*8G/4G= 8 SUs (from memory). In addition, 16 SUs are charged from 80G of GPU (A full GPU) memory, bringing the total SU charge to 24 SUs"},{"location":"Running_jobs/service-units/#check-quota","title":"Check Quota","text":"<p>Users can check their account(s) SU utilization and storage usage via <code>quota_info UCID</code> command. <pre><code>[ab1234@login01 ~]$ module load wulver\n[ab1234@login01 ~]$ quota_info $LOGNAME\nUsage for account: xy1234\n   SLURM Service Units (CPU Hours): 277557 (300000 Quota)\n     User ab1234 Usage: 1703 CPU Hours (of 277557 CPU Hours)\n   PROJECT Storage: 867 GB (of 2048 GB quota)\n     User ab1234 Usage: 11 GB (No quota)\n   SCRATCH Storage: 791 GB (of 10240 GB quota)\n     User ab1234 Usage: 50 GB (No quota)\nHOME Storage ab1234 Usage: 0 GB (of 50 GB quota)\n</code></pre> Here, <code>xy1234</code> represents the UCID of the PI, and \"SLURM Service Units (CPU Hours): 277557 (300000 Quota)\" indicates that members of the PI group have already utilized 277,557 CPU hours out of the allocated 300,000 SUs, and the user <code>xy1234</code> utilized 1703 CPU Hours out of 277,557 CPU Hours. This command also displays the storage usage of directories such as <code>$HOME</code>, <code>/project</code>, and <code>/scratch</code>. Users can view both the group usage and individual usage of each storage. In the given example, the group usage from the 2TB project quota is 867 GB, with the user's usage being 11 GB out of that 867 GB. For more details file system quota, see Wulver Filesystem.</p>"},{"location":"Services/hpc-services/","title":"HPC Service Catalog","text":"<p>The NJIT High Performance Computing (HPC) facility provides a wide range of services to support research, teaching, and computation across disciplines. This catalog outlines the core services available to students, faculty, and researchers.</p>"},{"location":"Services/hpc-services/#cluster-computing","title":"Cluster Computing","text":"<p>Built by Dell, our computing cluster Wulver provides over 197 compute nodes to support computationally intensive workloads across various academic and research domains. Users can run simulations, data processing, and large-scale modeling efficiently using Wulver\u2019s distributed computing infrastructure.</p> <p>Learn more: Wulver</p>"},{"location":"Services/hpc-services/#research-data-storage","title":"Research Data Storage","text":"<p>Wulver offers high-performance, multi-tier storage optimized for research. Each user gets access to dedicated directories for computation, collaboration, and long-term storage.</p> <ul> <li><code>/home</code> \u2013 Personal files and configurations (50 GB quota).</li> <li><code>/project</code> \u2013 Primary workspace for research data (2 TB per PI group).</li> <li><code>/research</code> \u2013 Long-term archival storage, available for purchase by PIs ($100 / TB for five years).</li> </ul> <p>Learn more: File Systems and Storage on Wulver</p>"},{"location":"Services/hpc-services/#education","title":"Education","text":"<p>The HPC facility supports course integration and instructional use, offering dedicated partitions and allocations for coursework that require high-performance computing environments. These resources help students gain hands-on experience with parallel computing and data-intensive applications.</p> <p>Learn more: Resources for Coursework</p>"},{"location":"Services/hpc-services/#scientific-software-development","title":"Scientific Software Development","text":"<p>The HPC team provides deep expertise in developing, installing, and optimizing scientific software for Wulver. We ensure that applications are configured for maximum performance in the cluster environment. We provide a wide range of software for research purposes, and you may also request the installation of additional software, subject to our guidelines and approval.</p> <p>Learn more: Available Software List</p>"},{"location":"Services/hpc-services/#hpc-facilitation-service","title":"HPC Facilitation Service","text":"<p>Our facilitation team empowers users to perform essential research computing projects through training, webinars, one-on-one consultation, and effective user support. We also have in person office hours held by our student interns.</p> <p>Learn more: Contact the HPC Facilitation Team</p> <p>Reach out to us</p> <p>For any inquiries or assistance with HPC resources, please reach out to us through our email: hpc@njit.edu</p>"},{"location":"Software/","title":"Software Environment","text":"<p>All software and numerical libraries available at the cluster can be found at <code>/apps/easybuild/software/</code>. We use EasyBuild to install, build and manage different version of packages. </p> <p>If you could not find software or libraries on HPC cluster, please submit a request for HPC Software Installation by visiting the Service Catalog. The list of installed software or packages on HPC cluster can be found in Software List.</p> <p>Warning</p> <p>When installing software on Wulver, please do not use <code>sudo</code> under any circumstances. The use of <code>sudo</code> is restricted to system administrators and is not permitted in user environments for security and stability reasons. Users should install software in their own directories using tools like conda, pip, or by compiling from source in their home or project space. You may come across the use of <code>sudo</code> in some software installation instructions, but please note that these are intended for personal computers only. If you require assistance with installation of a package, please contact the HPC support team or build the package in an apptainer container.</p>"},{"location":"Software/#modules","title":"Modules","text":"<p>We use Environment Modules to manage the user environment in HPC, which help users to easily load and unload software packages, switch between different versions of software, and manage complex software dependencies. Lmod is an extension of the Environment Modules system, implemented in Lua. It enhances the functionality of traditional Environment Modules by introducing features such as hierarchical module naming, module caching, and improved flexibility in managing environment variables.</p>"},{"location":"Software/#search-for-specific-package","title":"Search for Specific Package","text":"<p>You can check specific packages and list of their versions using <code>module spider</code>. For example, the list of Python installed on cluster can be checked by using</p> <p><pre><code>module spider Python\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n\n     Versions:\n        Python/2.7.18-bare\n        Python/3.9.6-bare\n        Python/3.9.6\n        Python/3.10.4-bare\n        Python/3.10.4\n        Python/3.10.8-bare\n        Python/3.10.8\n        Python/3.11.5\n     Other possible modules matches:\n        Biopython  Boost.Python  Python-bundle-PyPI  meson-python  python3  python39\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*Python.*'\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"Python\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider Python/3.11.5\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> This will show the different versions of Python available on Wulver. </p> <p>To see how to load the specific version of software (for example <code>Python/3.9.6</code>), the following command needs to be used.</p> <p><pre><code>module spider Python/3.9.6\n</code></pre> This will show the which prerequisite modules need to be loaded prior to loading <code>Python/3.9.6</code></p> <p><pre><code>---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  Python: Python/3.9.6\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n\n\n    You will need to load all module(s) on any one of the lines below before the \"Python/3.9.6\" module is available to load.\n\n      easybuild  GCCcore/11.2.0\n      slurm/wulver  GCCcore/11.2.0\n\n    Help:\n      Description\n      ===========\n      Python is a programming language that lets you work more quickly and integrate your systems\n       more effectively.\n\n\n      More information\n      ================\n       - Homepage: https://python.org/\n</code></pre> If you are unsure about the version, you can also use <code>module spider Python</code> to see the different versions of Python and prerequisite modules to be loaded. </p>"},{"location":"Software/#load-modules","title":"Load Modules","text":"<p>To use specific package, you need to use <code>module load</code> command which modified the environment to load the software package(s).</p> <p>Note</p> <ul> <li>The <code>module load</code> command will load dependencies automatically as needed, however you may still need to load prerequisite modules to load specific software package(s). For that you need to use <code>module spider</code> command as described above.</li> <li>For running jobs via batch script, you need to add module load command(s) to your submission script.</li> </ul> <p>For example, to load <code>Python</code> version <code>3.9.6</code> as shown in the above example, you need to load <code>GCCcore/11.2.0</code> module first before loading the Python module is available to load. To use <code>Python 3.9.6</code>, use the following command <pre><code>module load GCCcore/11.2.0 Python\n</code></pre> You can verify whether Python is loaded using</p> <p><pre><code>module li\n</code></pre> and this will result is the following output <pre><code>Currently Loaded Modules:\n  1) easybuild   2) wulver   3) slurm/wulver   4) null   5) GCCcore/11.2.0   6) zlib/1.2.11   7) binutils/2.37   8) bzip2/1.0.8   9) ncurses/6.2  10) libreadline/8.1  11) Tcl/8.6.11  12) SQLite/3.36  13) XZ/5.2.5  14) GMP/6.2.1  15) libffi/3.4.2  16) OpenSSL/1.1  17) Python/3.9.6\n</code></pre></p>"},{"location":"Software/#module-unload","title":"Module unload","text":"<p>To unload a specific module that you've previously loaded: <pre><code>module unload Python\n</code></pre> You can unload all modules at once with <pre><code>module purge\n</code></pre></p>"},{"location":"Software/#module-save-collections","title":"Module Save Collections","text":"<p>Since some package(s) require to load prerequisite modules to load, every time it might be inconvenient to users to load those modules everytime. Therefore, you can save those modules in particular environment after loading those modules. For example <pre><code>module save environment_name\n</code></pre> This will save the collection of  modules in <code>environment_name</code>. </p>"},{"location":"Software/#module-list-collections","title":"Module List Collections","text":"<p>To get a list of your collections, run: <pre><code>module savelist\n</code></pre></p>"},{"location":"Software/#module-command-summary","title":"Module Command Summary","text":"<p>Here are the list of <code>module</code> commands</p> Command Description <code>module list</code> Show active modules loaded in the user environment <code>module av [module]</code> Show list of available modules in MODULEPATH <code>module spider [module]</code> Query all modules in MODULEPATH and any module hierarchy <code>module overview [module]</code> List all modules with count of each module <code>module load [module]</code> Load a module file in the users environment <code>module unload [module]</code> Remove a loaded module from the user environment <code>module purge</code> Remove all modules from the user environment <code>module swap [module1] [module2]</code> Replace module1 with module2 <code>module show [module]</code> Show content of commands performed by loading module file <code>module --raw show [module]</code> Show raw content of module file <code>module help [module]</code> Show help for a given module <code>module whatis [module]</code> A brief description of the module, generally single line <code>module savelist</code> List all user collections <code>module save [collection]</code> Save active modules in a user collection <code>module describe [collection]</code> Show content of user collection <code>module restore [collection]</code> Load modules from a collection <code>module disable [collection]</code> Disable a user collection <code>module --config</code> Show Lmod configuration <code>module use [-a] [path]</code> Prepend or Append path to MODULEPATH <code>module unuse [path]</code> Remove path from MODULEPATH <code>module --show_hidden av</code> Show all available modules in MODULEPATH including hidden modules <code>module --show_hidden spider</code> Show all possible modules in MODULEPATH and module hierarchy including hidden modules"},{"location":"Software/#further-reading","title":"Further Reading","text":"<p>You can check the documentation of <code>module</code> on the cluster using the following command: <pre><code>man module\n</code></pre></p>"},{"location":"Software/#software-list","title":"Software List","text":"<p>The following applications are installed on Wulver.</p> Wulver RHEL9Wulver RHEL8 Software Version Dependent Toolchain Module Load Command ABAQUS 2024-hotfix-2405 - <code>module load ABAQUS/2024-hotfix-2405</code> Abseil 20250512.1 foss/2025a <code>module load foss/2025a Abseil/20250512.1</code> Abseil 20240722.0 foss/2024a <code>module load foss/2024a Abseil/20240722.0</code> AFNI 25.1.01 foss/2024a <code>module load foss/2024a AFNI/25.1.01</code> aiohttp 3.12.13 foss/2025a <code>module load foss/2025a aiohttp/3.12.13</code> Altair 2023 - <code>module load Altair/2023</code> AmberTools 25.2 foss/2025a <code>module load foss/2025a AmberTools/25.2</code> ANSYS 2024R1 - <code>module load ANSYS/2024R1</code> ANSYS 2025R1 - <code>module load ANSYS/2025R1</code> AOCL-BLAS 5.0 foss/2025a <code>module load foss/2025a AOCL-BLAS/5.0</code> archspec 0.2.5 foss/2024a <code>module load foss/2024a archspec/0.2.5</code> arpack-ng 3.9.1 foss/2024a <code>module load foss/2024a arpack-ng/3.9.1</code> Arrow 18.0.0 foss/2025a <code>module load foss/2025a Arrow/18.0.0</code> ASE 3.26.0 foss/2025a <code>module load foss/2025a ASE/3.26.0</code> assimp 5.4.3 foss/2024a <code>module load foss/2024a assimp/5.4.3</code> assimp 6.0.2 foss/2025a <code>module load foss/2025a assimp/6.0.2</code> astropy 7.0.0 foss/2024a <code>module load foss/2024a astropy/7.0.0</code> astropy-testing 7.0.0 foss/2024a <code>module load foss/2024a astropy-testing/7.0.0</code> ATK 2.38.0 foss/2024a <code>module load foss/2024a ATK/2.38.0</code> Autoconf 2.72 foss/2025a <code>module load foss/2025a Autoconf/2.72</code> Autoconf 2.72 foss/2024a <code>module load foss/2024a Autoconf/2.72</code> AutoDock-GPU 1.6-CUDA-12.8.0 foss/2025a <code>module load foss/2025a AutoDock-GPU/1.6-CUDA-12.8.0</code> Automake 1.16.5 foss/2024a <code>module load foss/2024a Automake/1.16.5</code> Automake 1.17 foss/2025a <code>module load foss/2025a Automake/1.17</code> Autotools 20231222 foss/2024a <code>module load foss/2024a Autotools/20231222</code> Autotools 20240712 foss/2025a <code>module load foss/2025a Autotools/20240712</code> Avogadro2 1.97.0-linux-x86_64 - <code>module load Avogadro2/1.97.0-linux-x86_64</code> BeautifulSoup 4.13.4 foss/2025a <code>module load foss/2025a BeautifulSoup/4.13.4</code> binutils 2.40 - <code>module load binutils/2.40</code> binutils 2.42 - <code>module load binutils/2.42</code> binutils 2.42 foss/2025a <code>module load foss/2025a binutils/2.42</code> binutils 2.42 foss/2024a <code>module load foss/2024a binutils/2.42</code> Biopython 1.85 foss/2025a <code>module load foss/2025a Biopython/1.85</code> Bison 3.8.2 foss/2025a <code>module load foss/2025a Bison/3.8.2</code> Bison 3.8.2 foss/2024a <code>module load foss/2024a Bison/3.8.2</code> Bison 3.8.2 - <code>module load Bison/3.8.2</code> BLIS 1.1 foss/2025a <code>module load foss/2025a BLIS/1.1</code> BLIS 1.0 foss/2024a <code>module load foss/2024a BLIS/1.0</code> Blosc 1.21.6 foss/2025a <code>module load foss/2025a Blosc/1.21.6</code> Blosc2 2.19.0 foss/2025a <code>module load foss/2025a Blosc2/2.19.0</code> Boost 1.88.0 foss/2025a <code>module load foss/2025a Boost/1.88.0</code> Boost 1.85.0 foss/2024a <code>module load foss/2024a Boost/1.85.0</code> Boost.Python-NumPy 1.88.0 foss/2025a <code>module load foss/2025a Boost.Python-NumPy/1.88.0</code> Brotli 1.1.0 foss/2024a <code>module load foss/2024a Brotli/1.1.0</code> Brotli 1.1.0 foss/2025a <code>module load foss/2025a Brotli/1.1.0</code> bzip2 1.0.8 foss/2025a <code>module load foss/2025a bzip2/1.0.8</code> bzip2 1.0.8 foss/2024a <code>module load foss/2024a bzip2/1.0.8</code> cairo 1.18.0 foss/2024a <code>module load foss/2024a cairo/1.18.0</code> cairo 1.18.4 foss/2025a <code>module load foss/2025a cairo/1.18.4</code> Catch2 2.13.10 foss/2024a <code>module load foss/2024a Catch2/2.13.10</code> Catch2 2.13.10 foss/2025a <code>module load foss/2025a Catch2/2.13.10</code> Catch2 3.8.1 foss/2025a <code>module load foss/2025a Catch2/3.8.1</code> cffi 1.16.0 foss/2024a <code>module load foss/2024a cffi/1.16.0</code> cffi 1.17.1 foss/2025a <code>module load foss/2025a cffi/1.17.1</code> CGAL 5.6.1 foss/2024a <code>module load foss/2024a CGAL/5.6.1</code> Chapel 2.5.0 foss/2025a <code>module load foss/2025a Chapel/2.5.0</code> Chapel 2.4.0 foss/2024a <code>module load foss/2024a Chapel/2.4.0</code> Clang 18.1.8 foss/2024a <code>module load foss/2024a Clang/18.1.8</code> CMake 3.31.3 foss/2025a <code>module load foss/2025a CMake/3.31.3</code> CMake 3.29.3 foss/2024a <code>module load foss/2024a CMake/3.29.3</code> coverage 7.9.2 foss/2024a <code>module load foss/2024a coverage/7.9.2</code> CP2K 2025.2 foss/2025a <code>module load foss/2025a CP2K/2025.2</code> cppy 1.3.1 foss/2025a <code>module load foss/2025a cppy/1.3.1</code> cppy 1.2.1 foss/2024a <code>module load foss/2024a cppy/1.2.1</code> cryptography 44.0.2 foss/2025a <code>module load foss/2025a cryptography/44.0.2</code> cryptography 42.0.8 foss/2024a <code>module load foss/2024a cryptography/42.0.8</code> CUDA 12.6.0 - <code>module load CUDA/12.6.0</code> CUDA 12.8.0 - <code>module load CUDA/12.8.0</code> cuDNN 9.5.0.50-CUDA-12.6.0 - <code>module load cuDNN/9.5.0.50-CUDA-12.6.0</code> cURL 8.11.1 foss/2025a <code>module load foss/2025a cURL/8.11.1</code> cURL 8.7.1 foss/2024a <code>module load foss/2024a cURL/8.7.1</code> Cython 3.1.1 foss/2025a <code>module load foss/2025a Cython/3.1.1</code> Cython 3.0.10 foss/2024a <code>module load foss/2024a Cython/3.0.10</code> DBus 1.16.2 foss/2025a <code>module load foss/2025a DBus/1.16.2</code> DBus 1.15.8 foss/2024a <code>module load foss/2024a DBus/1.15.8</code> DMTCP 4.0.0 foss/2025a <code>module load foss/2025a DMTCP/4.0.0</code> double-conversion 3.3.1 foss/2025a <code>module load foss/2025a double-conversion/3.3.1</code> double-conversion 3.3.0 foss/2024a <code>module load foss/2024a double-conversion/3.3.0</code> Doxygen 1.11.0 foss/2024a <code>module load foss/2024a Doxygen/1.11.0</code> Doxygen 1.14.0 foss/2025a <code>module load foss/2025a Doxygen/1.14.0</code> EasyBuild 5.1.1 - <code>module load EasyBuild/5.1.1</code> Eigen 3.4.0 foss/2025a <code>module load foss/2025a Eigen/3.4.0</code> Eigen 3.4.0 foss/2024a <code>module load foss/2024a Eigen/3.4.0</code> ELPA 2024.05.001 foss/2024a <code>module load foss/2024a ELPA/2024.05.001</code> expat 2.6.2 foss/2024a <code>module load foss/2024a expat/2.6.2</code> expat 2.6.4 foss/2025a <code>module load foss/2025a expat/2.6.4</code> FFmpeg 7.0.2 foss/2024a <code>module load foss/2024a FFmpeg/7.0.2</code> FFmpeg 7.1.1 foss/2025a <code>module load foss/2025a FFmpeg/7.1.1</code> ffnvcodec 12.2.72.0 - <code>module load ffnvcodec/12.2.72.0</code> ffnvcodec 13.0.19.0 - <code>module load ffnvcodec/13.0.19.0</code> FFTW 3.3.10 foss/2024a <code>module load foss/2024a FFTW/3.3.10</code> FFTW 3.3.10 foss/2025a <code>module load foss/2025a FFTW/3.3.10</code> FFTW.MPI 3.3.10 foss/2025a <code>module load foss/2025a FFTW.MPI/3.3.10</code> FFTW.MPI 3.3.10 foss/2024a <code>module load foss/2024a FFTW.MPI/3.3.10</code> FLAC 1.4.3 foss/2024a <code>module load foss/2024a FLAC/1.4.3</code> Flask 3.1.1 foss/2025a <code>module load foss/2025a Flask/3.1.1</code> flex 2.6.4 foss/2025a <code>module load foss/2025a flex/2.6.4</code> flex 2.6.4 - <code>module load flex/2.6.4</code> flex 2.6.4 foss/2024a <code>module load foss/2024a flex/2.6.4</code> FlexiBLAS 3.4.4 foss/2024a <code>module load foss/2024a FlexiBLAS/3.4.4</code> FlexiBLAS 3.4.5 foss/2025a <code>module load foss/2025a FlexiBLAS/3.4.5</code> FLINT 3.1.2 foss/2024a <code>module load foss/2024a FLINT/3.1.2</code> flit 3.10.1 foss/2025a <code>module load foss/2025a flit/3.10.1</code> flit 3.9.0 foss/2024a <code>module load foss/2024a flit/3.9.0</code> flook 0.8.4 foss/2024a <code>module load foss/2024a flook/0.8.4</code> fontconfig 2.15.0 foss/2024a <code>module load foss/2024a fontconfig/2.15.0</code> fontconfig 2.16.2 foss/2025a <code>module load foss/2025a fontconfig/2.16.2</code> fonttools 4.53.1 foss/2024a <code>module load foss/2024a fonttools/4.53.1</code> fonttools 4.58.4 foss/2025a <code>module load foss/2025a fonttools/4.58.4</code> foss 2024a - <code>module load foss/2024a</code> foss 2025a - <code>module load foss/2025a</code> freeglut 3.6.0 foss/2024a <code>module load foss/2024a freeglut/3.6.0</code> freetype 2.13.2 foss/2024a <code>module load foss/2024a freetype/2.13.2</code> freetype 2.13.3 foss/2025a <code>module load foss/2025a freetype/2.13.3</code> FriBidi 1.0.16 foss/2025a <code>module load foss/2025a FriBidi/1.0.16</code> FriBidi 1.0.15 foss/2024a <code>module load foss/2024a FriBidi/1.0.15</code> Gaussian 16.C.03-AVX2 - <code>module load Gaussian/16.C.03-AVX2</code> GCC 13.3.0 foss/2024a <code>module load GCC/13.3.0</code> GCC 14.2.0 foss/2025a <code>module load GCC/14.2.0</code> GCCcore 13.3.0 - <code>module load GCCcore/13.3.0</code> GCCcore 14.2.0 - <code>module load GCCcore/14.2.0</code> Gdk-Pixbuf 2.42.12 foss/2025a <code>module load foss/2025a Gdk-Pixbuf/2.42.12</code> Gdk-Pixbuf 2.42.11 foss/2024a <code>module load foss/2024a Gdk-Pixbuf/2.42.11</code> GDRCopy 2.4.1 foss/2024a <code>module load foss/2024a GDRCopy/2.4.1</code> GDRCopy 2.4.4 foss/2025a <code>module load foss/2025a GDRCopy/2.4.4</code> gettext 0.22.5 foss/2024a <code>module load foss/2024a gettext/0.22.5</code> gettext 0.22.5 - <code>module load gettext/0.22.5</code> gettext 0.24 foss/2025a <code>module load foss/2025a gettext/0.24</code> gfbf 2024a - <code>module load gfbf/2024a</code> gfbf 2025a - <code>module load gfbf/2025a</code> giflib 5.2.2 foss/2025a <code>module load foss/2025a giflib/5.2.2</code> giflib 5.2.1 foss/2024a <code>module load foss/2024a giflib/5.2.1</code> git 2.49.0 foss/2025a <code>module load foss/2025a git/2.49.0</code> git 2.45.1 foss/2024a <code>module load foss/2024a git/2.45.1</code> GLib 2.85.1 foss/2025a <code>module load foss/2025a GLib/2.85.1</code> GLib 2.80.4 foss/2024a <code>module load foss/2024a GLib/2.80.4</code> GLPK 5.0 foss/2025a <code>module load foss/2025a GLPK/5.0</code> GLPK 5.0 foss/2024a <code>module load foss/2024a GLPK/5.0</code> glslang 15.3.0 foss/2025a <code>module load foss/2025a glslang/15.3.0</code> GMP 6.3.0 foss/2025a <code>module load foss/2025a GMP/6.3.0</code> GMP 6.3.0 foss/2024a <code>module load foss/2024a GMP/6.3.0</code> gnuplot 6.0.1 foss/2024a <code>module load foss/2024a gnuplot/6.0.1</code> Go 1.22.1 - <code>module load Go/1.22.1</code> Go 1.23.6 - <code>module load Go/1.23.6</code> GObject-Introspection 1.84.0 foss/2025a <code>module load foss/2025a GObject-Introspection/1.84.0</code> GObject-Introspection 1.80.1 foss/2024a <code>module load foss/2024a GObject-Introspection/1.80.1</code> gompi 2024a - <code>module load gompi/2024a</code> gompi 2025a - <code>module load gompi/2025a</code> googlebenchmark 1.9.4 foss/2025a <code>module load foss/2025a googlebenchmark/1.9.4</code> googletest 1.17.0 foss/2025a <code>module load foss/2025a googletest/1.17.0</code> gperf 3.3 foss/2025a <code>module load foss/2025a gperf/3.3</code> gperf 3.1 foss/2024a <code>module load foss/2024a gperf/3.1</code> graphite2 1.3.14 foss/2025a <code>module load foss/2025a graphite2/1.3.14</code> graphite2 1.3.14 foss/2024a <code>module load foss/2024a graphite2/1.3.14</code> groff 1.23.0 foss/2025a <code>module load foss/2025a groff/1.23.0</code> groff 1.23.0 foss/2024a <code>module load foss/2024a groff/1.23.0</code> GROMACS 2024.4-CUDA-12.6.0 foss/2024a <code>module load foss/2024a GROMACS/2024.4-CUDA-12.6.0</code> GROMACS 2025.2 foss/2025a <code>module load foss/2025a GROMACS/2025.2</code> GROMACS 2024.4 foss/2024a <code>module load foss/2024a GROMACS/2024.4</code> GROMACS 2025.2-CUDA-12.8.0 foss/2025a <code>module load foss/2025a GROMACS/2025.2-CUDA-12.8.0</code> GSL 2.8 foss/2025a <code>module load foss/2025a GSL/2.8</code> GSL 2.8 foss/2024a <code>module load foss/2024a GSL/2.8</code> Gurobi 12.0.3 foss/2025a <code>module load foss/2025a Gurobi/12.0.3</code> gzip 1.13 foss/2024a <code>module load foss/2024a gzip/1.13</code> gzip 1.13 foss/2025a <code>module load foss/2025a gzip/1.13</code> HarfBuzz 9.0.0 foss/2024a <code>module load foss/2024a HarfBuzz/9.0.0</code> HarfBuzz 11.2.1 foss/2025a <code>module load foss/2025a HarfBuzz/11.2.1</code> hatch-jupyter-builder 0.9.1 foss/2025a <code>module load foss/2025a hatch-jupyter-builder/0.9.1</code> hatchling 1.24.2 foss/2024a <code>module load foss/2024a hatchling/1.24.2</code> hatchling 1.27.0 foss/2025a <code>module load foss/2025a hatchling/1.27.0</code> HDF5 1.14.5 foss/2024a <code>module load foss/2024a HDF5/1.14.5</code> HDF5 1.14.6 intel/2025a <code>module load intel/2025a HDF5/1.14.6</code> HDF5 1.14.6 foss/2025a <code>module load foss/2025a HDF5/1.14.6</code> help2man 1.49.3 foss/2024a <code>module load foss/2024a help2man/1.49.3</code> help2man 1.49.3 foss/2025a <code>module load foss/2025a help2man/1.49.3</code> HPL 2.3 foss/2024a <code>module load foss/2024a HPL/2.3</code> hwloc 2.11.2 foss/2025a <code>module load foss/2025a hwloc/2.11.2</code> hwloc 2.10.0 foss/2024a <code>module load foss/2024a hwloc/2.10.0</code> hypothesis 6.133.2 foss/2025a <code>module load foss/2025a hypothesis/6.133.2</code> hypothesis 6.103.1 foss/2024a <code>module load foss/2024a hypothesis/6.103.1</code> Hypre 2.33.0 foss/2025a <code>module load foss/2025a Hypre/2.33.0</code> Hypre 2.32.0 foss/2024a <code>module load foss/2024a Hypre/2.32.0</code> ICU 75.1 foss/2024a <code>module load foss/2024a ICU/75.1</code> ICU 76.1 foss/2025a <code>module load foss/2025a ICU/76.1</code> igraph 0.10.16 foss/2024a <code>module load foss/2024a igraph/0.10.16</code> iimpi 2024a - <code>module load iimpi/2024a</code> iimpi 2025a - <code>module load iimpi/2025a</code> imkl 2025.1.0 - <code>module load imkl/2025.1.0</code> imkl 2024.2.0 - <code>module load imkl/2024.2.0</code> imkl-FFTW 2025.1.0 intel/2025a <code>module load intel/2025a imkl-FFTW/2025.1.0</code> imkl-FFTW 2024.2.0 intel/2024a <code>module load intel/2024a imkl-FFTW/2024.2.0</code> impi 2021.15.0 intel/2025a <code>module load intel/2025a</code> impi 2021.13.0 intel/2024a <code>module load intel/2024a</code> intel 2024a - <code>module load intel/2024a</code> intel 2025a - <code>module load intel/2025a</code> intel-compilers 2025.1.1 intel/2025a <code>module load intel-compilers/2025.1.1</code> intel-compilers 2024.2.0 intel/2024a <code>module load intel-compilers/2024.2.0</code> intltool 0.51.0 foss/2025a <code>module load foss/2025a intltool/0.51.0</code> intltool 0.51.0 foss/2024a <code>module load foss/2024a intltool/0.51.0</code> IPython 9.3.0 foss/2025a <code>module load foss/2025a IPython/9.3.0</code> JasPer 4.2.4 foss/2024a <code>module load foss/2024a JasPer/4.2.4</code> JasPer 4.2.5 foss/2025a <code>module load foss/2025a JasPer/4.2.5</code> Java 23.0.2 - <code>module load Java/23.0.2</code> Java 17.0.15 - <code>module load Java/17.0.15</code> Java .modulerc - <code>module load Java/.modulerc</code> jbigkit 2.1 foss/2024a <code>module load foss/2024a jbigkit/2.1</code> jbigkit 2.1 foss/2025a <code>module load foss/2025a jbigkit/2.1</code> jedi 0.19.1 foss/2025a <code>module load foss/2025a jedi/0.19.1</code> json-fortran 9.0.3 foss/2024a <code>module load foss/2024a json-fortran/9.0.3</code> Julia 1.9.3-linux-x86_64 - <code>module load Julia/1.9.3-linux-x86_64</code> Julia 1.11.6-linux-x86_64 - <code>module load Julia/1.11.6-linux-x86_64</code> jupyter-server 2.16.0 foss/2025a <code>module load foss/2025a jupyter-server/2.16.0</code> JupyterLab 4.4.4 foss/2025a <code>module load foss/2025a JupyterLab/4.4.4</code> JupyterNotebook 7.4.4 foss/2025a <code>module load foss/2025a JupyterNotebook/7.4.4</code> KaHIP 3.18 foss/2024a <code>module load foss/2024a KaHIP/3.18</code> kim-api 2.4.1 foss/2024a <code>module load foss/2024a kim-api/2.4.1</code> LAME 3.100 foss/2024a <code>module load foss/2024a LAME/3.100</code> LAME 3.100 foss/2025a <code>module load foss/2025a LAME/3.100</code> LAMMPS 2Aug2023_update2-kokkos-CUDA-12.6.0 foss/2024a <code>module load foss/2024a LAMMPS/2Aug2023_update2-kokkos-CUDA-12.6.0</code> LAMMPS 29Aug2024_update2-kokkos-CUDA-12.6.0 foss/2024a <code>module load foss/2024a LAMMPS/29Aug2024_update2-kokkos-CUDA-12.6.0</code> LAMMPS 29Aug2024_update2-kokkos foss/2024a <code>module load foss/2024a LAMMPS/29Aug2024_update2-kokkos</code> libarchive 3.7.4 foss/2024a <code>module load foss/2024a libarchive/3.7.4</code> libarchive 3.7.7 foss/2025a <code>module load foss/2025a libarchive/3.7.7</code> libcerf 2.4 foss/2024a <code>module load foss/2024a libcerf/2.4</code> libde265 1.0.15 foss/2024a <code>module load foss/2024a libde265/1.0.15</code> libde265 1.0.16 foss/2025a <code>module load foss/2025a libde265/1.0.16</code> libdeflate 1.20 foss/2024a <code>module load foss/2024a libdeflate/1.20</code> libdeflate 1.24 foss/2025a <code>module load foss/2025a libdeflate/1.24</code> libdrm 2.4.125 foss/2025a <code>module load foss/2025a libdrm/2.4.125</code> libdrm 2.4.122 foss/2024a <code>module load foss/2024a libdrm/2.4.122</code> libevent 2.1.12 foss/2025a <code>module load foss/2025a libevent/2.1.12</code> libevent 2.1.12 - <code>module load libevent/2.1.12</code> libevent 2.1.12 foss/2024a <code>module load foss/2024a libevent/2.1.12</code> libfabric 2.0.0 foss/2025a <code>module load foss/2025a libfabric/2.0.0</code> libfabric 1.21.0 foss/2024a <code>module load foss/2024a libfabric/1.21.0</code> libfdf 0.5.1 foss/2024a <code>module load foss/2024a libfdf/0.5.1</code> libffi 3.4.5 foss/2025a <code>module load foss/2025a libffi/3.4.5</code> libffi 3.4.5 foss/2024a <code>module load foss/2024a libffi/3.4.5</code> libgd 2.3.3 foss/2024a <code>module load foss/2024a libgd/2.3.3</code> libgit2 1.9.1 foss/2025a <code>module load foss/2025a libgit2/1.9.1</code> libgit2 1.8.1 foss/2024a <code>module load foss/2024a libgit2/1.8.1</code> libGLU 9.0.3 foss/2024a <code>module load foss/2024a libGLU/9.0.3</code> libGLU 9.0.3 foss/2025a <code>module load foss/2025a libGLU/9.0.3</code> libglvnd 1.7.0 foss/2024a <code>module load foss/2024a libglvnd/1.7.0</code> libglvnd 1.7.0 foss/2025a <code>module load foss/2025a libglvnd/1.7.0</code> libGridXC 2.0.2 foss/2024a <code>module load foss/2024a libGridXC/2.0.2</code> libheif 1.19.8 foss/2025a <code>module load foss/2025a libheif/1.19.8</code> libheif 1.19.5 foss/2024a <code>module load foss/2024a libheif/1.19.5</code> libiconv 1.18 foss/2025a <code>module load foss/2025a libiconv/1.18</code> libiconv 1.17 foss/2024a <code>module load foss/2024a libiconv/1.17</code> libidn2 2.3.7 foss/2025a <code>module load foss/2025a libidn2/2.3.7</code> Libint 2.11.1-lmax-6-cp2k foss/2025a <code>module load foss/2025a Libint/2.11.1-lmax-6-cp2k</code> Libint 2.9.0-lmax-6-cp2k foss/2024a <code>module load foss/2024a Libint/2.9.0-lmax-6-cp2k</code> libjpeg-turbo 3.0.1 foss/2024a <code>module load foss/2024a libjpeg-turbo/3.0.1</code> libjpeg-turbo 3.1.0 foss/2025a <code>module load foss/2025a libjpeg-turbo/3.1.0</code> libogg 1.3.5 foss/2024a <code>module load foss/2024a libogg/1.3.5</code> libpciaccess 0.18.1 foss/2024a <code>module load foss/2024a libpciaccess/0.18.1</code> libpciaccess 0.18.1 foss/2025a <code>module load foss/2025a libpciaccess/0.18.1</code> libpng 1.6.43 foss/2024a <code>module load foss/2024a libpng/1.6.43</code> libpng 1.6.48 foss/2025a <code>module load foss/2025a libpng/1.6.48</code> libpsl 0.21.5 foss/2025a <code>module load foss/2025a libpsl/0.21.5</code> libPSML 2.1.0 foss/2024a <code>module load foss/2024a libPSML/2.1.0</code> libreadline 8.2 foss/2025a <code>module load foss/2025a libreadline/8.2</code> libreadline 8.2 foss/2024a <code>module load foss/2024a libreadline/8.2</code> libsodium 1.0.20 foss/2025a <code>module load foss/2025a libsodium/1.0.20</code> LibTIFF 4.7.0 foss/2025a <code>module load foss/2025a LibTIFF/4.7.0</code> LibTIFF 4.6.0 foss/2024a <code>module load foss/2024a LibTIFF/4.6.0</code> libtool 2.5.4 foss/2025a <code>module load foss/2025a libtool/2.5.4</code> libtool 2.4.7 foss/2024a <code>module load foss/2024a libtool/2.4.7</code> libunistring 1.3 foss/2025a <code>module load foss/2025a libunistring/1.3</code> libunwind 1.8.1 foss/2025a <code>module load foss/2025a libunwind/1.8.1</code> libunwind 1.8.1 foss/2024a <code>module load foss/2024a libunwind/1.8.1</code> libvori 220621 foss/2025a <code>module load foss/2025a libvori/220621</code> libvori 220621 foss/2024a <code>module load foss/2024a libvori/220621</code> libwebp 1.5.0 foss/2025a <code>module load foss/2025a libwebp/1.5.0</code> libwebp 1.4.0 foss/2024a <code>module load foss/2024a libwebp/1.4.0</code> libxc 7.0.0 foss/2025a <code>module load foss/2025a libxc/7.0.0</code> libxc 6.2.2 foss/2024a <code>module load foss/2024a libxc/6.2.2</code> libxml2 2.13.4 foss/2025a <code>module load foss/2025a libxml2/2.13.4</code> libxml2 2.12.7 foss/2024a <code>module load foss/2024a libxml2/2.12.7</code> libxslt 1.1.42 foss/2024a <code>module load foss/2024a libxslt/1.1.42</code> libxslt 1.1.42 foss/2025a <code>module load foss/2025a libxslt/1.1.42</code> libxsmm 1.17 foss/2024a <code>module load foss/2024a libxsmm/1.17</code> libxsmm 1.17 foss/2025a <code>module load foss/2025a libxsmm/1.17</code> libyaml 0.2.5 foss/2025a <code>module load foss/2025a libyaml/0.2.5</code> libyaml 0.2.5 foss/2024a <code>module load foss/2024a libyaml/0.2.5</code> lit 18.1.8 foss/2024a <code>module load foss/2024a lit/18.1.8</code> lit 18.1.8 foss/2025a <code>module load foss/2025a lit/18.1.8</code> LittleCMS 2.17 foss/2025a <code>module load foss/2025a LittleCMS/2.17</code> LittleCMS 2.16 foss/2024a <code>module load foss/2024a LittleCMS/2.16</code> LLVM 18.1.8 foss/2024a <code>module load foss/2024a LLVM/18.1.8</code> LLVM 20.1.5 foss/2024a <code>module load foss/2024a LLVM/20.1.5</code> LLVM 20.1.8 foss/2025a <code>module load foss/2025a LLVM/20.1.8</code> LLVM 20.1.5 foss/2025a <code>module load foss/2025a LLVM/20.1.5</code> Lua 5.4.7 foss/2024a <code>module load foss/2024a Lua/5.4.7</code> lxml 5.3.0 foss/2025a <code>module load foss/2025a lxml/5.3.0</code> lz4 1.10.0 foss/2025a <code>module load foss/2025a lz4/1.10.0</code> lz4 1.9.4 foss/2024a <code>module load foss/2024a lz4/1.9.4</code> LZO 2.10 foss/2025a <code>module load foss/2025a LZO/2.10</code> M4 1.4.19 foss/2024a <code>module load foss/2024a M4/1.4.19</code> M4 1.4.19 - <code>module load M4/1.4.19</code> M4 1.4.19 foss/2025a <code>module load foss/2025a M4/1.4.19</code> make 4.4.1 foss/2025a <code>module load foss/2025a make/4.4.1</code> make 4.4.1 foss/2024a <code>module load foss/2024a make/4.4.1</code> Mako 1.3.5 foss/2024a <code>module load foss/2024a Mako/1.3.5</code> Mako 1.3.10 foss/2025a <code>module load foss/2025a Mako/1.3.10</code> MATLAB 2024a - <code>module load MATLAB/2024a</code> MATLAB 2025a - <code>module load MATLAB/2025a</code> matlab-proxy 0.27.1 foss/2025a <code>module load foss/2025a matlab-proxy/0.27.1</code> matplotlib 3.10.3 foss/2025a <code>module load foss/2025a matplotlib/3.10.3</code> matplotlib 3.9.2 foss/2024a <code>module load foss/2024a matplotlib/3.9.2</code> maturin 1.8.3 foss/2025a <code>module load foss/2025a maturin/1.8.3</code> maturin 1.6.0 foss/2024a <code>module load foss/2024a maturin/1.6.0</code> mctc-lib 0.3.1 foss/2024a <code>module load foss/2024a mctc-lib/0.3.1</code> MDAnalysis 2.9.0 foss/2025a <code>module load foss/2025a MDAnalysis/2.9.0</code> MDI 1.4.26 foss/2024a <code>module load foss/2024a MDI/1.4.26</code> Mesa 24.1.3 foss/2024a <code>module load foss/2024a Mesa/24.1.3</code> Mesa 25.1.3 foss/2025a <code>module load foss/2025a Mesa/25.1.3</code> Mesa 24.1.3 foss/2025a <code>module load foss/2025a Mesa/24.1.3</code> Meson 1.4.0 foss/2024a <code>module load foss/2024a Meson/1.4.0</code> Meson 1.6.1 foss/2025a <code>module load foss/2025a Meson/1.6.1</code> meson-python 0.18.0 foss/2025a <code>module load foss/2025a meson-python/0.18.0</code> meson-python 0.16.0 foss/2024a <code>module load foss/2024a meson-python/0.16.0</code> METIS 5.1.0 foss/2025a <code>module load foss/2025a METIS/5.1.0</code> METIS 5.1.0 foss/2024a <code>module load foss/2024a METIS/5.1.0</code> Miniforge3 24.11.3-0 - <code>module load Miniforge3/24.11.3-0</code> motif 2.3.8 foss/2024a <code>module load foss/2024a motif/2.3.8</code> MPFR 4.2.1 foss/2024a <code>module load foss/2024a MPFR/4.2.1</code> MPFR 4.2.2 foss/2025a <code>module load foss/2025a MPFR/4.2.2</code> mpi4py 4.0.1 foss/2024a <code>module load foss/2024a mpi4py/4.0.1</code> mpi4py 4.1.0 foss/2025a <code>module load foss/2025a mpi4py/4.1.0</code> mrcfile 1.5.4 foss/2025a <code>module load foss/2025a mrcfile/1.5.4</code> mstore 0.3.0 foss/2024a <code>module load foss/2024a mstore/0.3.0</code> MUMPS 5.8.1-metis foss/2025a <code>module load foss/2025a MUMPS/5.8.1-metis</code> MUMPS 5.7.2-metis foss/2024a <code>module load foss/2024a MUMPS/5.7.2-metis</code> NASM 2.16.03 foss/2025a <code>module load foss/2025a NASM/2.16.03</code> NASM 2.16.03 foss/2024a <code>module load foss/2024a NASM/2.16.03</code> NCCL 2.22.3-CUDA-12.6.0 foss/2024a <code>module load foss/2024a NCCL/2.22.3-CUDA-12.6.0</code> NCCL 2.26.6-CUDA-12.8.0 foss/2025a <code>module load foss/2025a NCCL/2.26.6-CUDA-12.8.0</code> ncurses .6.5 foss/2024a <code>module load foss/2024a ncurses/.6.5</code> ncurses .6.5 - <code>module load ncurses/.6.5</code> ncurses .6.5 foss/2025a <code>module load foss/2025a ncurses/.6.5</code> netCDF 4.9.3 foss/2025a <code>module load foss/2025a netCDF/4.9.3</code> netCDF 4.9.2 foss/2024a <code>module load foss/2024a netCDF/4.9.2</code> netCDF-Fortran 4.6.1 foss/2024a <code>module load foss/2024a netCDF-Fortran/4.6.1</code> netCDF-Fortran 4.6.2 foss/2025a <code>module load foss/2025a netCDF-Fortran/4.6.2</code> netcdf4-python 1.7.2 foss/2025a <code>module load foss/2025a netcdf4-python/1.7.2</code> nettle 3.10 foss/2024a <code>module load foss/2024a nettle/3.10</code> nettle 3.10.1 foss/2025a <code>module load foss/2025a nettle/3.10.1</code> networkx 3.4.2 foss/2024a <code>module load foss/2024a networkx/3.4.2</code> networkx 3.5 foss/2025a <code>module load foss/2025a networkx/3.5</code> Ninja 1.12.1 foss/2024a <code>module load foss/2024a Ninja/1.12.1</code> Ninja 1.12.1 foss/2025a <code>module load foss/2025a Ninja/1.12.1</code> nodejs 22.16.0 foss/2025a <code>module load foss/2025a nodejs/22.16.0</code> nodejs 20.13.1 foss/2024a <code>module load foss/2024a nodejs/20.13.1</code> NSPR 4.35 foss/2024a <code>module load foss/2024a NSPR/4.35</code> NSPR 4.36 foss/2025a <code>module load foss/2025a NSPR/4.36</code> NSS 3.104 foss/2024a <code>module load foss/2024a NSS/3.104</code> NSS 3.113 foss/2025a <code>module load foss/2025a NSS/3.113</code> NTL 11.5.1 foss/2024a <code>module load foss/2024a NTL/11.5.1</code> numactl 2.0.18 foss/2024a <code>module load foss/2024a numactl/2.0.18</code> numactl 2.0.19 foss/2025a <code>module load foss/2025a numactl/2.0.19</code> NVHPC 25.3-CUDA-12.8.0 - <code>module load NVHPC/25.3-CUDA-12.8.0</code> OpenBLAS 0.3.29 foss/2025a <code>module load foss/2025a OpenBLAS/0.3.29</code> OpenBLAS 0.3.27 foss/2024a <code>module load foss/2024a OpenBLAS/0.3.27</code> OpenFOAM v2406 foss/2024a <code>module load foss/2024a OpenFOAM/v2406</code> OpenJPEG 2.5.2 foss/2024a <code>module load foss/2024a OpenJPEG/2.5.2</code> OpenJPEG 2.5.3 foss/2025a <code>module load foss/2025a OpenJPEG/2.5.3</code> OpenMPI 5.0.3 foss/2024a <code>module load foss/2024a</code> OpenMPI 5.0.7 foss/2025a <code>module load foss/2025a</code> OpenPGM 5.2.122 foss/2025a <code>module load foss/2025a OpenPGM/5.2.122</code> OpenSSL 3 - <code>module load OpenSSL/3</code> ORCA 6.0.1-avx2 foss/2024a <code>module load foss/2024a ORCA/6.0.1-avx2</code> OVITO 3.12.3-basic foss/2024a <code>module load foss/2024a OVITO/3.12.3-basic</code> p7zip 17.05 foss/2025a <code>module load foss/2025a p7zip/17.05</code> p7zip 17.04 - <code>module load p7zip/17.04</code> packmol 21.0.4 foss/2025a <code>module load foss/2025a packmol/21.0.4</code> Pango 1.54.0 foss/2024a <code>module load foss/2024a Pango/1.54.0</code> parallel 20240722 foss/2024a <code>module load foss/2024a parallel/20240722</code> ParaView 5.13.2 foss/2024a <code>module load foss/2024a ParaView/5.13.2</code> ParaView 5.11.2-egl - <code>module load ParaView/5.11.2-egl</code> ParaView 5.11.2-osmesa - <code>module load ParaView/5.11.2-osmesa</code> ParMETIS 4.0.3 foss/2025a <code>module load foss/2025a ParMETIS/4.0.3</code> ParMETIS 4.0.3 foss/2024a <code>module load foss/2024a ParMETIS/4.0.3</code> patchelf 0.18.0 foss/2024a <code>module load foss/2024a patchelf/0.18.0</code> patchelf 0.18.0 foss/2025a <code>module load foss/2025a patchelf/0.18.0</code> PCRE 8.45 foss/2024a <code>module load foss/2024a PCRE/8.45</code> PCRE2 10.43 foss/2024a <code>module load foss/2024a PCRE2/10.43</code> PCRE2 10.45 foss/2025a <code>module load foss/2025a PCRE2/10.45</code> Perl 5.40.0 foss/2025a <code>module load foss/2025a Perl/5.40.0</code> Perl 5.38.0 - <code>module load Perl/5.38.0</code> Perl 5.38.2 foss/2024a <code>module load foss/2024a Perl/5.38.2</code> Perl-bundle-CPAN 5.38.2 foss/2024a <code>module load foss/2024a Perl-bundle-CPAN/5.38.2</code> Perl-bundle-CPAN 5.40.0 foss/2025a <code>module load foss/2025a Perl-bundle-CPAN/5.40.0</code> perm-md-count main foss/2025a <code>module load foss/2025a perm-md-count/main</code> PETSc 3.23.5 foss/2025a <code>module load foss/2025a PETSc/3.23.5</code> PETSc 3.23.5 foss/2024a <code>module load foss/2024a PETSc/3.23.5</code> petsc4py 3.23.5 foss/2025a <code>module load foss/2025a petsc4py/3.23.5</code> Pillow 11.3.0 foss/2025a <code>module load foss/2025a Pillow/11.3.0</code> Pillow 10.4.0 foss/2024a <code>module load foss/2024a Pillow/10.4.0</code> pixman 0.43.4 foss/2024a <code>module load foss/2024a pixman/0.43.4</code> pixman 0.46.2 foss/2025a <code>module load foss/2025a pixman/0.46.2</code> pkgconf 2.2.0 foss/2024a <code>module load foss/2024a pkgconf/2.2.0</code> pkgconf 1.8.0 - <code>module load pkgconf/1.8.0</code> pkgconf 2.3.0 foss/2025a <code>module load foss/2025a pkgconf/2.3.0</code> PLUMED 2.9.3 foss/2024a <code>module load foss/2024a PLUMED/2.9.3</code> PLUMED 2.9.4 foss/2025a <code>module load foss/2025a PLUMED/2.9.4</code> PLY 3.11 foss/2024a <code>module load foss/2024a PLY/3.11</code> PMIx 5.0.2 foss/2024a <code>module load foss/2024a PMIx/5.0.2</code> PMIx 5.0.6 foss/2025a <code>module load foss/2025a PMIx/5.0.6</code> PnetCDF 1.14.0 foss/2024a <code>module load foss/2024a PnetCDF/1.14.0</code> PnetCDF 1.14.0 foss/2025a <code>module load foss/2025a PnetCDF/1.14.0</code> pocl 6.0 foss/2024a <code>module load foss/2024a pocl/6.0</code> poetry 2.1.2 foss/2025a <code>module load foss/2025a poetry/2.1.2</code> poetry 1.8.3 foss/2024a <code>module load foss/2024a poetry/1.8.3</code> pre-commit 3.7.0 foss/2024a <code>module load foss/2024a pre-commit/3.7.0</code> PRRTE 3.0.8 foss/2025a <code>module load foss/2025a PRRTE/3.0.8</code> PRRTE 3.0.5 foss/2024a <code>module load foss/2024a PRRTE/3.0.5</code> psutil 6.0.0 foss/2024a <code>module load foss/2024a psutil/6.0.0</code> psutil 7.0.0 foss/2025a <code>module load foss/2025a psutil/7.0.0</code> py-cpuinfo 9.0.0 foss/2025a <code>module load foss/2025a py-cpuinfo/9.0.0</code> pybind11 2.13.6 foss/2025a <code>module load foss/2025a pybind11/2.13.6</code> pybind11 2.12.0 foss/2024a <code>module load foss/2024a pybind11/2.12.0</code> PyQt-builder 1.18.1 foss/2024a <code>module load foss/2024a PyQt-builder/1.18.1</code> PyQt5 5.15.11 foss/2024a <code>module load foss/2024a PyQt5/5.15.11</code> PyTables 3.10.2 foss/2025a <code>module load foss/2025a PyTables/3.10.2</code> Python 3.12.3 foss/2024a <code>module load foss/2024a Python/3.12.3</code> Python 3.13.1 foss/2025a <code>module load foss/2025a Python/3.13.1</code> Python-bundle-PyPI 2025.04 foss/2025a <code>module load foss/2025a Python-bundle-PyPI/2025.04</code> Python-bundle-PyPI 2024.06 foss/2024a <code>module load foss/2024a Python-bundle-PyPI/2024.06</code> PyYAML 6.0.2 foss/2025a <code>module load foss/2025a PyYAML/6.0.2</code> PyYAML 6.0.2 foss/2024a <code>module load foss/2024a PyYAML/6.0.2</code> PyZMQ 27.0.0 foss/2025a <code>module load foss/2025a PyZMQ/27.0.0</code> Qhull 2020.2 foss/2024a <code>module load foss/2024a Qhull/2020.2</code> Qhull 2020.2 foss/2025a <code>module load foss/2025a Qhull/2020.2</code> Qt5 5.15.16 foss/2024a <code>module load foss/2024a Qt5/5.15.16</code> Qt6 6.7.2 foss/2024a <code>module load foss/2024a Qt6/6.7.2</code> Qt6 6.9.1 foss/2025a <code>module load foss/2025a Qt6/6.9.1</code> QuantumESPRESSO 7.4 foss/2024a <code>module load foss/2024a QuantumESPRESSO/7.4</code> R 4.4.2 foss/2024a <code>module load foss/2024a R/4.4.2</code> R 4.4.2 foss/2025a <code>module load foss/2025a R/4.4.2</code> RapidJSON 1.1.0-20250205 foss/2025a <code>module load foss/2025a RapidJSON/1.1.0-20250205</code> rclone 1.68.1 - <code>module load rclone/1.68.1</code> RDKit 2025.03.4 foss/2025a <code>module load foss/2025a RDKit/2025.03.4</code> RE2 2024-07-02 foss/2025a <code>module load foss/2025a RE2/2024-07-02</code> re2c 4.2 foss/2025a <code>module load foss/2025a re2c/4.2</code> re2c 3.1 foss/2024a <code>module load foss/2024a re2c/3.1</code> ruamel.yaml 0.18.6 foss/2024a <code>module load foss/2024a ruamel.yaml/0.18.6</code> Rust 1.78.0 foss/2024a <code>module load foss/2024a Rust/1.78.0</code> Rust 1.85.1 foss/2025a <code>module load foss/2025a Rust/1.85.1</code> ScaFaCoS 1.0.4 foss/2024a <code>module load foss/2024a ScaFaCoS/1.0.4</code> ScaLAPACK 2.2.0-fb foss/2024a <code>module load foss/2024a ScaLAPACK/2.2.0-fb</code> ScaLAPACK 2.2.2-fb foss/2025a <code>module load foss/2025a ScaLAPACK/2.2.2-fb</code> scikit-build 0.17.6 foss/2024a <code>module load foss/2024a scikit-build/0.17.6</code> scikit-build 0.18.1 foss/2025a <code>module load foss/2025a scikit-build/0.18.1</code> scikit-build-core 0.11.1 foss/2025a <code>module load foss/2025a scikit-build-core/0.11.1</code> scikit-build-core 0.10.6 foss/2024a <code>module load foss/2024a scikit-build-core/0.10.6</code> scikit-learn 1.7.0 foss/2025a <code>module load foss/2025a scikit-learn/1.7.0</code> SciPy-bundle 2024.05 foss/2024a <code>module load foss/2024a SciPy-bundle/2024.05</code> SciPy-bundle 2025.06 foss/2025a <code>module load foss/2025a SciPy-bundle/2025.06</code> SCOTCH 7.0.8 foss/2025a <code>module load foss/2025a SCOTCH/7.0.8</code> SCOTCH 7.0.6 foss/2024a <code>module load foss/2024a SCOTCH/7.0.6</code> SDL2 2.30.6 foss/2024a <code>module load foss/2024a SDL2/2.30.6</code> SDL2 2.32.8 foss/2025a <code>module load foss/2025a SDL2/2.32.8</code> Seaborn 0.13.2 foss/2025a <code>module load foss/2025a Seaborn/0.13.2</code> setuptools 80.9.0 foss/2025a <code>module load foss/2025a setuptools/80.9.0</code> setuptools-rust 1.11.0 foss/2025a <code>module load foss/2025a setuptools-rust/1.11.0</code> setuptools-rust 1.9.0 foss/2024a <code>module load foss/2024a setuptools-rust/1.9.0</code> Siesta 5.4.0 foss/2024a <code>module load foss/2024a Siesta/5.4.0</code> Simple-DFTD3 1.2.1 foss/2024a <code>module load foss/2024a Simple-DFTD3/1.2.1</code> SIP 6.10.0 foss/2024a <code>module load foss/2024a SIP/6.10.0</code> SLEPc 3.23.2 foss/2024a <code>module load foss/2024a SLEPc/3.23.2</code> SLEPc 3.23.2 foss/2025a <code>module load foss/2025a SLEPc/3.23.2</code> slepc4py 3.23.2 foss/2025a <code>module load foss/2025a slepc4py/3.23.2</code> snappy 1.2.1 foss/2024a <code>module load foss/2024a snappy/1.2.1</code> snappy 1.2.2 foss/2025a <code>module load foss/2025a snappy/1.2.2</code> spglib-python 2.6.0 foss/2025a <code>module load foss/2025a spglib-python/2.6.0</code> spin 0.14 foss/2025a <code>module load foss/2025a spin/0.14</code> SPIRV-tools 2025.2.rc2 foss/2025a <code>module load foss/2025a SPIRV-tools/2025.2.rc2</code> SQLite 3.45.3 foss/2024a <code>module load foss/2024a SQLite/3.45.3</code> SQLite 3.47.2 foss/2025a <code>module load foss/2025a SQLite/3.47.2</code> SuiteSparse 7.10.1 foss/2024a <code>module load foss/2024a SuiteSparse/7.10.1</code> SuiteSparse 7.10.3 foss/2025a <code>module load foss/2025a SuiteSparse/7.10.3</code> SuperLU_DIST 9.1.0 foss/2025a <code>module load foss/2025a SuperLU_DIST/9.1.0</code> SuperLU_DIST 9.1.0 foss/2024a <code>module load foss/2024a SuperLU_DIST/9.1.0</code> Szip 2.1.1 foss/2025a <code>module load foss/2025a Szip/2.1.1</code> Szip 2.1.1 foss/2024a <code>module load foss/2024a Szip/2.1.1</code> tbb 2021.13.0 foss/2024a <code>module load foss/2024a tbb/2021.13.0</code> Tcl 8.6.16 foss/2025a <code>module load foss/2025a Tcl/8.6.16</code> Tcl 8.6.14 foss/2024a <code>module load foss/2024a Tcl/8.6.14</code> tcsh 6.24.13 foss/2024a <code>module load foss/2024a tcsh/6.24.13</code> tecplot 2024R1 - <code>module load tecplot/2024R1</code> test-drive 0.5.0 foss/2024a <code>module load foss/2024a test-drive/0.5.0</code> Tk 8.6.16 foss/2025a <code>module load foss/2025a Tk/8.6.16</code> Tk 8.6.14 foss/2024a <code>module load foss/2024a Tk/8.6.14</code> Tkinter 3.13.1 foss/2025a <code>module load foss/2025a Tkinter/3.13.1</code> Tkinter 3.12.3 foss/2024a <code>module load foss/2024a Tkinter/3.12.3</code> tmux 3.5a - <code>module load tmux/3.5a</code> TOML-Fortran 0.4.2 foss/2024a <code>module load foss/2024a TOML-Fortran/0.4.2</code> tornado 6.5.1 foss/2025a <code>module load foss/2025a tornado/6.5.1</code> tqdm 4.67.1 foss/2025a <code>module load foss/2025a tqdm/4.67.1</code> UCC 1.3.0 foss/2025a <code>module load foss/2025a UCC/1.3.0</code> UCC 1.3.0 foss/2024a <code>module load foss/2024a UCC/1.3.0</code> UCX 1.18.0 foss/2025a <code>module load foss/2025a UCX/1.18.0</code> UCX 1.16.0 foss/2024a <code>module load foss/2024a UCX/1.16.0</code> UCX-CUDA 1.16.0-CUDA-12.6.0 foss/2024a <code>module load foss/2024a UCX-CUDA/1.16.0-CUDA-12.6.0</code> UCX-CUDA 1.18.0-CUDA-12.8.0 foss/2025a <code>module load foss/2025a UCX-CUDA/1.18.0-CUDA-12.8.0</code> UnZip 6.0 foss/2024a <code>module load foss/2024a UnZip/6.0</code> UnZip 6.0 foss/2025a <code>module load foss/2025a UnZip/6.0</code> utf8proc 2.9.0 foss/2025a <code>module load foss/2025a utf8proc/2.9.0</code> util-linux 2.40 foss/2024a <code>module load foss/2024a util-linux/2.40</code> util-linux 2.41 foss/2025a <code>module load foss/2025a util-linux/2.41</code> VASP 6.5.0 intel/2025a <code>module load intel/2025a VASP/6.5.0</code> VESTA 3.90.5-gtk3 - <code>module load VESTA/3.90.5-gtk3</code> virtualenv 20.29.2 foss/2025a <code>module load foss/2025a virtualenv/20.29.2</code> virtualenv 20.26.2 foss/2024a <code>module load foss/2024a virtualenv/20.26.2</code> VirtualGL 3.1.1 foss/2024a <code>module load foss/2024a VirtualGL/3.1.1</code> Voro++ 0.4.6 foss/2024a <code>module load foss/2024a Voro++/0.4.6</code> VSCode 1.88.1 - <code>module load VSCode/1.88.1</code> VTK 9.3.1 foss/2024a <code>module load foss/2024a VTK/9.3.1</code> Wayland 1.23.0 foss/2024a <code>module load foss/2024a Wayland/1.23.0</code> Wayland 1.23.92 foss/2025a <code>module load foss/2025a Wayland/1.23.92</code> X11 20250521 foss/2025a <code>module load foss/2025a X11/20250521</code> X11 20240607 foss/2024a <code>module load foss/2024a X11/20240607</code> x264 20250619 foss/2025a <code>module load foss/2025a x264/20250619</code> x264 20240513 foss/2024a <code>module load foss/2024a x264/20240513</code> x265 4.1 foss/2025a <code>module load foss/2025a x265/4.1</code> x265 3.6 foss/2024a <code>module load foss/2024a x265/3.6</code> xmlf90 1.6.3 foss/2024a <code>module load foss/2024a xmlf90/1.6.3</code> xorg-macros 1.20.1 foss/2024a <code>module load foss/2024a xorg-macros/1.20.1</code> xorg-macros 1.20.2 foss/2025a <code>module load foss/2025a xorg-macros/1.20.2</code> Xvfb 21.1.18 foss/2025a <code>module load foss/2025a Xvfb/21.1.18</code> Xvfb 21.1.14 foss/2024a <code>module load foss/2024a Xvfb/21.1.14</code> xxd 9.1.1457 foss/2025a <code>module load foss/2025a xxd/9.1.1457</code> xxd 9.1.1275 foss/2024a <code>module load foss/2024a xxd/9.1.1275</code> XZ 5.4.5 foss/2024a <code>module load foss/2024a XZ/5.4.5</code> XZ 5.6.3 foss/2025a <code>module load foss/2025a XZ/5.6.3</code> Yasm 1.3.0 foss/2024a <code>module load foss/2024a Yasm/1.3.0</code> Yasm 1.3.0 foss/2025a <code>module load foss/2025a Yasm/1.3.0</code> Z3 4.13.0 foss/2024a <code>module load foss/2024a Z3/4.13.0</code> Z3 4.13.4 foss/2025a <code>module load foss/2025a Z3/4.13.4</code> ZeroMQ 4.3.5 foss/2025a <code>module load foss/2025a ZeroMQ/4.3.5</code> zlib .1.3.1 foss/2025a <code>module load foss/2025a zlib/.1.3.1</code> zlib .1.3.1 foss/2024a <code>module load foss/2024a zlib/.1.3.1</code> zlib .1.2.13 foss/2024a <code>module load foss/2024a zlib/.1.2.13</code> zlib .1.2.13 - <code>module load zlib/.1.2.13</code> zlib .1.3.1 - <code>module load zlib/.1.3.1</code> zstd 1.5.6 foss/2025a <code>module load foss/2025a zstd/1.5.6</code> zstd 1.5.6 foss/2024a <code>module load foss/2024a zstd/1.5.6</code> Software Version Dependent Toolchain Module Load Command ABAQUS 2024-hotfix-2405 - <code>module load ABAQUS/2024-hotfix-2405</code> Abseil 20240116.1 foss/2023b <code>module load foss/2023b Abseil/20240116.1</code> ACTC 1.1 foss/2022b <code>module load foss/2022b ACTC/1.1</code> ACTC 1.1 foss/2021b <code>module load foss/2021b ACTC/1.1</code> AFNI 23.0.04-Python-3.9.6 foss/2021b <code>module load foss/2021b AFNI/23.0.04-Python-3.9.6</code> aiohttp 3.9.5 foss/2023b <code>module load foss/2023b aiohttp/3.9.5</code> Altair 2023 - <code>module load Altair/2023</code> Anaconda3 5.3.0 - <code>module load Anaconda3/5.3.0</code> Anaconda3 2023.09-0 - <code>module load Anaconda3/2023.09-0</code> ANSYS 2022 - <code>module load ANSYS/2022</code> ANSYS 2023R1 foss/2021b <code>module load foss/2021b ANSYS/2023R1</code> apptainer 1.1.9 - <code>module load apptainer/1.1.9</code> Arb 2.23.0 foss/2022b <code>module load foss/2022b Arb/2.23.0</code> archspec 0.2.2 foss/2023b <code>module load foss/2023b archspec/0.2.2</code> archspec 0.1.3 foss/2021b <code>module load foss/2021b archspec/0.1.3</code> arkouda 2023.11.15 foss/2022b <code>module load foss/2022b arkouda/2023.11.15</code> Armadillo 12.8.0 foss/2023b <code>module load foss/2023b Armadillo/12.8.0</code> Armadillo 11.4.3 foss/2022b <code>module load foss/2022b Armadillo/11.4.3</code> arpack-ng 3.9.0 foss/2023b <code>module load foss/2023b arpack-ng/3.9.0</code> arpack-ng 3.8.0 foss/2022b <code>module load foss/2022b arpack-ng/3.8.0</code> Arrow 11.0.0 foss/2022b <code>module load foss/2022b Arrow/11.0.0</code> assimp 5.3.1 foss/2023b <code>module load foss/2023b assimp/5.3.1</code> at-spi2-atk 2.38.0 foss/2022b <code>module load foss/2022b at-spi2-atk/2.38.0</code> at-spi2-core 2.46.0 foss/2022b <code>module load foss/2022b at-spi2-core/2.46.0</code> ATK 2.38.0 foss/2022b <code>module load foss/2022b ATK/2.38.0</code> Autoconf 2.71 foss/2022b <code>module load foss/2022b Autoconf/2.71</code> Autoconf 2.71 foss/2021b <code>module load foss/2021b Autoconf/2.71</code> Autoconf 2.71 foss/2023b <code>module load foss/2023b Autoconf/2.71</code> Autoconf 2.71 - <code>module load Autoconf/2.71</code> AutoDock-GPU 1.5.3-CUDA-11.4.1 foss/2021b <code>module load foss/2021b AutoDock-GPU/1.5.3-CUDA-11.4.1</code> Automake 1.16.5 foss/2022b <code>module load foss/2022b Automake/1.16.5</code> Automake 1.16.5 - <code>module load Automake/1.16.5</code> Automake 1.16.4 foss/2021b <code>module load foss/2021b Automake/1.16.4</code> Automake 1.16.5 foss/2023b <code>module load foss/2023b Automake/1.16.5</code> Autotools 20220317 foss/2023b <code>module load foss/2023b Autotools/20220317</code> Autotools 20210726 foss/2021b <code>module load foss/2021b Autotools/20210726</code> Autotools 20220317 foss/2022b <code>module load foss/2022b Autotools/20220317</code> Autotools 20220317 - <code>module load Autotools/20220317</code> Avogadro2 1.97.0-linux-x86_64 - <code>module load Avogadro2/1.97.0-linux-x86_64</code> BeautifulSoup 4.12.2 foss/2023b <code>module load foss/2023b BeautifulSoup/4.12.2</code> binutils 2.39 - <code>module load binutils/2.39</code> binutils 2.38 - <code>module load binutils/2.38</code> binutils 2.37 - <code>module load binutils/2.37</code> binutils 2.40 foss/2023b <code>module load foss/2023b binutils/2.40</code> binutils 2.37 foss/2021b <code>module load foss/2021b binutils/2.37</code> binutils 2.36.1 - <code>module load binutils/2.36.1</code> binutils 2.39 foss/2022b <code>module load foss/2022b binutils/2.39</code> binutils 2.40 - <code>module load binutils/2.40</code> Biopython 1.81 foss/2022b <code>module load foss/2022b Biopython/1.81</code> Biopython 1.79 foss/2021b <code>module load foss/2021b Biopython/1.79</code> Bison 3.8.2 - <code>module load Bison/3.8.2</code> Bison 3.8.2 foss/2023b <code>module load foss/2023b Bison/3.8.2</code> Bison 3.8.2 foss/2022b <code>module load foss/2022b Bison/3.8.2</code> Bison 3.7.6 foss/2021b <code>module load foss/2021b Bison/3.7.6</code> BLIS 0.9.0 foss/2022b <code>module load foss/2022b BLIS/0.9.0</code> BLIS 0.9.0 foss/2023b <code>module load foss/2023b BLIS/0.9.0</code> BLIS 0.8.1 foss/2021b <code>module load foss/2021b BLIS/0.8.1</code> Blosc 1.21.3 foss/2022b <code>module load foss/2022b Blosc/1.21.3</code> Blosc2 2.8.0 foss/2022b <code>module load foss/2022b Blosc2/2.8.0</code> Boost 1.77.0 intel/2021b <code>module load intel/2021b Boost/1.77.0</code> Boost 1.77.0 foss/2021b <code>module load foss/2021b Boost/1.77.0</code> Boost 1.83.0 foss/2023b <code>module load foss/2023b Boost/1.83.0</code> Boost 1.81.0 foss/2022b <code>module load foss/2022b Boost/1.81.0</code> Boost 1.79.0 foss/2021b <code>module load foss/2021b Boost/1.79.0</code> Boost.MPI 1.77.0 foss/2021b <code>module load foss/2021b Boost.MPI/1.77.0</code> Boost.Python 1.77.0 foss/2021b <code>module load foss/2021b Boost.Python/1.77.0</code> Brotli 1.0.9 foss/2021b <code>module load foss/2021b Brotli/1.0.9</code> Brotli 1.1.0 foss/2023b <code>module load foss/2023b Brotli/1.1.0</code> Brotli 1.0.9 foss/2022b <code>module load foss/2022b Brotli/1.0.9</code> Brunsli 0.1 foss/2023b <code>module load foss/2023b Brunsli/0.1</code> Brunsli 0.1 foss/2022b <code>module load foss/2022b Brunsli/0.1</code> bzip2 1.0.8 foss/2021b <code>module load foss/2021b bzip2/1.0.8</code> bzip2 1.0.8 foss/2023b <code>module load foss/2023b bzip2/1.0.8</code> bzip2 1.0.8 foss/2022b <code>module load foss/2022b bzip2/1.0.8</code> cairo 1.17.4 foss/2022b <code>module load foss/2022b cairo/1.17.4</code> cairo 1.18.0 foss/2023b <code>module load foss/2023b cairo/1.18.0</code> cairo 1.16.0 foss/2021b <code>module load foss/2021b cairo/1.16.0</code> Catch2 2.13.9 foss/2023b <code>module load foss/2023b Catch2/2.13.9</code> Cbc 2.10.11 foss/2023b <code>module load foss/2023b Cbc/2.10.11</code> cffi 1.15.1 foss/2023b <code>module load foss/2023b cffi/1.15.1</code> CFITSIO 4.3.1 foss/2023b <code>module load foss/2023b CFITSIO/4.3.1</code> CFITSIO 4.2.0 foss/2022b <code>module load foss/2022b CFITSIO/4.2.0</code> CGAL 5.5.2 foss/2022b <code>module load foss/2022b CGAL/5.5.2</code> CGAL 5.6.1 foss/2023b <code>module load foss/2023b CGAL/5.6.1</code> CGAL 4.14.3 foss/2021b <code>module load foss/2021b CGAL/4.14.3</code> Cgl 0.60.8 foss/2023b <code>module load foss/2023b Cgl/0.60.8</code> chapel 1.33.0 foss/2022b <code>module load foss/2022b chapel/1.33.0</code> Chapel 2.3.0 foss/2023b <code>module load foss/2023b Chapel/2.3.0</code> Chapel 2.1.0 foss/2023b <code>module load foss/2023b Chapel/2.1.0</code> Clang 12.0.1 foss/2021b <code>module load foss/2021b Clang/12.0.1</code> Clang 16.0.4 foss/2022b <code>module load foss/2022b Clang/16.0.4</code> Clp 1.17.9 foss/2023b <code>module load foss/2023b Clp/1.17.9</code> CMake 3.22.1 foss/2021b <code>module load foss/2021b CMake/3.22.1</code> CMake 3.27.6 foss/2023b <code>module load foss/2023b CMake/3.27.6</code> CMake 3.21.1 foss/2021b <code>module load foss/2021b CMake/3.21.1</code> CMake 3.29.3 foss/2023b <code>module load foss/2023b CMake/3.29.3</code> CMake 3.24.3 foss/2022b <code>module load foss/2022b CMake/3.24.3</code> CoinUtils 2.11.10 foss/2023b <code>module load foss/2023b CoinUtils/2.11.10</code> COMSOL 6.2 - <code>module load COMSOL/6.2</code> COMSOL 5.6 - <code>module load COMSOL/5.6</code> CP2K 2023.1 foss/2022b <code>module load foss/2022b CP2K/2023.1</code> CP2K 8.2 intel/2021b <code>module load intel/2021b CP2K/8.2</code> cppy 1.1.0 foss/2021b <code>module load foss/2021b cppy/1.1.0</code> cppy 1.2.1 foss/2023b <code>module load foss/2023b cppy/1.2.1</code> cppy 1.2.1 foss/2022b <code>module load foss/2022b cppy/1.2.1</code> CREST 2.12 foss/2022b <code>module load foss/2022b CREST/2.12</code> cryptography 41.0.5 foss/2023b <code>module load foss/2023b cryptography/41.0.5</code> CUDA 11.4.1 - <code>module load CUDA/11.4.1</code> CUDA 12.4.0 - <code>module load CUDA/12.4.0</code> CUDA 12.0.0 - <code>module load CUDA/12.0.0</code> cuDNN 8.8.0.121-CUDA-12.0.0 - <code>module load cuDNN/8.8.0.121-CUDA-12.0.0</code> cURL 7.86.0 foss/2022b <code>module load foss/2022b cURL/7.86.0</code> cURL 7.78.0 foss/2021b <code>module load foss/2021b cURL/7.78.0</code> cURL 8.3.0 foss/2023b <code>module load foss/2023b cURL/8.3.0</code> DB 18.1.40 foss/2022b <code>module load foss/2022b DB/18.1.40</code> DB 18.1.40 foss/2021b <code>module load foss/2021b DB/18.1.40</code> DBus 1.15.8 foss/2023b <code>module load foss/2023b DBus/1.15.8</code> DBus 1.15.2 foss/2022b <code>module load foss/2022b DBus/1.15.2</code> DBus 1.13.18 foss/2021b <code>module load foss/2021b DBus/1.13.18</code> DMTCP 2.6.0 - <code>module load DMTCP/2.6.0</code> DMTCP 3.0.0 - <code>module load DMTCP/3.0.0</code> double-conversion 3.3.0 foss/2023b <code>module load foss/2023b double-conversion/3.3.0</code> double-conversion 3.2.1 foss/2022b <code>module load foss/2022b double-conversion/3.2.1</code> double-conversion 3.1.5 foss/2021b <code>module load foss/2021b double-conversion/3.1.5</code> Doxygen 1.9.8 foss/2023b <code>module load foss/2023b Doxygen/1.9.8</code> Doxygen 1.9.1 foss/2021b <code>module load foss/2021b Doxygen/1.9.1</code> Doxygen 1.9.5 foss/2022b <code>module load foss/2022b Doxygen/1.9.5</code> EasyBuild 4.8.2 - <code>module load EasyBuild/4.8.2</code> EasyBuild 4.9.0 - <code>module load EasyBuild/4.9.0</code> EasyBuild 5.0.0 - <code>module load EasyBuild/5.0.0</code> EasyBuild 4.8.0 - <code>module load EasyBuild/4.8.0</code> EasyBuild 4.9.1 - <code>module load EasyBuild/4.9.1</code> Eigen 3.4.0 foss/2021b <code>module load foss/2021b Eigen/3.4.0</code> Eigen 3.4.0 foss/2023b <code>module load foss/2023b Eigen/3.4.0</code> Eigen 3.3.9 foss/2021b <code>module load foss/2021b Eigen/3.3.9</code> Eigen 3.4.0 foss/2022b <code>module load foss/2022b Eigen/3.4.0</code> elfutils 0.185 foss/2021b <code>module load foss/2021b elfutils/0.185</code> elfutils 0.189 foss/2022b <code>module load foss/2022b elfutils/0.189</code> ELPA 2021.11.001 intel/2021b <code>module load intel/2021b ELPA/2021.11.001</code> ELPA 2021.05.001 foss/2021b <code>module load foss/2021b ELPA/2021.05.001</code> expat 2.4.1 foss/2021b <code>module load foss/2021b expat/2.4.1</code> expat 2.5.0 foss/2023b <code>module load foss/2023b expat/2.5.0</code> expat 2.4.9 foss/2022b <code>module load foss/2022b expat/2.4.9</code> FFmpeg 4.3.2 foss/2021b <code>module load foss/2021b FFmpeg/4.3.2</code> FFmpeg 6.0 foss/2023b <code>module load foss/2023b FFmpeg/6.0</code> FFmpeg 6.1.1 foss/2022b <code>module load foss/2022b FFmpeg/6.1.1</code> FFmpeg 5.1.2 foss/2022b <code>module load foss/2022b FFmpeg/5.1.2</code> ffnvcodec 12.1.14.0 - <code>module load ffnvcodec/12.1.14.0</code> ffnvcodec 11.1.5.2 - <code>module load ffnvcodec/11.1.5.2</code> FFTW 3.3.10 foss/2023b <code>module load foss/2023b FFTW/3.3.10</code> FFTW 3.3.10 foss/2022b <code>module load foss/2022b FFTW/3.3.10</code> FFTW 3.3.10 foss/2021b <code>module load foss/2021b FFTW/3.3.10</code> FFTW.MPI 3.3.10 foss/2023b <code>module load foss/2023b FFTW.MPI/3.3.10</code> FFTW.MPI 3.3.10 foss/2022b <code>module load foss/2022b FFTW.MPI/3.3.10</code> Firefox 44.0.2 - <code>module load Firefox/44.0.2</code> FLAC 1.4.2 foss/2022b <code>module load foss/2022b FLAC/1.4.2</code> FLAC 1.3.3 foss/2021b <code>module load foss/2021b FLAC/1.3.3</code> flex 2.6.4 foss/2021b <code>module load foss/2021b flex/2.6.4</code> flex 2.6.4 foss/2023b <code>module load foss/2023b flex/2.6.4</code> flex 2.6.4 - <code>module load flex/2.6.4</code> flex 2.6.4 foss/2022b <code>module load foss/2022b flex/2.6.4</code> FlexiBLAS 3.0.4 foss/2021b <code>module load foss/2021b FlexiBLAS/3.0.4</code> FlexiBLAS 3.3.1 foss/2023b <code>module load foss/2023b FlexiBLAS/3.3.1</code> FlexiBLAS 3.2.1 foss/2022b <code>module load foss/2022b FlexiBLAS/3.2.1</code> FLINT 3.0.1 foss/2022b <code>module load foss/2022b FLINT/3.0.1</code> FLINT 2.9.0 foss/2022b <code>module load foss/2022b FLINT/2.9.0</code> flit 3.9.0 foss/2023b <code>module load foss/2023b flit/3.9.0</code> FLTK 1.3.7 foss/2021b <code>module load foss/2021b FLTK/1.3.7</code> FLTK 1.3.8 foss/2022b <code>module load foss/2022b FLTK/1.3.8</code> fontconfig 2.14.2 foss/2023b <code>module load foss/2023b fontconfig/2.14.2</code> fontconfig 2.14.1 foss/2022b <code>module load foss/2022b fontconfig/2.14.1</code> fontconfig 2.13.94 foss/2021b <code>module load foss/2021b fontconfig/2.13.94</code> foss 2021b - <code>module load foss/2021b</code> foss 2022b - <code>module load foss/2022b</code> foss 2023b - <code>module load foss/2023b</code> freetype 2.11.0 foss/2021b <code>module load foss/2021b freetype/2.11.0</code> freetype 2.13.2 foss/2023b <code>module load foss/2023b freetype/2.13.2</code> freetype 2.12.1 foss/2022b <code>module load foss/2022b freetype/2.12.1</code> FriBidi 1.0.10 foss/2021b <code>module load foss/2021b FriBidi/1.0.10</code> FriBidi 1.0.13 foss/2023b <code>module load foss/2023b FriBidi/1.0.13</code> FriBidi 1.0.12 foss/2022b <code>module load foss/2022b FriBidi/1.0.12</code> Gaussian 16.C.01-AVX2 - <code>module load Gaussian/16.C.01-AVX2</code> GCC 11.2.0 foss/2021b <code>module load GCC/11.2.0</code> GCC 12.2.0 foss/2022b <code>module load GCC/12.2.0</code> GCC 13.2.0 foss/2023b <code>module load GCC/13.2.0</code> GCCcore 11.2.0 - <code>module load GCCcore/11.2.0</code> GCCcore 13.2.0 - <code>module load GCCcore/13.2.0</code> GCCcore 11.3.0 - <code>module load GCCcore/11.3.0</code> GCCcore 12.2.0 - <code>module load GCCcore/12.2.0</code> GDAL 3.9.0 foss/2023b <code>module load foss/2023b GDAL/3.9.0</code> GDAL 3.6.2 foss/2022b <code>module load foss/2022b GDAL/3.6.2</code> GDAL 3.3.2 foss/2021b <code>module load foss/2021b GDAL/3.3.2</code> Gdk-Pixbuf 2.42.10 foss/2022b <code>module load foss/2022b Gdk-Pixbuf/2.42.10</code> GDRCopy 2.3 foss/2021b <code>module load foss/2021b GDRCopy/2.3</code> GDRCopy 2.3 foss/2022b <code>module load foss/2022b GDRCopy/2.3</code> GEOS 3.11.1 foss/2022b <code>module load foss/2022b GEOS/3.11.1</code> GEOS 3.12.1 foss/2023b <code>module load foss/2023b GEOS/3.12.1</code> GEOS 3.9.1 foss/2021b <code>module load foss/2021b GEOS/3.9.1</code> gettext 0.22 foss/2023b <code>module load foss/2023b gettext/0.22</code> gettext 0.21 foss/2021b <code>module load foss/2021b gettext/0.21</code> gettext 0.21 - <code>module load gettext/0.21</code> gettext 0.21.1 foss/2022b <code>module load foss/2022b gettext/0.21.1</code> gettext 0.22 - <code>module load gettext/0.22</code> gettext 0.21.1 - <code>module load gettext/0.21.1</code> gfbf 2022b - <code>module load gfbf/2022b</code> gfbf 2023b - <code>module load gfbf/2023b</code> Ghostscript 9.54.0 foss/2021b <code>module load foss/2021b Ghostscript/9.54.0</code> Ghostscript 10.0.0 foss/2022b <code>module load foss/2022b Ghostscript/10.0.0</code> giflib 5.2.1 foss/2022b <code>module load foss/2022b giflib/5.2.1</code> giflib 5.2.1 foss/2023b <code>module load foss/2023b giflib/5.2.1</code> git 2.38.1-nodocs foss/2022b <code>module load foss/2022b git/2.38.1-nodocs</code> git 2.42.0 foss/2023b <code>module load foss/2023b git/2.42.0</code> git 2.33.1-nodocs foss/2021b <code>module load foss/2021b git/2.33.1-nodocs</code> git 2.33.1 - <code>module load git/2.33.1</code> git-lfs 3.2.0 - <code>module load git-lfs/3.2.0</code> glew 2.2.0-egl foss/2022b <code>module load foss/2022b glew/2.2.0-egl</code> glew 2.2.0-osmesa foss/2022b <code>module load foss/2022b glew/2.2.0-osmesa</code> GLib 2.78.1 foss/2023b <code>module load foss/2023b GLib/2.78.1</code> GLib 2.69.1 foss/2021b <code>module load foss/2021b GLib/2.69.1</code> GLib 2.75.0 foss/2022b <code>module load foss/2022b GLib/2.75.0</code> GLPK 5.0 foss/2021b <code>module load foss/2021b GLPK/5.0</code> GLPK 5.0 foss/2022b <code>module load foss/2022b GLPK/5.0</code> GLPK 5.0 foss/2023b <code>module load foss/2023b GLPK/5.0</code> GMP 6.2.1 foss/2021b <code>module load foss/2021b GMP/6.2.1</code> GMP 6.3.0 foss/2023b <code>module load foss/2023b GMP/6.3.0</code> GMP 6.2.1 foss/2022b <code>module load foss/2022b GMP/6.2.1</code> gnuplot 5.4.2 foss/2021b <code>module load foss/2021b gnuplot/5.4.2</code> gnuplot 6.0.1 foss/2023b <code>module load foss/2023b gnuplot/6.0.1</code> gnuplot 5.4.6 foss/2022b <code>module load foss/2022b gnuplot/5.4.6</code> Go 1.17.3 - <code>module load Go/1.17.3</code> Go 1.17.6 - <code>module load Go/1.17.6</code> GObject-Introspection 1.68.0 foss/2021b <code>module load foss/2021b GObject-Introspection/1.68.0</code> GObject-Introspection 1.74.0 foss/2022b <code>module load foss/2022b GObject-Introspection/1.74.0</code> GObject-Introspection 1.78.1 foss/2023b <code>module load foss/2023b GObject-Introspection/1.78.1</code> gompi 2022b - <code>module load gompi/2022b</code> gompi 2023b - <code>module load gompi/2023b</code> gompi 2021b - <code>module load gompi/2021b</code> googletest 1.14.0 foss/2023b <code>module load foss/2023b googletest/1.14.0</code> googletest 1.12.1 foss/2022b <code>module load foss/2022b googletest/1.12.1</code> gperf 3.1 foss/2022b <code>module load foss/2022b gperf/3.1</code> gperf 3.1 foss/2023b <code>module load foss/2023b gperf/3.1</code> gperf 3.1 foss/2021b <code>module load foss/2021b gperf/3.1</code> Grace 5.1.25 foss/2022b <code>module load foss/2022b Grace/5.1.25</code> Graphene 1.10.8 foss/2022b <code>module load foss/2022b Graphene/1.10.8</code> graphite2 1.3.14 foss/2022b <code>module load foss/2022b graphite2/1.3.14</code> graphite2 1.3.14 foss/2023b <code>module load foss/2023b graphite2/1.3.14</code> graphite2 1.3.14 foss/2021b <code>module load foss/2021b graphite2/1.3.14</code> groff 1.23.0 foss/2023b <code>module load foss/2023b groff/1.23.0</code> groff 1.22.4 foss/2021b <code>module load foss/2021b groff/1.22.4</code> groff 1.22.4 foss/2022b <code>module load foss/2022b groff/1.22.4</code> GROMACS 2021.5-CUDA-11.4.1 foss/2021b <code>module load foss/2021b GROMACS/2021.5-CUDA-11.4.1</code> GROMACS 2024.1 foss/2023b <code>module load foss/2023b GROMACS/2024.1</code> GROMACS 2023.1-CUDA-12.0.0 foss/2022b <code>module load foss/2022b GROMACS/2023.1-CUDA-12.0.0</code> GROMACS 2021.5 foss/2021b <code>module load foss/2021b GROMACS/2021.5</code> GROMACS 2024.4-CUDA-12.4.0 foss/2022b <code>module load foss/2022b GROMACS/2024.4-CUDA-12.4.0</code> GSL 2.7 foss/2023b <code>module load foss/2023b GSL/2.7</code> GSL 2.7 intel/2021b <code>module load intel/2021b GSL/2.7</code> GSL 2.7 foss/2022b <code>module load foss/2022b GSL/2.7</code> GSL 2.7 foss/2021b <code>module load foss/2021b GSL/2.7</code> GSL 2.7 foss/2022b <code>module load foss/2022b GSL/2.7</code> GST-plugins-base 1.22.1 foss/2022b <code>module load foss/2022b GST-plugins-base/1.22.1</code> GStreamer 1.22.1 foss/2022b <code>module load foss/2022b GStreamer/1.22.1</code> GTK3 3.24.35 foss/2022b <code>module load foss/2022b GTK3/3.24.35</code> Gurobi 10.0.1 foss/2022b <code>module load foss/2022b Gurobi/10.0.1</code> Gurobi 11.0.1 foss/2023b <code>module load foss/2023b Gurobi/11.0.1</code> Gurobi_license license - <code>module load Gurobi_license/license</code> gzip 1.13 foss/2023b <code>module load foss/2023b gzip/1.13</code> gzip 1.10 foss/2021b <code>module load foss/2021b gzip/1.10</code> gzip 1.12 foss/2022b <code>module load foss/2022b gzip/1.12</code> HarfBuzz 2.8.2 foss/2021b <code>module load foss/2021b HarfBuzz/2.8.2</code> HarfBuzz 5.3.1 foss/2022b <code>module load foss/2022b HarfBuzz/5.3.1</code> HarfBuzz 8.2.2 foss/2023b <code>module load foss/2023b HarfBuzz/8.2.2</code> hatch-jupyter-builder 0.9.1 foss/2023b <code>module load foss/2023b hatch-jupyter-builder/0.9.1</code> hatchling 1.18.0 foss/2023b <code>module load foss/2023b hatchling/1.18.0</code> HDF 4.2.15 foss/2021b <code>module load foss/2021b HDF/4.2.15</code> HDF 4.2.16-2 foss/2023b <code>module load foss/2023b HDF/4.2.16-2</code> HDF 4.2.15 foss/2022b <code>module load foss/2022b HDF/4.2.15</code> HDF5 1.14.0 intel/2022b <code>module load intel/2022b HDF5/1.14.0</code> HDF5 1.14.0 foss/2022b <code>module load foss/2022b HDF5/1.14.0</code> HDF5 1.12.1 intel/2021b <code>module load intel/2021b HDF5/1.12.1</code> HDF5 1.14.3 foss/2023b <code>module load foss/2023b HDF5/1.14.3</code> HDF5 1.12.1 foss/2021b <code>module load foss/2021b HDF5/1.12.1</code> help2man 1.49.2 foss/2022b <code>module load foss/2022b help2man/1.49.2</code> help2man 1.49.3 foss/2023b <code>module load foss/2023b help2man/1.49.3</code> help2man 1.48.3 foss/2021b <code>module load foss/2021b help2man/1.48.3</code> HiGHS 1.7.0 foss/2023b <code>module load foss/2023b HiGHS/1.7.0</code> Highway 1.0.3 foss/2022b <code>module load foss/2022b Highway/1.0.3</code> HPL 2.3 intel/2022b <code>module load intel/2022b HPL/2.3</code> HPL 2.3 foss/2022b <code>module load foss/2022b HPL/2.3</code> hwloc 2.8.0 foss/2022b <code>module load foss/2022b hwloc/2.8.0</code> hwloc 2.9.2 foss/2023b <code>module load foss/2023b hwloc/2.9.2</code> hwloc 2.5.0 foss/2021b <code>module load foss/2021b hwloc/2.5.0</code> hypothesis 6.68.2 foss/2022b <code>module load foss/2022b hypothesis/6.68.2</code> hypothesis 6.90.0 foss/2023b <code>module load foss/2023b hypothesis/6.90.0</code> hypothesis 6.14.6 foss/2021b <code>module load foss/2021b hypothesis/6.14.6</code> Hypre 2.27.0 foss/2022b <code>module load foss/2022b Hypre/2.27.0</code> ICU 69.1 foss/2021b <code>module load foss/2021b ICU/69.1</code> ICU 72.1 foss/2022b <code>module load foss/2022b ICU/72.1</code> ICU 74.1 foss/2023b <code>module load foss/2023b ICU/74.1</code> iimpi 2023b - <code>module load iimpi/2023b</code> iimpi 2022a - <code>module load iimpi/2022a</code> iimpi 2021b - <code>module load iimpi/2021b</code> iimpi 2022b - <code>module load iimpi/2022b</code> ImageMagick 7.1.0-4 foss/2021b <code>module load foss/2021b ImageMagick/7.1.0-4</code> ImageMagick 7.1.0-53 foss/2022b <code>module load foss/2022b ImageMagick/7.1.0-53</code> Imath 3.1.9 foss/2023b <code>module load foss/2023b Imath/3.1.9</code> Imath 3.1.6 foss/2022b <code>module load foss/2022b Imath/3.1.6</code> imkl 2023.2.0 - <code>module load imkl/2023.2.0</code> imkl 2022.2.1 - <code>module load imkl/2022.2.1</code> imkl 2022.1.0 - <code>module load imkl/2022.1.0</code> imkl 2021.4.0 - <code>module load imkl/2021.4.0</code> imkl-FFTW 2022.2.1 intel/2022b <code>module load intel/2022b imkl-FFTW/2022.2.1</code> imkl-FFTW 2021.4.0 intel/2021b <code>module load intel/2021b imkl-FFTW/2021.4.0</code> impi 2021.7.1 intel/2022b <code>module load intel/2022b</code> impi 2021.4.0 intel/2021b <code>module load intel/2021b</code> intel 2022a - <code>module load intel/2022a</code> intel 2023b - <code>module load intel/2023b</code> intel 2022b - <code>module load intel/2022b</code> intel 2021b - <code>module load intel/2021b</code> intel-compilers 2021.4.0 intel/2021b <code>module load intel-compilers/2021.4.0</code> intel-compilers 2022.2.1 intel/2022b <code>module load intel-compilers/2022.2.1</code> intltool 0.51.0 foss/2022b <code>module load foss/2022b intltool/0.51.0</code> intltool 0.51.0 foss/2021b <code>module load foss/2021b intltool/0.51.0</code> intltool 0.51.0 foss/2023b <code>module load foss/2023b intltool/0.51.0</code> IPython 8.17.2 foss/2023b <code>module load foss/2023b IPython/8.17.2</code> IQ-TREE 2.2.2.6 foss/2022b <code>module load foss/2022b IQ-TREE/2.2.2.6</code> JasPer 4.0.0 foss/2023b <code>module load foss/2023b JasPer/4.0.0</code> JasPer 4.0.0 foss/2022b <code>module load foss/2022b JasPer/4.0.0</code> JasPer 2.0.33 foss/2021b <code>module load foss/2021b JasPer/2.0.33</code> Java 11.0.16 - <code>module load Java/11.0.16</code> Java .modulerc - <code>module load Java/.modulerc</code> jbigkit 2.1 foss/2023b <code>module load foss/2023b jbigkit/2.1</code> jbigkit 2.1 foss/2022b <code>module load foss/2022b jbigkit/2.1</code> jbigkit 2.1 foss/2021b <code>module load foss/2021b jbigkit/2.1</code> jedi 0.19.1 foss/2023b <code>module load foss/2023b jedi/0.19.1</code> jemalloc 5.3.0 foss/2022b <code>module load foss/2022b jemalloc/5.3.0</code> json-c 0.16 foss/2022b <code>module load foss/2022b json-c/0.16</code> json-c 0.17 foss/2023b <code>module load foss/2023b json-c/0.17</code> jupyter-server 2.14.0 foss/2023b <code>module load foss/2023b jupyter-server/2.14.0</code> JupyterLab 4.2.0 foss/2023b <code>module load foss/2023b JupyterLab/4.2.0</code> JupyterNotebook 7.2.0 foss/2023b <code>module load foss/2023b JupyterNotebook/7.2.0</code> KaHIP 3.14 foss/2022b <code>module load foss/2022b KaHIP/3.14</code> KaHIP 3.16 foss/2023b <code>module load foss/2023b KaHIP/3.16</code> kim-api 2.3.0 foss/2023b <code>module load foss/2023b kim-api/2.3.0</code> kim-api 2.3.0 foss/2021b <code>module load foss/2021b kim-api/2.3.0</code> klayout 0.29.1 foss/2023b <code>module load foss/2023b klayout/0.29.1</code> LAME 3.100 foss/2023b <code>module load foss/2023b LAME/3.100</code> LAME 3.100 foss/2021b <code>module load foss/2021b LAME/3.100</code> LAME 3.100 foss/2022b <code>module load foss/2022b LAME/3.100</code> LAMMPS 23Jun2022-kokkos foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos</code> LAMMPS 23Jun2022-kokkos-CUDA-11.4.1 foss/2021b <code>module load foss/2021b LAMMPS/23Jun2022-kokkos-CUDA-11.4.1</code> LAMMPS 2Aug2023_update2-kokkos foss/2023b <code>module load foss/2023b LAMMPS/2Aug2023_update2-kokkos</code> LEMON 1.3.1 foss/2023b <code>module load foss/2023b LEMON/1.3.1</code> LERC 4.0.0 foss/2023b <code>module load foss/2023b LERC/4.0.0</code> LERC 4.0.0 foss/2022b <code>module load foss/2022b LERC/4.0.0</code> libarchive 3.5.1 foss/2021b <code>module load foss/2021b libarchive/3.5.1</code> libarchive 3.7.2 foss/2023b <code>module load foss/2023b libarchive/3.7.2</code> libarchive 3.6.1 foss/2022b <code>module load foss/2022b libarchive/3.6.1</code> libcerf 2.4 foss/2023b <code>module load foss/2023b libcerf/2.4</code> libcerf 1.17 foss/2021b <code>module load foss/2021b libcerf/1.17</code> libcerf 2.3 foss/2022b <code>module load foss/2022b libcerf/2.3</code> libdeflate 1.15 foss/2022b <code>module load foss/2022b libdeflate/1.15</code> libdeflate 1.19 foss/2023b <code>module load foss/2023b libdeflate/1.19</code> libdrm 2.4.114 foss/2022b <code>module load foss/2022b libdrm/2.4.114</code> libdrm 2.4.107 foss/2021b <code>module load foss/2021b libdrm/2.4.107</code> libdrm 2.4.117 foss/2023b <code>module load foss/2023b libdrm/2.4.117</code> libepoxy 1.5.10 foss/2022b <code>module load foss/2022b libepoxy/1.5.10</code> libevent 2.1.12 foss/2023b <code>module load foss/2023b libevent/2.1.12</code> libevent 2.1.12 - <code>module load libevent/2.1.12</code> libevent 2.1.12 foss/2022b <code>module load foss/2022b libevent/2.1.12</code> libevent 2.1.12 foss/2021b <code>module load foss/2021b libevent/2.1.12</code> libfabric 1.19.0 foss/2023b <code>module load foss/2023b libfabric/1.19.0</code> libfabric 1.13.2 foss/2021b <code>module load foss/2021b libfabric/1.13.2</code> libfabric 1.16.1 foss/2022b <code>module load foss/2022b libfabric/1.16.1</code> libffi 3.4.4 foss/2023b <code>module load foss/2023b libffi/3.4.4</code> libffi 3.4.2 foss/2021b <code>module load foss/2021b libffi/3.4.2</code> libffi 3.4.4 foss/2022b <code>module load foss/2022b libffi/3.4.4</code> libgd 2.3.3 foss/2023b <code>module load foss/2023b libgd/2.3.3</code> libgd 2.3.3 foss/2021b <code>module load foss/2021b libgd/2.3.3</code> libgd 2.3.3 foss/2022b <code>module load foss/2022b libgd/2.3.3</code> libgeotiff 1.7.1 foss/2022b <code>module load foss/2022b libgeotiff/1.7.1</code> libgeotiff 1.7.3 foss/2023b <code>module load foss/2023b libgeotiff/1.7.3</code> libgeotiff 1.7.0 foss/2021b <code>module load foss/2021b libgeotiff/1.7.0</code> libgit2 1.5.0 foss/2022b <code>module load foss/2022b libgit2/1.5.0</code> libgit2 1.7.2 foss/2023b <code>module load foss/2023b libgit2/1.7.2</code> libgit2 1.1.1 foss/2021b <code>module load foss/2021b libgit2/1.1.1</code> libGLU 9.0.2 foss/2022b <code>module load foss/2022b libGLU/9.0.2</code> libGLU 9.0.2 foss/2021b <code>module load foss/2021b libGLU/9.0.2</code> libGLU 9.0.3 foss/2023b <code>module load foss/2023b libGLU/9.0.3</code> libglvnd 1.7.0 foss/2023b <code>module load foss/2023b libglvnd/1.7.0</code> libglvnd 1.3.3 foss/2021b <code>module load foss/2021b libglvnd/1.3.3</code> libglvnd 1.6.0 foss/2022b <code>module load foss/2022b libglvnd/1.6.0</code> libiconv 1.17 foss/2023b <code>module load foss/2023b libiconv/1.17</code> libiconv 1.17 foss/2022b <code>module load foss/2022b libiconv/1.17</code> libiconv 1.16 foss/2021b <code>module load foss/2021b libiconv/1.16</code> libidn2 2.3.2 foss/2022b <code>module load foss/2022b libidn2/2.3.2</code> Libint 2.7.2-lmax-6-cp2k foss/2022b <code>module load foss/2022b Libint/2.7.2-lmax-6-cp2k</code> Libint 2.7.2-lmax-6-cp2k intel/2021b <code>module load intel/2021b Libint/2.7.2-lmax-6-cp2k</code> libjpeg-turbo 3.0.1 foss/2023b <code>module load foss/2023b libjpeg-turbo/3.0.1</code> libjpeg-turbo 2.1.4 foss/2022b <code>module load foss/2022b libjpeg-turbo/2.1.4</code> libjpeg-turbo 2.0.6 foss/2021b <code>module load foss/2021b libjpeg-turbo/2.0.6</code> libogg 1.3.5 foss/2021b <code>module load foss/2021b libogg/1.3.5</code> libogg 1.3.5 foss/2022b <code>module load foss/2022b libogg/1.3.5</code> libopus 1.3.1 foss/2022b <code>module load foss/2022b libopus/1.3.1</code> libpciaccess 0.17 foss/2022b <code>module load foss/2022b libpciaccess/0.17</code> libpciaccess 0.16 foss/2021b <code>module load foss/2021b libpciaccess/0.16</code> libpciaccess 0.17 foss/2023b <code>module load foss/2023b libpciaccess/0.17</code> libpng 1.6.37 foss/2021b <code>module load foss/2021b libpng/1.6.37</code> libpng 1.6.40 foss/2023b <code>module load foss/2023b libpng/1.6.40</code> libpng 1.6.38 foss/2022b <code>module load foss/2022b libpng/1.6.38</code> libreadline 8.1 foss/2021b <code>module load foss/2021b libreadline/8.1</code> libreadline 8.2 foss/2023b <code>module load foss/2023b libreadline/8.2</code> libreadline 8.2 foss/2022b <code>module load foss/2022b libreadline/8.2</code> libsndfile 1.2.0 foss/2022b <code>module load foss/2022b libsndfile/1.2.0</code> libsndfile 1.0.31 foss/2021b <code>module load foss/2021b libsndfile/1.0.31</code> libsodium 1.0.19 foss/2023b <code>module load foss/2023b libsodium/1.0.19</code> libsodium 1.0.18 foss/2022b <code>module load foss/2022b libsodium/1.0.18</code> LibTIFF 4.4.0 foss/2022b <code>module load foss/2022b LibTIFF/4.4.0</code> LibTIFF 4.3.0 foss/2021b <code>module load foss/2021b LibTIFF/4.3.0</code> LibTIFF 4.6.0 foss/2023b <code>module load foss/2023b LibTIFF/4.6.0</code> libtirpc 1.3.4 foss/2023b <code>module load foss/2023b libtirpc/1.3.4</code> libtirpc 1.3.3 foss/2022b <code>module load foss/2022b libtirpc/1.3.3</code> libtirpc 1.3.2 foss/2021b <code>module load foss/2021b libtirpc/1.3.2</code> libtool 2.4.6 foss/2021b <code>module load foss/2021b libtool/2.4.6</code> libtool 2.4.7 - <code>module load libtool/2.4.7</code> libtool 2.4.7 foss/2023b <code>module load foss/2023b libtool/2.4.7</code> libtool 2.4.7 foss/2022b <code>module load foss/2022b libtool/2.4.7</code> libunwind 1.5.0 foss/2021b <code>module load foss/2021b libunwind/1.5.0</code> libunwind 1.6.2 foss/2022b <code>module load foss/2022b libunwind/1.6.2</code> libunwind 1.6.2 foss/2023b <code>module load foss/2023b libunwind/1.6.2</code> libvorbis 1.3.7 foss/2022b <code>module load foss/2022b libvorbis/1.3.7</code> libvorbis 1.3.7 foss/2021b <code>module load foss/2021b libvorbis/1.3.7</code> libvori 220621 foss/2022b <code>module load foss/2022b libvori/220621</code> libwebp 1.3.2 foss/2023b <code>module load foss/2023b libwebp/1.3.2</code> libxc 5.2.3 intel/2022b <code>module load intel/2022b libxc/5.2.3</code> libxc 6.1.0 intel/2022b <code>module load intel/2022b libxc/6.1.0</code> libxc 4.3.4 intel/2021b <code>module load intel/2021b libxc/4.3.4</code> libxc 6.1.0 foss/2022b <code>module load foss/2022b libxc/6.1.0</code> libxc 5.1.6 intel/2021b <code>module load intel/2021b libxc/5.1.6</code> libxc 5.1.6 foss/2021b <code>module load foss/2021b libxc/5.1.6</code> libxml2 2.9.10 foss/2021b <code>module load foss/2021b libxml2/2.9.10</code> libxml2 2.11.5 foss/2023b <code>module load foss/2023b libxml2/2.11.5</code> libxml2 2.10.3 foss/2022b <code>module load foss/2022b libxml2/2.10.3</code> libxslt 1.1.37 foss/2022b <code>module load foss/2022b libxslt/1.1.37</code> libxslt 1.1.38 foss/2023b <code>module load foss/2023b libxslt/1.1.38</code> libxsmm 1.16.2 intel/2022b <code>module load intel/2022b libxsmm/1.16.2</code> libxsmm 1.17 foss/2022b <code>module load foss/2022b libxsmm/1.17</code> libxsmm 1.17 intel/2021b <code>module load intel/2021b libxsmm/1.17</code> libyaml 0.2.5 foss/2023b <code>module load foss/2023b libyaml/0.2.5</code> LIGGGHTS-PUBLIC 3.8.0-kokkos-CUDA-11.4.1 foss/2021b <code>module load foss/2021b LIGGGHTS-PUBLIC/3.8.0-kokkos-CUDA-11.4.1</code> LittleCMS 2.15 foss/2023b <code>module load foss/2023b LittleCMS/2.15</code> LittleCMS 2.12 foss/2021b <code>module load foss/2021b LittleCMS/2.12</code> LittleCMS 2.14 foss/2022b <code>module load foss/2022b LittleCMS/2.14</code> LLVM 15.0.5 foss/2022b <code>module load foss/2022b LLVM/15.0.5</code> LLVM 16.0.6 foss/2023b <code>module load foss/2023b LLVM/16.0.6</code> LLVM 12.0.1 foss/2021b <code>module load foss/2021b LLVM/12.0.1</code> LSD2 2.4.1 foss/2022b <code>module load foss/2022b LSD2/2.4.1</code> Lua 5.4.4 foss/2022b <code>module load foss/2022b Lua/5.4.4</code> Lua 5.4.3 foss/2021b <code>module load foss/2021b Lua/5.4.3</code> Lua 5.4.6 foss/2023b <code>module load foss/2023b Lua/5.4.6</code> lxml 4.9.3 foss/2023b <code>module load foss/2023b lxml/4.9.3</code> lz4 1.9.3 foss/2021b <code>module load foss/2021b lz4/1.9.3</code> lz4 1.9.4 foss/2023b <code>module load foss/2023b lz4/1.9.4</code> lz4 1.9.4 foss/2022b <code>module load foss/2022b lz4/1.9.4</code> LZO 2.10 foss/2022b <code>module load foss/2022b LZO/2.10</code> M4 1.4.19 foss/2021b <code>module load foss/2021b M4/1.4.19</code> M4 1.4.18 - <code>module load M4/1.4.18</code> M4 1.4.19 - <code>module load M4/1.4.19</code> M4 1.4.19 foss/2023b <code>module load foss/2023b M4/1.4.19</code> M4 1.4.19 foss/2022b <code>module load foss/2022b M4/1.4.19</code> magma 2.6.2-CUDA-11.4.1 foss/2021b <code>module load foss/2021b magma/2.6.2-CUDA-11.4.1</code> make 4.4.1 foss/2023b <code>module load foss/2023b make/4.4.1</code> make 4.3 foss/2021b <code>module load foss/2021b make/4.3</code> makeinfo 7.1 foss/2023b <code>module load foss/2023b makeinfo/7.1</code> Mako 1.2.4 foss/2022b <code>module load foss/2022b Mako/1.2.4</code> Mako 1.2.4 foss/2023b <code>module load foss/2023b Mako/1.2.4</code> Mako 1.1.4 foss/2021b <code>module load foss/2021b Mako/1.1.4</code> Mamba 4.14.0-0 - <code>module load Mamba/4.14.0-0</code> MATLAB 2023a - <code>module load MATLAB/2023a</code> MATLAB 2024a - <code>module load MATLAB/2024a</code> matlab-proxy 0.24.2 foss/2023b <code>module load foss/2023b matlab-proxy/0.24.2</code> matplotlib 3.8.2 foss/2023b <code>module load foss/2023b matplotlib/3.8.2</code> matplotlib 3.4.3 intel/2021b <code>module load intel/2021b matplotlib/3.4.3</code> matplotlib 3.7.0 foss/2022b <code>module load foss/2022b matplotlib/3.7.0</code> matplotlib 3.4.3 foss/2021b <code>module load foss/2021b matplotlib/3.4.3</code> maturin 1.3.1 foss/2023b <code>module load foss/2023b maturin/1.3.1</code> Maven 3.6.3 - <code>module load Maven/3.6.3</code> MDAnalysis 2.4.2 foss/2022b <code>module load foss/2022b MDAnalysis/2.4.2</code> MDAnalysis 2.0.0 foss/2021b <code>module load foss/2021b MDAnalysis/2.0.0</code> MDI 1.4.29 foss/2023b <code>module load foss/2023b MDI/1.4.29</code> Mesa 23.1.9 foss/2023b <code>module load foss/2023b Mesa/23.1.9</code> Mesa 21.1.7 foss/2021b <code>module load foss/2021b Mesa/21.1.7</code> Mesa 22.2.4 foss/2022b <code>module load foss/2022b Mesa/22.2.4</code> Meson 0.58.2 foss/2021b <code>module load foss/2021b Meson/0.58.2</code> Meson 1.2.3 foss/2023b <code>module load foss/2023b Meson/1.2.3</code> Meson 0.64.0 foss/2022b <code>module load foss/2022b Meson/0.64.0</code> meson-python 0.15.0 foss/2023b <code>module load foss/2023b meson-python/0.15.0</code> METIS 5.1.0 foss/2023b <code>module load foss/2023b METIS/5.1.0</code> METIS 5.1.0 foss/2022b <code>module load foss/2022b METIS/5.1.0</code> METIS 5.1.0 foss/2021b <code>module load foss/2021b METIS/5.1.0</code> Miniforge3 24.1.2-0 - <code>module load Miniforge3/24.1.2-0</code> motif 2.3.8 foss/2021b <code>module load foss/2021b motif/2.3.8</code> motif 2.3.8 foss/2022b <code>module load foss/2022b motif/2.3.8</code> MPAS 8.0.1 foss/2022b <code>module load foss/2022b MPAS/8.0.1</code> MPFR 4.2.0 foss/2022b <code>module load foss/2022b MPFR/4.2.0</code> MPFR 4.1.0 foss/2021b <code>module load foss/2021b MPFR/4.1.0</code> MPFR 4.2.1 foss/2023b <code>module load foss/2023b MPFR/4.2.1</code> mpi4py 3.1.5 foss/2023b <code>module load foss/2023b mpi4py/3.1.5</code> mpi4py 3.1.4 foss/2022b <code>module load foss/2022b mpi4py/3.1.4</code> MUMPS 5.4.1-metis foss/2021b <code>module load foss/2021b MUMPS/5.4.1-metis</code> MUMPS 5.6.1-metis foss/2023b <code>module load foss/2023b MUMPS/5.6.1-metis</code> MUMPS 5.6.1-metis foss/2022b <code>module load foss/2022b MUMPS/5.6.1-metis</code> NASM 2.15.05 foss/2022b <code>module load foss/2022b NASM/2.15.05</code> NASM 2.15.05 foss/2021b <code>module load foss/2021b NASM/2.15.05</code> NASM 2.16.01 foss/2023b <code>module load foss/2023b NASM/2.16.01</code> NCCL 2.10.3-CUDA-11.4.1 foss/2021b <code>module load foss/2021b NCCL/2.10.3-CUDA-11.4.1</code> ncdu 1.20 foss/2023b <code>module load foss/2023b ncdu/1.20</code> ncurses 6.4 foss/2023b <code>module load foss/2023b ncurses/6.4</code> ncurses 6.2 - <code>module load ncurses/6.2</code> ncurses 6.4 - <code>module load ncurses/6.4</code> ncurses 6.3 - <code>module load ncurses/6.3</code> ncurses 6.3 foss/2022b <code>module load foss/2022b ncurses/6.3</code> ncurses 6.2 foss/2021b <code>module load foss/2021b ncurses/6.2</code> ncview 2.1.8 foss/2022b <code>module load foss/2022b ncview/2.1.8</code> netCDF 4.9.2 foss/2023b <code>module load foss/2023b netCDF/4.9.2</code> netCDF 4.8.1 intel/2021b <code>module load intel/2021b netCDF/4.8.1</code> netCDF 4.8.1 foss/2021b <code>module load foss/2021b netCDF/4.8.1</code> netCDF 4.9.0 foss/2022b <code>module load foss/2022b netCDF/4.9.0</code> netCDF-C++4 4.3.1 foss/2023b <code>module load foss/2023b netCDF-C++4/4.3.1</code> netCDF-Fortran 4.6.0 foss/2022b <code>module load foss/2022b netCDF-Fortran/4.6.0</code> netCDF-Fortran 4.5.3 foss/2021b <code>module load foss/2021b netCDF-Fortran/4.5.3</code> netCDF-Fortran 4.5.3 intel/2021b <code>module load intel/2021b netCDF-Fortran/4.5.3</code> nettle 3.7.3 foss/2021b <code>module load foss/2021b nettle/3.7.3</code> nettle 3.9.1 foss/2023b <code>module load foss/2023b nettle/3.9.1</code> nettle 3.8.1 foss/2022b <code>module load foss/2022b nettle/3.8.1</code> networkx 2.6.3 foss/2021b <code>module load foss/2021b networkx/2.6.3</code> networkx 3.0 foss/2022b <code>module load foss/2022b networkx/3.0</code> networkx 2.8.8 foss/2022b <code>module load foss/2022b networkx/2.8.8</code> networkx 3.2.1 foss/2023b <code>module load foss/2023b networkx/3.2.1</code> Ninja 1.11.1 foss/2022b <code>module load foss/2022b Ninja/1.11.1</code> Ninja 1.10.2 foss/2021b <code>module load foss/2021b Ninja/1.10.2</code> Ninja 1.11.1 foss/2023b <code>module load foss/2023b Ninja/1.11.1</code> nlohmann_json 3.11.2 foss/2022b <code>module load foss/2022b nlohmann_json/3.11.2</code> nlohmann_json 3.11.3 foss/2023b <code>module load foss/2023b nlohmann_json/3.11.3</code> NLopt 2.7.1 foss/2022b <code>module load foss/2022b NLopt/2.7.1</code> NLopt 2.7.0 foss/2021b <code>module load foss/2021b NLopt/2.7.0</code> nodejs 14.17.6 foss/2021b <code>module load foss/2021b nodejs/14.17.6</code> nodejs 20.9.0 foss/2023b <code>module load foss/2023b nodejs/20.9.0</code> nodejs 18.12.1 foss/2022b <code>module load foss/2022b nodejs/18.12.1</code> NSPR 4.35 foss/2022b <code>module load foss/2022b NSPR/4.35</code> NSPR 4.32 foss/2021b <code>module load foss/2021b NSPR/4.32</code> NSPR 4.35 foss/2023b <code>module load foss/2023b NSPR/4.35</code> NSS 3.94 foss/2023b <code>module load foss/2023b NSS/3.94</code> NSS 3.85 foss/2022b <code>module load foss/2022b NSS/3.85</code> NSS 3.69 foss/2021b <code>module load foss/2021b NSS/3.69</code> NTL 11.5.1 foss/2022b <code>module load foss/2022b NTL/11.5.1</code> numactl 2.0.14 foss/2021b <code>module load foss/2021b numactl/2.0.14</code> numactl 2.0.16 foss/2022b <code>module load foss/2022b numactl/2.0.16</code> numactl 2.0.16 foss/2023b <code>module load foss/2023b numactl/2.0.16</code> NVHPC 23.1-CUDA-12.0.0 - <code>module load NVHPC/23.1-CUDA-12.0.0</code> OpenBLAS 0.3.21 foss/2022b <code>module load foss/2022b OpenBLAS/0.3.21</code> OpenBLAS 0.3.24 foss/2023b <code>module load foss/2023b OpenBLAS/0.3.24</code> OpenBLAS 0.3.18 foss/2021b <code>module load foss/2021b OpenBLAS/0.3.18</code> opendihu 1.5 - <code>module load opendihu/1.5</code> opendihu 2204 - <code>module load opendihu/2204</code> OpenEXR 3.1.1 foss/2021b <code>module load foss/2021b OpenEXR/3.1.1</code> OpenEXR 3.1.5 foss/2022b <code>module load foss/2022b OpenEXR/3.1.5</code> OpenEXR 3.2.0 foss/2023b <code>module load foss/2023b OpenEXR/3.2.0</code> OpenFOAM v2406 foss/2023b <code>module load foss/2023b OpenFOAM/v2406</code> OpenFOAM v2112 foss/2021b <code>module load foss/2021b OpenFOAM/v2112</code> OpenFOAM v2306 foss/2022b <code>module load foss/2022b OpenFOAM/v2306</code> OpenJPEG 2.5.0 foss/2023b <code>module load foss/2023b OpenJPEG/2.5.0</code> OpenJPEG 2.5.0 foss/2022b <code>module load foss/2022b OpenJPEG/2.5.0</code> OpenJPEG 2.4.0 foss/2021b <code>module load foss/2021b OpenJPEG/2.4.0</code> OpenMPI 4.1.4 foss/2022b <code>module load foss/2022b</code> OpenMPI 4.1.6 foss/2023b <code>module load foss/2023b</code> OpenMPI 4.1.1 foss/2021b <code>module load foss/2021b</code> OpenPGM 5.2.122 foss/2023b <code>module load foss/2023b OpenPGM/5.2.122</code> OpenPGM 5.2.122 foss/2022b <code>module load foss/2022b OpenPGM/5.2.122</code> OpenSSL 1.1 - <code>module load OpenSSL/1.1</code> OptiX 6.5.0 - <code>module load OptiX/6.5.0</code> OR-Tools 9.9 foss/2023b <code>module load foss/2023b OR-Tools/9.9</code> ORCA 5.0.4 foss/2022b <code>module load foss/2022b ORCA/5.0.4</code> Osi 0.108.10 foss/2023b <code>module load foss/2023b Osi/0.108.10</code> OVITO 3.7.11-basic foss/2021b <code>module load foss/2021b OVITO/3.7.11-basic</code> p7zip 17.04 - <code>module load p7zip/17.04</code> packmol 20.14.3 foss/2022b <code>module load foss/2022b packmol/20.14.3</code> Pango 1.50.12 foss/2022b <code>module load foss/2022b Pango/1.50.12</code> Pango 1.48.8 foss/2021b <code>module load foss/2021b Pango/1.48.8</code> Pango 1.51.0 foss/2023b <code>module load foss/2023b Pango/1.51.0</code> parallel 20230722 foss/2022b <code>module load foss/2022b parallel/20230722</code> ParallelIO 2.5.10 foss/2022b <code>module load foss/2022b ParallelIO/2.5.10</code> ParaView 5.11.1 foss/2022b <code>module load foss/2022b ParaView/5.11.1</code> ParaView 5.11.0-mpi foss/2022b <code>module load foss/2022b ParaView/5.11.0-mpi</code> ParaView 5.11.0-egl - <code>module load ParaView/5.11.0-egl</code> ParaView 5.11.0-osmesa - <code>module load ParaView/5.11.0-osmesa</code> ParaView 5.11.2-osmesa - <code>module load ParaView/5.11.2-osmesa</code> ParaView 5.11.2-egl - <code>module load ParaView/5.11.2-egl</code> ParaView 5.12.0 foss/2023b <code>module load foss/2023b ParaView/5.12.0</code> ParaView 5.9.1-mpi foss/2021b <code>module load foss/2021b ParaView/5.9.1-mpi</code> ParMETIS 4.0.3 foss/2021b <code>module load foss/2021b ParMETIS/4.0.3</code> ParMETIS 4.0.3 foss/2022b <code>module load foss/2022b ParMETIS/4.0.3</code> patchelf 0.18.0 foss/2023b <code>module load foss/2023b patchelf/0.18.0</code> PCRE 8.45 foss/2023b <code>module load foss/2023b PCRE/8.45</code> PCRE 8.45 foss/2022b <code>module load foss/2022b PCRE/8.45</code> PCRE 8.45 foss/2021b <code>module load foss/2021b PCRE/8.45</code> PCRE2 10.40 foss/2022b <code>module load foss/2022b PCRE2/10.40</code> PCRE2 10.37 foss/2021b <code>module load foss/2021b PCRE2/10.37</code> PCRE2 10.42 foss/2023b <code>module load foss/2023b PCRE2/10.42</code> pdsh 2.34 foss/2022b <code>module load foss/2022b pdsh/2.34</code> Perl 5.36.0-minimal foss/2022b <code>module load foss/2022b Perl/5.36.0-minimal</code> Perl 5.34.0 foss/2021b <code>module load foss/2021b Perl/5.34.0</code> Perl 5.38.0 foss/2023b <code>module load foss/2023b Perl/5.38.0</code> Perl 5.36.0 foss/2022b <code>module load foss/2022b Perl/5.36.0</code> Perl-bundle-CPAN 5.38.0 foss/2023b <code>module load foss/2023b Perl-bundle-CPAN/5.38.0</code> Perl-bundle-njit 5.38.0 foss/2023b <code>module load foss/2023b Perl-bundle-njit/5.38.0</code> perm-md-count main foss/2021b <code>module load foss/2021b perm-md-count/main</code> perm-md-count main foss/2022b <code>module load foss/2022b perm-md-count/main</code> PETSc 3.19.2 foss/2022b <code>module load foss/2022b PETSc/3.19.2</code> Pillow 10.2.0 foss/2023b <code>module load foss/2023b Pillow/10.2.0</code> Pillow 8.3.2 foss/2021b <code>module load foss/2021b Pillow/8.3.2</code> Pillow 9.4.0 foss/2022b <code>module load foss/2022b Pillow/9.4.0</code> pixman 0.42.2 foss/2023b <code>module load foss/2023b pixman/0.42.2</code> pixman 0.42.2 foss/2022b <code>module load foss/2022b pixman/0.42.2</code> pixman 0.40.0 foss/2021b <code>module load foss/2021b pixman/0.40.0</code> pkg-config 0.29.2 foss/2021b <code>module load foss/2021b pkg-config/0.29.2</code> pkgconf 1.8.0 - <code>module load pkgconf/1.8.0</code> pkgconf 2.0.3 foss/2023b <code>module load foss/2023b pkgconf/2.0.3</code> pkgconf 1.8.0 foss/2021b <code>module load foss/2021b pkgconf/1.8.0</code> pkgconf 1.9.3 foss/2022b <code>module load foss/2022b pkgconf/1.9.3</code> pkgconfig 1.5.5-python foss/2023b <code>module load foss/2023b pkgconfig/1.5.5-python</code> pkgconfig 1.5.5-python foss/2021b <code>module load foss/2021b pkgconfig/1.5.5-python</code> PLUMED 2.9.2 foss/2023b <code>module load foss/2023b PLUMED/2.9.2</code> PLUMED 2.8.0 intel/2021b <code>module load intel/2021b PLUMED/2.8.0</code> PLUMED 2.9.0 foss/2022b <code>module load foss/2022b PLUMED/2.9.0</code> PLUMED 2.7.3 foss/2021b <code>module load foss/2021b PLUMED/2.7.3</code> PMIx 4.2.2 foss/2022b <code>module load foss/2022b PMIx/4.2.2</code> PMIx 4.1.0 foss/2021b <code>module load foss/2021b PMIx/4.1.0</code> PMIx 4.2.6 foss/2023b <code>module load foss/2023b PMIx/4.2.6</code> PnetCDF 1.12.3 foss/2022b <code>module load foss/2022b PnetCDF/1.12.3</code> pocl 1.8 foss/2021b <code>module load foss/2021b pocl/1.8</code> pocl 4.0 foss/2022b <code>module load foss/2022b pocl/4.0</code> poetry 1.6.1 foss/2023b <code>module load foss/2023b poetry/1.6.1</code> poppler 22.01.0 foss/2021b <code>module load foss/2021b poppler/22.01.0</code> POV-Ray 3.7.0.10 foss/2021b <code>module load foss/2021b POV-Ray/3.7.0.10</code> POV-Ray 3.7.0.10 foss/2022b <code>module load foss/2022b POV-Ray/3.7.0.10</code> PROJ 8.1.0 foss/2021b <code>module load foss/2021b PROJ/8.1.0</code> PROJ 9.1.1 foss/2022b <code>module load foss/2022b PROJ/9.1.1</code> PROJ 9.3.1 foss/2023b <code>module load foss/2023b PROJ/9.3.1</code> protobuf 25.3 foss/2023b <code>module load foss/2023b protobuf/25.3</code> protobuf-python 4.25.3 foss/2023b <code>module load foss/2023b protobuf-python/4.25.3</code> py-cpuinfo 9.0.0 foss/2022b <code>module load foss/2022b py-cpuinfo/9.0.0</code> pybind11 2.11.1 foss/2023b <code>module load foss/2023b pybind11/2.11.1</code> pybind11 2.7.1 foss/2021b <code>module load foss/2021b pybind11/2.7.1</code> pybind11 2.10.3 foss/2022b <code>module load foss/2022b pybind11/2.10.3</code> PyCharm 2022.3.2 - <code>module load PyCharm/2022.3.2</code> PyInstaller 6.3.0 foss/2023b <code>module load foss/2023b PyInstaller/6.3.0</code> PyQt5 5.15.4 foss/2021b <code>module load foss/2021b PyQt5/5.15.4</code> PyTables 3.8.0 foss/2022b <code>module load foss/2022b PyTables/3.8.0</code> Python 3.9.6 foss/2021b <code>module load foss/2021b Python/3.9.6</code> Python 3.9.6-bare foss/2021b <code>module load foss/2021b Python/3.9.6-bare</code> Python 3.11.5 foss/2023b <code>module load foss/2023b Python/3.11.5</code> Python 2.7.18-bare foss/2021b <code>module load foss/2021b Python/2.7.18-bare</code> Python 3.10.8 foss/2022b <code>module load foss/2022b Python/3.10.8</code> Python 3.10.8-bare foss/2022b <code>module load foss/2022b Python/3.10.8-bare</code> Python 2.7.18-bare foss/2022b <code>module load foss/2022b Python/2.7.18-bare</code> Python-bundle-PyPI 2023.10 foss/2023b <code>module load foss/2023b Python-bundle-PyPI/2023.10</code> PyYAML 6.0.1 foss/2023b <code>module load foss/2023b PyYAML/6.0.1</code> PyZMQ 25.1.2 foss/2023b <code>module load foss/2023b PyZMQ/25.1.2</code> PyZMQ 25.1.0 foss/2022b <code>module load foss/2022b PyZMQ/25.1.0</code> Qhull 2020.2 foss/2022b <code>module load foss/2022b Qhull/2020.2</code> Qhull 2020.2 foss/2023b <code>module load foss/2023b Qhull/2020.2</code> Qhull 2020.2 foss/2021b <code>module load foss/2021b Qhull/2020.2</code> Qt5 5.15.13 foss/2023b <code>module load foss/2023b Qt5/5.15.13</code> Qt5 5.15.2 foss/2021b <code>module load foss/2021b Qt5/5.15.2</code> Qt5 5.15.7 foss/2022b <code>module load foss/2022b Qt5/5.15.7</code> Qt6 6.6.3 foss/2023b <code>module load foss/2023b Qt6/6.6.3</code> QuantumESPRESSO 7.1 foss/2021b <code>module load foss/2021b QuantumESPRESSO/7.1</code> R 4.2.2 foss/2022b <code>module load foss/2022b R/4.2.2</code> R 4.2.0 foss/2021b <code>module load foss/2021b R/4.2.0</code> R 4.4.1 foss/2023b <code>module load foss/2023b R/4.4.1</code> RapidJSON 1.1.0 foss/2022b <code>module load foss/2022b RapidJSON/1.1.0</code> RASPA2 2.0.47 foss/2022b <code>module load foss/2022b RASPA2/2.0.47</code> rclone 1.57.0 - <code>module load rclone/1.57.0</code> RE2 2023-03-01 foss/2022b <code>module load foss/2022b RE2/2023-03-01</code> RE2 2024-03-01 foss/2023b <code>module load foss/2023b RE2/2024-03-01</code> re2c 3.0 foss/2022b <code>module load foss/2022b re2c/3.0</code> re2c 2.2 foss/2021b <code>module load foss/2021b re2c/2.2</code> re2c 3.1 foss/2023b <code>module load foss/2023b re2c/3.1</code> RIP-MD master foss/2021b <code>module load foss/2021b RIP-MD/master</code> Rust 1.65.0 foss/2022b <code>module load foss/2022b Rust/1.65.0</code> Rust 1.73.0 foss/2023b <code>module load foss/2023b Rust/1.73.0</code> Rust 1.54.0 foss/2021b <code>module load foss/2021b Rust/1.54.0</code> ScaFaCoS 1.0.4 foss/2023b <code>module load foss/2023b ScaFaCoS/1.0.4</code> ScaFaCoS 1.0.1 foss/2021b <code>module load foss/2021b ScaFaCoS/1.0.1</code> ScaLAPACK 2.2.0-fb foss/2022b <code>module load foss/2022b ScaLAPACK/2.2.0-fb</code> ScaLAPACK 2.1.0-fb foss/2021b <code>module load foss/2021b ScaLAPACK/2.1.0-fb</code> ScaLAPACK 2.2.0-fb foss/2023b <code>module load foss/2023b ScaLAPACK/2.2.0-fb</code> scikit-build 0.17.2 foss/2022b <code>module load foss/2022b scikit-build/0.17.2</code> scikit-build 0.17.6 foss/2023b <code>module load foss/2023b scikit-build/0.17.6</code> scikit-build 0.11.1 foss/2021b <code>module load foss/2021b scikit-build/0.11.1</code> SciPy-bundle 2021.10 intel/2021b <code>module load intel/2021b SciPy-bundle/2021.10</code> SciPy-bundle 2023.11 foss/2023b <code>module load foss/2023b SciPy-bundle/2023.11</code> SciPy-bundle 2023.02 foss/2022b <code>module load foss/2022b SciPy-bundle/2023.02</code> SciPy-bundle 2021.10 foss/2021b <code>module load foss/2021b SciPy-bundle/2021.10</code> SCons 4.6.0 foss/2023b <code>module load foss/2023b SCons/4.6.0</code> SCOTCH 7.0.4 foss/2023b <code>module load foss/2023b SCOTCH/7.0.4</code> SCOTCH 7.0.3 foss/2022b <code>module load foss/2022b SCOTCH/7.0.3</code> SCOTCH 6.1.2 foss/2021b <code>module load foss/2021b SCOTCH/6.1.2</code> SCOTCH 6.1.2 intel/2021b <code>module load intel/2021b SCOTCH/6.1.2</code> SDL2 2.26.3 foss/2022b <code>module load foss/2022b SDL2/2.26.3</code> SDL2 2.28.5 foss/2023b <code>module load foss/2023b SDL2/2.28.5</code> SDL2 2.0.20 foss/2021b <code>module load foss/2021b SDL2/2.0.20</code> setuptools-rust 1.8.0 foss/2023b <code>module load foss/2023b setuptools-rust/1.8.0</code> silo 4.10.2 foss/2022b <code>module load foss/2022b silo/4.10.2</code> snappy 1.1.10 foss/2023b <code>module load foss/2023b snappy/1.1.10</code> snappy 1.1.9 foss/2022b <code>module load foss/2022b snappy/1.1.9</code> snappy 1.1.9 foss/2021b <code>module load foss/2021b snappy/1.1.9</code> Spack 0.20.0 - <code>module load Spack/0.20.0</code> Spack 0.17.2 - <code>module load Spack/0.17.2</code> spdlog 1.12.0 foss/2023b <code>module load foss/2023b spdlog/1.12.0</code> SQLite 3.39.4 foss/2022b <code>module load foss/2022b SQLite/3.39.4</code> SQLite 3.43.1 foss/2023b <code>module load foss/2023b SQLite/3.43.1</code> SQLite 3.36 foss/2021b <code>module load foss/2021b SQLite/3.36</code> SuiteSparse 5.13.0-METIS-5.1.0 foss/2022b <code>module load foss/2022b SuiteSparse/5.13.0-METIS-5.1.0</code> SuiteSparse 5.10.1-METIS-5.1.0 foss/2021b <code>module load foss/2021b SuiteSparse/5.10.1-METIS-5.1.0</code> SuperLU 5.3.0 intel/2021b <code>module load intel/2021b SuperLU/5.3.0</code> SuperLU 5.3.0 foss/2022b <code>module load foss/2022b SuperLU/5.3.0</code> SuperLU_DIST 8.1.0 foss/2022b <code>module load foss/2022b SuperLU_DIST/8.1.0</code> SuperLU_DIST 8.1.2 foss/2022b <code>module load foss/2022b SuperLU_DIST/8.1.2</code> SWIG 4.1.1 foss/2023b <code>module load foss/2023b SWIG/4.1.1</code> SWIG 4.0.2 foss/2021b <code>module load foss/2021b SWIG/4.0.2</code> Szip 2.1.1 foss/2021b <code>module load foss/2021b Szip/2.1.1</code> Szip 2.1.1 foss/2023b <code>module load foss/2023b Szip/2.1.1</code> Szip 2.1.1 foss/2022b <code>module load foss/2022b Szip/2.1.1</code> tbb 2020.3 foss/2021b <code>module load foss/2021b tbb/2020.3</code> tbb 2021.13.0 foss/2023b <code>module load foss/2023b tbb/2021.13.0</code> Tcl 8.6.13 foss/2023b <code>module load foss/2023b Tcl/8.6.13</code> Tcl 8.6.11 foss/2021b <code>module load foss/2021b Tcl/8.6.11</code> Tcl 8.6.12 foss/2022b <code>module load foss/2022b Tcl/8.6.12</code> tcsh 6.24.01 foss/2021b <code>module load foss/2021b tcsh/6.24.01</code> tecplot 2024 - <code>module load tecplot/2024</code> tecplot360ex 2022R2 - <code>module load tecplot360ex/2022R2</code> Tk 8.6.12 foss/2022b <code>module load foss/2022b Tk/8.6.12</code> Tk 8.6.13 foss/2023b <code>module load foss/2023b Tk/8.6.13</code> Tk 8.6.11 foss/2021b <code>module load foss/2021b Tk/8.6.11</code> Tkinter 3.10.8 foss/2022b <code>module load foss/2022b Tkinter/3.10.8</code> Tkinter 3.11.5 foss/2023b <code>module load foss/2023b Tkinter/3.11.5</code> Tkinter 3.9.6 foss/2021b <code>module load foss/2021b Tkinter/3.9.6</code> tmux 3.3a - <code>module load tmux/3.3a</code> tmux 3.2a foss/2021b <code>module load foss/2021b tmux/3.2a</code> tornado 6.4 foss/2023b <code>module load foss/2023b tornado/6.4</code> tqdm 4.64.1 foss/2022b <code>module load foss/2022b tqdm/4.64.1</code> tqdm 4.62.3 foss/2021b <code>module load foss/2021b tqdm/4.62.3</code> UCC 1.2.0 foss/2023b <code>module load foss/2023b UCC/1.2.0</code> UCC 1.1.0 foss/2022b <code>module load foss/2022b UCC/1.1.0</code> UCX 1.13.1 foss/2022b <code>module load foss/2022b UCX/1.13.1</code> UCX 1.11.2 foss/2021b <code>module load foss/2021b UCX/1.11.2</code> UCX 1.15.0 foss/2023b <code>module load foss/2023b UCX/1.15.0</code> UCX-CUDA 1.11.2-CUDA-11.4.1 foss/2021b <code>module load foss/2021b UCX-CUDA/1.11.2-CUDA-11.4.1</code> UCX-CUDA 1.13.1-CUDA-12.0.0 foss/2022b <code>module load foss/2022b UCX-CUDA/1.13.1-CUDA-12.0.0</code> UCX-CUDA 1.13.1-CUDA-12.4.0 foss/2022b <code>module load foss/2022b UCX-CUDA/1.13.1-CUDA-12.4.0</code> UDUNITS 2.2.28 foss/2022b <code>module load foss/2022b UDUNITS/2.2.28</code> UDUNITS 2.2.28 foss/2021b <code>module load foss/2021b UDUNITS/2.2.28</code> UnZip 6.0 foss/2021b <code>module load foss/2021b UnZip/6.0</code> UnZip 6.0 foss/2022b <code>module load foss/2022b UnZip/6.0</code> UnZip 6.0 foss/2023b <code>module load foss/2023b UnZip/6.0</code> utf8proc 2.8.0 foss/2022b <code>module load foss/2022b utf8proc/2.8.0</code> util-linux 2.37 foss/2021b <code>module load foss/2021b util-linux/2.37</code> util-linux 2.38.1 foss/2022b <code>module load foss/2022b util-linux/2.38.1</code> util-linux 2.39 foss/2023b <code>module load foss/2023b util-linux/2.39</code> Valgrind 3.21.0 foss/2022b <code>module load foss/2022b Valgrind/3.21.0</code> VASP 6.4.2 intel/2022b <code>module load intel/2022b VASP/6.4.2</code> VASP 6.5.0 intel/2022b <code>module load intel/2022b VASP/6.5.0</code> virtualenv 20.24.6 foss/2023b <code>module load foss/2023b virtualenv/20.24.6</code> VirtualGL 3.1 foss/2022b <code>module load foss/2022b VirtualGL/3.1</code> VirtualGL 3.0 foss/2021b <code>module load foss/2021b VirtualGL/3.0</code> VMD 1.9.4a57 foss/2022b <code>module load foss/2022b VMD/1.9.4a57</code> Voro++ 0.4.6 foss/2021b <code>module load foss/2021b Voro++/0.4.6</code> Voro++ 0.4.6 foss/2023b <code>module load foss/2023b Voro++/0.4.6</code> VTK 9.1.0 foss/2021b <code>module load foss/2021b VTK/9.1.0</code> VTK 9.3.0 foss/2023b <code>module load foss/2023b VTK/9.3.0</code> Wayland 1.22.0 foss/2023b <code>module load foss/2023b Wayland/1.22.0</code> wxWidgets 3.2.2.1 foss/2022b <code>module load foss/2022b wxWidgets/3.2.2.1</code> X11 20221110 foss/2022b <code>module load foss/2022b X11/20221110</code> X11 20231019 foss/2023b <code>module load foss/2023b X11/20231019</code> X11 20210802 foss/2021b <code>module load foss/2021b X11/20210802</code> x264 20231019 foss/2023b <code>module load foss/2023b x264/20231019</code> x264 20210613 foss/2021b <code>module load foss/2021b x264/20210613</code> x264 20230226 foss/2022b <code>module load foss/2022b x264/20230226</code> x265 3.5 foss/2022b <code>module load foss/2022b x265/3.5</code> x265 3.5 foss/2021b <code>module load foss/2021b x265/3.5</code> x265 3.5 foss/2023b <code>module load foss/2023b x265/3.5</code> Xerces-C++ 3.2.4 foss/2022b <code>module load foss/2022b Xerces-C++/3.2.4</code> Xerces-C++ 3.2.5 foss/2023b <code>module load foss/2023b Xerces-C++/3.2.5</code> xorg-macros 1.20.0 foss/2023b <code>module load foss/2023b xorg-macros/1.20.0</code> xorg-macros 1.19.3 foss/2022b <code>module load foss/2022b xorg-macros/1.19.3</code> xorg-macros 1.19.3 foss/2021b <code>module load foss/2021b xorg-macros/1.19.3</code> xprop 1.2.5 foss/2022b <code>module load foss/2022b xprop/1.2.5</code> xprop 1.2.5 foss/2021b <code>module load foss/2021b xprop/1.2.5</code> xtb 6.6.1 foss/2022b <code>module load foss/2022b xtb/6.6.1</code> xtb 6.6.1 foss/2022b <code>module load foss/2022b xtb/6.6.1</code> Xvfb 21.1.9 foss/2023b <code>module load foss/2023b Xvfb/21.1.9</code> Xvfb 1.20.13 foss/2021b <code>module load foss/2021b Xvfb/1.20.13</code> Xvfb 21.1.6 foss/2022b <code>module load foss/2022b Xvfb/21.1.6</code> xxd 9.0.1696 foss/2022b <code>module load foss/2022b xxd/9.0.1696</code> xxd 9.1.0307 foss/2023b <code>module load foss/2023b xxd/9.1.0307</code> xxd 8.2.4220 foss/2021b <code>module load foss/2021b xxd/8.2.4220</code> XZ 5.2.7 foss/2022b <code>module load foss/2022b XZ/5.2.7</code> XZ 5.4.4 foss/2023b <code>module load foss/2023b XZ/5.4.4</code> XZ 5.2.5 foss/2021b <code>module load foss/2021b XZ/5.2.5</code> Yasm 1.3.0 foss/2021b <code>module load foss/2021b Yasm/1.3.0</code> Yasm 1.3.0 foss/2023b <code>module load foss/2023b Yasm/1.3.0</code> Yasm 1.3.0 foss/2022b <code>module load foss/2022b Yasm/1.3.0</code> Z3 4.12.2 foss/2022b <code>module load foss/2022b Z3/4.12.2</code> Z3 4.8.12 foss/2021b <code>module load foss/2021b Z3/4.8.12</code> ZeroMQ 4.3.5 foss/2023b <code>module load foss/2023b ZeroMQ/4.3.5</code> ZeroMQ 4.3.4 foss/2022b <code>module load foss/2022b ZeroMQ/4.3.4</code> zlib 1.2.11 - <code>module load zlib/1.2.11</code> zlib 1.2.12 - <code>module load zlib/1.2.12</code> zlib 1.2.13 - <code>module load zlib/1.2.13</code> zlib 1.2.11 foss/2021b <code>module load foss/2021b zlib/1.2.11</code> zlib 1.2.12 foss/2022b <code>module load foss/2022b zlib/1.2.12</code> zlib 1.2.13 foss/2023b <code>module load foss/2023b zlib/1.2.13</code> zstd 1.5.5 foss/2023b <code>module load foss/2023b zstd/1.5.5</code> zstd 1.5.0 foss/2021b <code>module load foss/2021b zstd/1.5.0</code> zstd 1.5.2 foss/2022b <code>module load foss/2022b zstd/1.5.2</code>"},{"location":"Software/CFD/ansys/","title":"ANSYS","text":"<p>ANSYS is a computer-aided engineering (CAE) software suite used to simulate, analyze, and design products and systems in various industries such as aerospace, automotive, energy, and healthcare. ANSYS provides a wide range of simulation tools that can be used to model and analyze various physical phenomena such as fluid dynamics, structural mechanics, electromagnetics, and thermal analysis.</p> <p>The software suite is known for its high level of accuracy and versatility, and is widely used by engineers and designers to optimize product performance, reduce costs, and improve time to market. ANSYS offers a wide range of modules and add-ons, which can be customized to suit specific engineering needs and requirements.</p>"},{"location":"Software/CFD/ansys/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command ANSYS 2025R1 - <code>module load ANSYS/2025R1</code> ANSYS 2024R1 - <code>module load ANSYS/2024R1</code>"},{"location":"Software/CFD/ansys/#application-information-documentation","title":"Application Information, Documentation","text":"<p>Please download ANSYS and follow the instructions to install ANSYS on your local machine.</p>"},{"location":"Software/CFD/ansys/#using-ansys","title":"Using ANSYS","text":"<p>ANSYS workbench will be available on Wulver via Open OnDemand. We will provide the instructions soon.</p>"},{"location":"Software/CFD/ansys/#related-applications","title":"Related Applications","text":"<ul> <li>COMSOL</li> </ul>"},{"location":"Software/CFD/ansys/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/CFD/comsol/","title":"COMSOL","text":"<p>COMSOL is a commercial finite element analysis (FEA) software package used for modeling and simulation of multi-physics systems. It allows users to build and solve complex multiphysics models involving various physical phenomena such as fluid dynamics, structural mechanics, heat transfer, electromagnetics, and chemical reactions.</p> <p>The software uses a graphical user interface to create models, and offers a wide range of pre-built modeling components that can be used to quickly create models. It also supports user-defined models and can handle a variety of boundary conditions and physics.</p> <p>COMSOL is widely used in engineering and science fields, such as mechanical engineering, chemical engineering, electrical engineering, physics, and materials science, among others. Its multiphysics capabilities make it a powerful tool for simulating complex systems and optimizing designs.</p>"},{"location":"Software/CFD/comsol/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command"},{"location":"Software/CFD/comsol/#application-information-documentation","title":"Application Information, Documentation","text":""},{"location":"Software/CFD/comsol/#using-comsol","title":"Using COMSOL","text":""},{"location":"Software/CFD/comsol/#related-applications","title":"Related Applications","text":"<ul> <li>ANSYS </li> </ul>"},{"location":"Software/CFD/comsol/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/CFD/fluent/","title":"FLUENT","text":""},{"location":"Software/CFD/fluent/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command ANSYS 2025R1 - <code>module load ANSYS/2025R1</code> ANSYS 2024R1 - <code>module load ANSYS/2024R1</code>"},{"location":"Software/CFD/fluent/#application-information-documentation","title":"Application Information, Documentation","text":"<p>To use Fluent on cluster, Users need to prepare the case using ANSYS on their local machine first. Please download ANSYS and follow the instructions to install ANSYS on your local machine.</p>"},{"location":"Software/CFD/fluent/#using-fluent","title":"Using Fluent","text":"<p>To use Fluent in cluster, users first need to prepare the fluent case (meshing, setting boundary conditions) on their local machine  and save the case and data in <code>.cas</code> and <code>.dat</code> format respectively.  If you are running transient problem and want to save the data at particular timestep or time interval, please see the steps below.</p> <ul> <li>Go to <code>Calculation Activities</code> option in the left pane and double-click the <code>Autosave (Every  Flow Time)</code> option, you will notice a separate dialogue box <code>Autosave</code>, where you need to specify how frequently you want to save the data, you can choose eiter <code>timestep</code> or <code>Flow Time</code> interval. In the <code>File name</code> option you need to specify the subdirectory where you want to save the data and the case name. In the example shown below the subdirectory is <code>data</code> and the problem name is heatpipe. You need to make sure to create the subdirectoy (<code>data</code> in this example) in the cluster where you want to intend to submit the job script.</li> </ul> <p></p> <p>This is required if your job is somehow cancelled, or you need to restart from specific flow time.</p> <ul> <li>If you want  to ppstprocess the data using a different software , e.g. Tecplot or ParaView, you need to save the data in different file format at certain flow time or time step interval.  To set up the postprocessing configuration, select <code>File --&gt; Export --&gt; During Calculation --&gt; Solution Data</code> option (see the figure below)</li> </ul> <p></p> <ul> <li>Once you open the configuration, you will notice separate dialogue box <code>Automatic Export</code>. You need the file format in <code>File Type</code> option. Select the drop-down option to see different options. In this example above, <code>ensight</code> format has been selected. In right pane, you need to select which parameters you want to visualize. In the example above, <code>Volume Fraction</code> for different phases have been selected. </li> <li>Next, you to select the file name and the subdirectory on cluster where you intend to write the post processed data. In this example, we choose to write the files in <code>ensi</code> subdirectory. Please make sure to create this subdirectory inside case directory on cluster before running the simulation. </li> <li>You can also set how often you want to write your data. You need to select <code>Export Data Everty (s)</code> option in terms of time step interval or flow time interval.  </li> <li>Once you set up your case and initialize the problem, you need to go to <code>File --&gt; write --&gt; case and data</code> option to write the case and data in <code>.cas</code> and <code>.dat</code> respectively. </li> <li>Transfer the <code>.cas</code> and <code>.dat</code> files to the cluster. See the steps to transfer the data for transferring the files from local machine to cluster.</li> <li>Once you transfer the files, log on the cluster and go the directory where you transferred the <code>.cas</code> and <code>.dat</code> files. </li> <li>Create a journal file which defines the input data file and some additional settings required by Fluent.  You can use the following journal file</li> </ul> Sample journal file : journal.JOU <pre><code>   /file/set-tui-version \"21.1\"\n   /file/read-case-data tube_vof.cas.h5\n   solve/dual-time-iterate 20 50\n   (print-case-timer)\n   parallel/timer/usage\n</code></pre> <p>In the above <code>Journal</code> script, the full name of case file (<code>tube_vof.cas.h5</code>) is mentioned. You need to modify based on the case file based on the problem. The <code>solve/dual-time-iterate</code> specifies the end flow time and number of iterations. In the above example, <code>20</code> is the end flow time while the maximum number of iterations are <code>50</code>. The \"dual-time\" approach allows for a larger time step size by introducing an additional iteration loop within each time step. Users can select different approach based on their problems and need to modify it accordingly. </p> Sample Batch Script to Run FLUENT : fluent.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH --job-name=fluent\n#SBATCH --output=%x.%j.out # i%x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n# Use \"sinfo\" to see what partitions are available to you\n#SBATCH --partition=general\n#SBATCH --ntasks=8\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n# Memory required; lower amount gets scheduling priority\n#SBATCH --mem-per-cpu=2G\n\n# Time required in d-hh:mm:ss format; lower time gets scheduling priority\n#SBATCH --time=71:59:00\n\n# Purge and load the correct modules\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver # Load the slurm, easybuild \nmodule load ANSYS\n\n# Run the mpi program\n\nmachines=hosts.$SLURM_JOB_ID\ntouch $machines\nfor node in `scontrol show hostnames`\n    do\n        echo \"$node\"  &gt;&gt; $machines\n    done\n\nfluent 3ddp -affinity=off -ssh -t$SLURM_NTASKS -pib -mpi=intel -cnf=\"$machines\" -g -i journal.JOU\n</code></pre> <p>Submit the job using <code>sbatch fluent.submit.sh</code> command.</p>"},{"location":"Software/CFD/fluent/#related-applications","title":"Related Applications","text":"<ul> <li>OpenFOAM</li> </ul>"},{"location":"Software/CFD/fluent/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/CFD/openfoam/","title":"OpenFOAM","text":"<p>OpenFOAM (Open Field Operation and Manipulation) is a free and open-source computational fluid dynamics (CFD) software package that is used to simulate and analyze fluid flow, heat transfer, and other related phenomena. It is written primarily in C++ and can be used on various operating systems such as Linux, Windows, and macOS.</p> <p>OpenFOAM offers a wide range of solvers, turbulence models, and physical models that can be used to model various engineering applications such as automotive, aerospace, chemical processing, and power generation. It also allows users to customize and extend its functionality to meet specific needs.</p> <p>The software is widely used in academia, research, and industry, and is known for its robustness, flexibility, and scalability. It offers a high degree of parallelization, which allows it to be used on large clusters of computers for complex simulations. Because it is open source, it is often used by researchers and developers to create new solvers, models, and add-ons for specific applications.</p>"},{"location":"Software/CFD/openfoam/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command OpenFOAM v2406 foss/2024a <code>module load foss/2024a OpenFOAM/v2406</code>"},{"location":"Software/CFD/openfoam/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of OpenFOAM is available at OpenFOAM Documentation, where you can find the tutorials in OpenFOAM meshing (blockMesh), postprocessing, setting boundary conditions etc. </p>"},{"location":"Software/CFD/openfoam/#using-openfoam","title":"Using OpenFOAM","text":"<p>OpenFOAM can be used for both serial and parallel jobs. To run OpenFOAM in parallel, you need to use the following job script.</p> Sample Batch Script to Run OpenFOAM in parallel: openfoam_parallel.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH --job-name=openfoam_parallel\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load foss/2024a OpenFOAM\n################################################\n#\n# Source OpenFOAM bashrc\n# The modulefile doesn't do this\n#\n################################################\nsource $FOAM_BASH\n################################################\n#\n# copy into cavity directory from /opt/site/examples/openFoam/parallel \n# run blockMesh and\n# icoFoam. Note: this is running on one node and\n# using all 32 cores on the node\n#\n################################################\ncp -r /apps/easybuild/examples/openFoam/parallel/cavity /path/to/destination\n# /path/to/destination is destination path where user wants to copy the cavity directory\ncd cavity\nblockMesh\ndecomposePar -force\nsrun icoFoam -parallel\nreconstructPar\n</code></pre> <p>Note</p> Wulver <p>You can copy the tutorial <code>cavity</code> mentioned in the above job script from the <code>/apps/easybuild/examples/openFoam/parallel</code> directory.  </p> <p>To run OpenFOAM in serial, the following job script can be used.</p> Sample Batch Script to Run OpenFOAM in serial: openfoam_serial.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH --job-name=openfoam_serial\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load foss/2021b OpenFOAM\n################################################\n#\n# Source OpenFOAM bashrc\n# The modulefile doesn't do this\n#\n################################################\nsource $FOAM_BASH\n################################################\n#\n# copy into cavity directory from /apps/easybuild/examples/openFoam/serial \n# run blockMesh and\n# icoFoam. Note: this is running on one node and\n# using all 32 cores on the node\n#\n################################################\ncp -r /apps/easybuild/examples/openFoam/serial/cavity /path/to/destination\n# /path/to/destination is destination path where user wants to copy the cavity directory\ncd cavity\nblockMesh\nicoFoam\n</code></pre> <p>Submit the job script using the sbatch command: <code>sbatch openfoam_parallel.submit.sh</code> or <code>sbatch openfoam_serial.submit.sh</code>.</p>"},{"location":"Software/CFD/openfoam/#building-openfoam-from-source","title":"Building OpenFOAM from source","text":"<p>Sometimes, users need to create a new solver or modify the existing solver by adding different functions for their research. In that case, users need to build openFOAM from source since user do not have the permission to add libraries in the root directory where OpenFOAM is installed. The following instructions are provided on how to build openFOAM from source on cluster. If you have any queries or issues regarding building OpenFOAM, please contact us at hpc@njit.edu.</p> <pre><code>  # This is to build a completely self contained OpenFOAM using MPICH mpi. Everything from GCC on up will be built.\n\n  # start an interactive session with compute node. Replace \"PI_UCID\" with the UCID of PI. Modify the other parameters if required.\n  srun --partition=general --nodes=1 --ntasks-per-node=16 --mem-per-cpu=2G --account=PI_UCID --qos=standard --time=2:00:00 --pty bash  \n\n  # purge all loaded modules\n  module purge\n  # Download the latest version of OpenFOAM, visit https://develop.openfoam.com/Development/openfoam/-/blob/master/doc/Build.md for details\n  # Download the source\n  wget https://dl.openfoam.com/source/v2212/OpenFOAM-v2212.tgz \n  # Download ThirdParty\n  wget https://dl.openfoam.com/source/v2212/ThirdParty-v2212.tgz\n\n  cd ThirdParty-v2212\n\n  #Packages to download :\n  wget https://ftp.gnu.org/gnu/gcc/gcc-4.8.5/gcc-4.8.5.tar.bz2\n  wget https://src.fedoraproject.org/repo/pkgs/metis/metis-5.1.0.tar.gz/5465e67079419a69e0116de24fce58fe/metis-5.1.0.tar.gz\n  wget ftp://ftp.gnu.org/gnu/gmp/gmp-6.2.0.tar.bz2\n  wget ftp://ftp.gnu.org/gnu/mpfr/mpfr-4.0.2.tar.bz2\n  wget ftp://ftp.gnu.org/gnu/mpc/mpc-1.1.0.tar.gz\n  wget http://www.mpich.org/static/downloads/3.3/mpich-3.3.tar.gz\n\n  # unpack the above packages\n\n  vi ../OpenFOAM-v2212/etc/bashrc\n  # Change the following in bashrc \n  # User needs to specify the full path of the project directory below, it can be either the research directory or $HOME directory.\n    projectDir=\"path/to/OpenFOAM/2212/OpenFOAM-$WM_PROJECT_VERSION\" \n    export WM_MPLIB=MPICH\n    export WM_LABEL_SIZE=64\n\n  vi ../OpenFOAM-v2212/etc/config.sh/compiler\n  # Change the following in compiler\n    default_gmp_version=gmp-system\n    default_mpfr_version=mpfr-system\n    default_mpc_version=mpc-system\n\n    gmp_version=\"gmp-6.2.0\"\n    mpfr_version=\"mpfr-4.0.2\"\n    mpc_version=\"mpc-1.1.0\"\n  # Source the RC script\n  source ../OpenFOAM-v2212/etc/bashrc FOAMY_HEX_MESH=yes\n  # You might see the following warning message\n  ===============================================================================\n  Warning in /opt/site/apps/OpenFOAM/2212/OpenFOAM-v2212/etc/config.sh/settings:\n  Cannot find 'Gcc' compiler installation\n    /opt/site/apps/OpenFOAM/2212/ThirdParty-v2212/platforms/linux64/gcc-4.8.5\n\n  Either install this compiler version, or use the system compiler by setting\n  WM_COMPILER_TYPE to 'system' in $WM_PROJECT_DIR/etc/bashrc.\n  ===============================================================================\n  No completions for /opt/site/apps/OpenFOAM/2212/OpenFOAM-v2212/platforms/linux64GccDPInt64Opt/bin\n  [ignore if OpenFOAM is not yet compiled]\n\n  ./makeGcc\n  wmRefresh\n  # make MPICH\n  ./makeMPICH\n  wmRefresh\n\n  # We should be able to make the rest of the utilities.\n  # load the cmake module\n  module load cmake\n\n  /Allwmake -j 8\n\n  cd ../OpenFOAM-v2212\n  wmRefresh\n\n  ./Allwmake -j 16\n</code></pre>"},{"location":"Software/CFD/openfoam/#related-applications","title":"Related Applications","text":"<ul> <li>FLUENT</li> </ul>"},{"location":"Software/CFD/openfoam/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/IDE/VSCode/","title":"VS Code","text":"<p>Visual Studio Code (often abbreviated as VS Code) is an Integrated Development Environment (IDE). It is a lightweight and highly extensible code editor developed by Microsoft. Although referred to as a code editor, VS Code offers many features. One of the important feature of VS Code is an integrated terminal that allows developers to execute commands, run scripts, and interact with the command-line interface without leaving the editor. VS Code also provides Git integration, enabling developers to manage version control operations, such as committing, branching, and merging, without switching to a separate Git client. </p>"},{"location":"Software/IDE/VSCode/#availability","title":"Availability","text":"<p>VS Code is not installed on the cluster. To use VS Code, you need to install it on your computer and connect remotely to the cluster using NJIT VPN.</p>"},{"location":"Software/IDE/VSCode/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of VS Code is available at VS Code documentation. You can download the VS Code from VS Code download page</p>"},{"location":"Software/IDE/VSCode/#using-vs-code-on-cluster","title":"Using VS Code on Cluster","text":"<p>Warning</p> <p>If you want to use VS Code on NJIT cluster, don't use VS Code installed on your machine to connect to cluster! Please use the method described belelow so that you can use VS Code not only to edit scripts, but also run your script on the cluster.</p> <p>Use the following slurm script and submit the job script using <code>sbatch vs-code.submit.sh</code> command.</p> Batch Script to use VS Code : vs-code.submit.sh <pre><code>#!/bin/bash -l\n#SBATCH --job-name=vs-code\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.out # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\nset -e\n\nmodule purge\nmodule load wulver # load slurn, easybuild\n\n# add any required module loads here, e.g. a specific Python\n\nCLI_PATH=\"${HOME}/vscode_cli\"\n\n# Install the VS Code CLI command if it doesn't exist\nif [[ ! -e ${CLI_PATH}/code ]]; then\n    echo \"Downloading and installing the VS Code CLI command\"\n    mkdir -p \"${HOME}/vscode_cli\"\n    pushd \"${HOME}/vscode_cli\"\n    # Process from: https://code.visualstudio.com/docs/remote/tunnels#_using-the-code-cli\n    curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&amp;os=cli-alpine-x64' --output vscode_cli.tar.gz\n    # unpack the code binary file\n    tar -xf vscode_cli.tar.gz\n    # clean-up\n    rm vscode_cli.tar.gz\n    popd\nfi\n\n# run the code tunnel command and accept the licence\n${CLI_PATH}/code tunnel --accept-server-license-terms\n</code></pre> <p>Once you submit the job, you will see an output file with <code>.out</code> extension. Once you open the file, you will see the following <pre><code>*\n\n* Visual Studio Code Server\n\n*\n\n* By using the software, you agree to\n\n* the Visual Studio Code Server License Terms (https://aka.ms/vscode-server-license)and\n\n* the Microsoft Privacy Statement (https://privacy.microsoft.com/en-US/privacystatement).\n\n*\n\nTo grant access to the server, please log into https://github.com/login/device and use code XXXX-XXXX\n</code></pre> You need to have the GitHub account, please open the GitHub profile and use the code printed in the output file. Once you authorize GitHub, you will see the following in the output file</p> <pre><code>Open this link in your browser https://vscode.dev/tunnel/nodeXXX\n</code></pre> <p>Now copy and paste this link in your browser and VS Code is ready to use.</p>"},{"location":"Software/IDE/VSCode/#related-applications","title":"Related Applications","text":"<ul> <li>PyCharm</li> </ul>"},{"location":"Software/IDE/VSCode/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/MATLAB/","title":"MATLAB","text":"<p>MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and proprietary programming language developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages. Although MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.</p>"},{"location":"Software/MATLAB/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command MATLAB 2024a - <code>module load MATLAB/2024a</code> MATLAB 2025a - <code>module load MATLAB/2025a</code>"},{"location":"Software/MATLAB/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of MATLAB is available at MATLAB Tutorial.</p>"},{"location":"Software/MATLAB/#using-matlab","title":"Using MATLAB","text":""},{"location":"Software/MATLAB/#serial-job","title":"Serial Job","text":"Sample Batch Script to run MATLAB: matlab-serial.sh Wulver <pre><code>#!/bin/bash\n#SBATCH -J test_matlab\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Load matlab module\nmodule purge\nmodule load wulver # Load the slurm, easybuild \nmodule load MATLAB\n\nmatlab -nodisplay -nosplash -r test\n</code></pre> Sample MATLAB script <pre><code>A = [ 1 2; 3 4]\nA.^2\n</code></pre>"},{"location":"Software/MATLAB/#parallel-job","title":"Parallel Job","text":""},{"location":"Software/MATLAB/#single-node-parallelization","title":"Single node parallelization","text":"Sample Batch Script to run MATLAB: matlab_parallel.sh Wulver <p><pre><code>#!/bin/bash\n#SBATCH -J test_matlab\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Load matlab module\nmodule purge\nmodule load wulver # Load the slurm, easybuild\nmodule load MATLAB\n\n# Run matlab\nmatlab -nodisplay -nosplash -r 'cd('/path/to/for_loop.m');for_loop; quit'\n</code></pre> Replace <code>cd('/path/to/for_loop.m')</code> with the actual path of the matlab script. You don't need to use <code>cd('/path/to/for_loop.m')</code> if the Matlab script and job script are in the same directory. In that case, use <code>matlab -nodisplay -nosplash -r 'for_loop; quit'</code></p> Sample Parallel MATLAB script: for_loop.m <pre><code>poolobj = parpool('local',32);\nfprintf('Number of workers: %g\\n', poolobj.NumWorkers);\n\ntic\nn = 2000;\nA = 500;\na = zeros(n);\nparfor i = 1:n\n    a(i) = max(abs(eig(rand(A))));\nend\ntoc\n</code></pre>"},{"location":"Software/MATLAB/#multi-node-parallelization","title":"Multi node parallelization","text":"<p>If you want to learn how to install a version of MATLAB on your local system and use it to run jobs on Wulver, please see Using Local MATLAB on Wulver</p>"},{"location":"Software/MATLAB/matlab_local/","title":"Use MATLAB on NJIT HPC","text":"<p>Tip</p> <p>Since MFA is enabled, the instructions for running MATLAB via HPC resources have been modified. If you already installed MATLAB on the local machine, skip to Setup Slurm profile to run MATLAB on Wulver. </p>"},{"location":"Software/MATLAB/matlab_local/#installation-steps-of-matlab-on-local-machine","title":"Installation steps of MATLAB on local machine","text":"<ul> <li>Go to Mathworks Download and register with your NJIT email address.</li> <li>Select the MATLAB version installed on Wulver.</li> <li>User needs to select the correct installer based on the OS (Mac or Windows). </li> <li>Run the installer.</li> </ul> <ul> <li>Make sure to check Parallel Computing Toolbox option.</li> </ul> <ul> <li>Continue by selecting Next and MATLAB will be installed on your computer.</li> </ul>"},{"location":"Software/MATLAB/matlab_local/#setup-slurm-profile-to-run-matlab-on-wulver","title":"Setup Slurm profile to run MATLAB on Wulver","text":"<ul> <li>Open MATLAB \u2192 select Create and Manage Clusters.</li> </ul> <ul> <li>A new dialogue box will open and under the Add Cluster Profile, select Slurm.</li> </ul> <ul> <li>This will open a Slurm cluster Profile and select the edit option to modify the parameters</li> </ul> <ul> <li>Modify the following parameters as mentioned in the screenshot</li> </ul> <p>a. <code>Description</code> - Set the name as <code>Wulver</code></p> <p>b. <code>JobStorageLocation</code> - No Change</p> <p>c. <code>NumWorkers</code> - 512</p> <p>d. <code>NumThreads</code> - No Change</p> <p>e. <code>ClusterMatlabRoot</code> - Use <code>module av MATLAB</code> command first.</p> <p><pre><code>  login-1-45 ~ &gt;: module av MATLAB\n  ------------------------------------/apps/easybuild/modules/all/Core---------------------------------------------------------\n   MATLAB/2024a\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre> This will show you the list of MATLAB versions installed on Wulver. Next, use <code>module show MATLAB/2024a</code> to check MATLAB installation path.</p> <p><pre><code>   login-1-45 ~ &gt;: module show MATLAB/2024a\n---------------------------------------------------------------------------------------------------------------------------------\n   /apps/easybuild/modules/all/Core/MATLAB/2024a.lua:\n---------------------------------------------------------------------------------------------------------------------------------\nhelp([[\nDescription\n===========\nThe MATLAB Parallel Server Toolbox.\n\n\nMore information\n================\n - Homepage: https://www.mathworks.com/help/matlab/matlab-engine-for-python.html\n]])\nwhatis(\"Description: The MATLAB Parallel Server Toolbox.\")\nwhatis(\"Homepage: https://www.mathworks.com/help/matlab/matlab-engine-for-python.html\")\nwhatis(\"URL: https://www.mathworks.com/help/matlab/matlab-engine-for-python.html\")\nconflict(\"MATLAB\")\nprepend_path(\"CMAKE_PREFIX_PATH\",\"/apps/easybuild/software/MATLAB/2024a\")\nprepend_path(\"PATH\",\"/apps/easybuild/software/MATLAB/2024a/bin\")\nsetenv(\"EBROOTMATLAB\",\"/apps/easybuild/software/MATLAB/2024a\")\nsetenv(\"EBVERSIONMATLAB\",\"2024a\")\nsetenv(\"EBDEVELMATLAB\",\"/apps/easybuild/software/MATLAB/R2023a/easybuild/Core-MATLAB-2023a-easybuild-devel\")\nprepend_path(\"PATH\",\"/apps/easybuild/software/MATLAB/2024a/toolbox/parallel/bin\")\nprepend_path(\"PATH\",\"/apps/easybuild/software/MATLAB/2024a\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/apps/easybuild/software/MATLAB/2024a/runtime/glnxa64\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/apps/easybuild/software/MATLAB/2024a/bin/glnxa64\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/apps/easybuild/software/MATLAB/2024a/sys/os/glnxa64\")\nsetenv(\"_JAVA_OPTIONS\",\"-Xmx2048m\")\n</code></pre> The MATLAB installation path is defined by the <code>EBROOTMATLAB</code> environment variable, which, in the above example, is set to <code>/apps/easybuild/software/MATLAB/2024a</code>.</p> <p>f. <code>RequireOnlineLicensing</code> - false</p> <p>g. <code>AdditionalProperties</code> - Select add  and add the following as mentioned in the table. </p> <p></p> Name Value Type <code>ClusterHost</code> <code>wulver.njit.edu</code> String <code>AuthenticationMode</code> Multifactor String <code>UseUniqueSubfolders</code> True Logical <code>UseIdentityFile</code> False Logical <code>RemoteJobStorageLocation</code> <code>$PATH</code> String <code>user</code> <code>$UCID</code> String <p>Replace <code>$PATH</code> with the actual path of Wulver where you want to save the output file. Make sure to use <code>/project</code> directory for remote job storage as <code>$HOME</code> has fixed quota of 50GB and cannot be increased. See Wulver Filesystems for details. Replace <code>$UCID</code> with the NJIT UCID.</p>"},{"location":"Software/MATLAB/matlab_local/#submitting-a-serial-job","title":"Submitting a Serial Job","text":"<p>This section will demonstrate how to create a cluster object and submit a simple job to the cluster. The job will run the 'hostname' command on the node assigned to the job. The output will indicate clearly that the job ran on the cluster and not on the local computer.</p> <p>The hostname.m file used in this demonstration can be downloaded here.</p> <pre><code> &gt;&gt; c=parcluster \n</code></pre> <p>Certain arguments need to be passed to SLURM in order for the job to run properly. Here we will set values for partition, and time. In the Matlab window enter: <pre><code> &gt;&gt; c.AdditionalProperties.AdditionalSubmitArgs=['--partition=general --qos=standard --account=$PI_UCID --time=2-00:00:00'] \n</code></pre> Replace <code>$PI_UCID</code> with the UCID of PI. Check the Batch Jobs for other SLURM parameters. To make this persistent between Matlab sessions these arguments need to be saved to the profile. In the Matlab window enter: <pre><code> &gt;&gt; c.saveProfile \n</code></pre></p> <p>We will now submit the hostname.m function to the cluster. In the Matlab window enter the following: <pre><code>&gt;&gt; j=c.batch(@hostname, 1, {}, 'AutoAddClientPath', false); \n</code></pre></p> <ul> <li> <p><code>@</code>: Submitting a function.</p> </li> <li> <p><code>1</code>: The number of output arguments from the evaluated function.</p> </li> <li> <p><code>{}</code>: Cell array of input arguments to the function. In this case empty.</p> </li> <li> <p><code>'AutoAddClientPath', false</code>: The client path is not available on the cluster.</p> </li> </ul> <p>When the job is submitted, you will be prompted for your password.</p> <p>To wait for the job to finish, enter the following in the Matlab window: <pre><code> &gt;&gt;j.wait\n</code></pre> Finally, to get the results: <pre><code> &gt;&gt;fetchOutputs(j)\n</code></pre></p>"},{"location":"Software/MATLAB/matlab_local/#submitting-a-parallel-function","title":"Submitting a Parallel Function","text":"<p>The <code>Job Monitor</code> is a convenient way to monitor jobs submitted to the cluster. In the Matlab window select <code>Parallel</code> and then <code>Monitor Jobs</code>.</p> <p>For more information see the Mathworks page: Job Monitor.</p> <p>Here we will submit a simple function using a \"parfor\" loop. The code for this example is as follows: <pre><code>function t = parallel_example\n\nt0 = tic;\nparfor idx = 1:16\n        A(idx) = idx;\n        pause (2)\nend\n\nt=toc(t0);\n</code></pre> To submit this job: <pre><code> &gt;&gt; c.AdditionalProperties.AdditionalSubmitArgs=['--partition=general --qos=standard --account=$PI_UCID --ntasks=8 --time=2-00:00:00'] \n &gt;&gt; c.saveProfile\n &gt;&gt; j=c.batch(@parallel_example, 1, {}, 'AutoAddClientPath', false, 'Pool', 7)\n</code></pre> Since this is a parallel job a 'Pool' must be started. The actual number of tasks started will be one more than requested in the pool. In this case, the batch command calls for a pool of seven. Eight tasks will be started on the cluster.</p> <p>The job takes a few minutes to run and the state of the job changes to \"finished.\"</p> <p>Once again to get the results enter: <pre><code> &gt;&gt; fetchOutputs(j) \n</code></pre> As can be seen the parfor loop was completed in 6.7591 seconds.</p> <p></p>"},{"location":"Software/MATLAB/matlab_local/#submitting-a-script-requiring-a-gpu","title":"Submitting a Script Requiring a GPU","text":"<p>In this section we will submit a matlab script using a GPU. The results will be written to the job diary. The code for this example is as follows: <pre><code>% MATLAB script that defines a random matrix and does FFT\n%\n% The first FFT is without a GPU\n% The second is with the GPU\n%\n% MATLAB knows to use the GPU the second time because it\n%   is passed a type gpuArray as an argument to FFT\n% We do the FFT a bunch of times to make using the GPU worth it,\n%   or else it spends more time offloading to the GPU\n%   than performning the calculation\n%\n% This example is meant to provide a general understanding\n%   of MATLAB GPU usage\n% Meaningful performance measurements depend on many factors\n%   beyond the scope of this example\n% Downloaded from https://projects.ncsu.edu/hpc/Software/examples/matlab/gpu/gpu_m\n\n% Define a matrix\nA1 = rand(3000,3000);\n\n% Just use the compute node, no GPU\ntic;\n% Do 1000 FFT's\nfor i = 1:1000\n      B2 = fft(A1);\nend\ntime1 = toc;\nfprintf('%s\\n',\"Time to run FFT on the node:\")\ndisp(time1);\n\n% Use GPU\ntic;\nA2 = gpuArray(A1);\n% Do 1000 FFT's\nfor i = 1:1000\n      % MALAB knows to use GPU FFT because A2 is defined by gpuArray\n        B2 = fft(A2);\nend\ntime2 = toc;\nfprintf('%s\\n',\"Time to run FFT on the GPU:\")\ndisp(time2);\n\n% Will be greater than 1 if GPU is faster\nspeedup = time1/time2 \n</code></pre> We will need to change the partition to <code>gpu</code> to request a gpu. In the Matlab window enter: <pre><code> &gt;&gt; c.AdditionalProperties.AdditionalSubmitArgs=['--partition=gpu --qos=standard --account=PI_UCID --gres=gpu:1 --mem-per-cpu=4G --time=2-00:00:00'] \n</code></pre> </p> <p>Submit the job as before. Since a script is submitted as opposed to a function, only the name of the script is included in the batch command. Do not include the <code>@</code> symbol. In a script there are no inputs or ouptuts. <pre><code> &gt;&gt; j=c.batch('gpu', 'AutoAddClientPath', false) \n</code></pre></p> <p>To get the result: <pre><code> &gt;&gt; j.diary \n</code></pre></p>"},{"location":"Software/MATLAB/matlab_local/#load-and-plot-results-from-a-job","title":"Load and Plot Results from A Job","text":"<p>In this section we will run a job on the cluster and then load and plot the results in the local Matlab workspace. The code for this example is as follows: <pre><code>n=100;\ndisp(\"n = \" + n);\nA = gallery('poisson',n-2);\nb = convn(([1,zeros(1,n-2),1]'|[1,zeros(1,n-1)]), 0.5*ones(3,3),'valid')';\nx = reshape(A\\b(:),n-2,n-2)';%\n</code></pre> As before submit the job: <pre><code> &gt;&gt; j=c.batch('plot_demo', 'AutoAddClientPath', false);\n</code></pre></p> <p>To load 'x' into the local Matlab workspace: <pre><code> &gt;&gt; load(j,'x') \n</code></pre></p> <p>Finally, plot the results: <pre><code> &gt;&gt; plot(x) \n</code></pre></p>"},{"location":"Software/Molecular_Dynamics/gromacs/","title":"GROMACS","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.</p> <p>It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p>"},{"location":"Software/Molecular_Dynamics/gromacs/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command GROMACS 2025.2-CUDA-12.8.0 foss/2025a <code>module load foss/2025a GROMACS/2025.2-CUDA-12.8.0</code> GROMACS 2025.2 foss/2025a <code>module load foss/2025a GROMACS/2025.2</code> GROMACS 2024.4-CUDA-12.6.0 foss/2024a <code>module load foss/2024a GROMACS/2024.4-CUDA-12.6.0</code> GROMACS 2024.4 foss/2024a <code>module load foss/2024a GROMACS/2024.4</code>"},{"location":"Software/Molecular_Dynamics/gromacs/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of GROMACS is available at GROMACS Manual, where you can find the tutorials in topologies, input file format, setting parameters, etc. </p>"},{"location":"Software/Molecular_Dynamics/gromacs/#using-gromacs","title":"Using GROMACS","text":"<p>GROMACS can be used on CPU or GPU. When using GROMACS with GPUs (Graphics Processing Units), the calculations can be significantly accelerated, allowing for faster simulations. You can use GROMACS with GPU acceleration, but you need to use GPU nodes on our cluster. </p> Sample Batch Script to Run GROMACS GPUCPUCPU with threads <pre><code>#!/bin/bash -l\n# NOTE the -l (login) flag!\n#SBATCH -J gmx2023\n#SBATCH -o test.%x.%j.out\n#SBATCH -e test.%x.%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --partition=gpu\n#SBATCH --qos=standard\n#SBATCH --time 72:00:00   # Max 3 days\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --gpus-per-node=2  \n#SBATCH --account=$PI_ucid  # Replace PI_ucid with the UCID of PI\n\nmodule purge\nmodule load wulver\nmodule load foss/2025a GROMACS/2025.2-CUDA-12.8.0\n\nINPUT_DIR=${PWD}/INPUT\nOUTPUT_DIR=${PWD}/OUTPUT\n\ncp -r $INPUT_DIR/* $OUTPUT_DIR/\ncd $OUTPUT_DIR\n\nsrun gmx_mpi mdrun -deffnm run -cpi -v -ntomp 1 -pin on -tunepme -dlb yes -nb gpu -noappend\n</code></pre> <pre><code>#!/bin/bash -l\n# NOTE the -l (login) flag!\n#SBATCH -J gmx2021\n#SBATCH -o test.%x.%j.out\n#SBATCH -e test.%x.%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --time 72:00:00   # Max 3 days\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --account=$PI_ucid  # Replace PI_ucid with the UCID of PI\n\nmodule purge\nmodule load wulver\nmodule load foss/2025a GROMACS/2025.2\n\nINPUT_DIR=${PWD}/INPUT\nOUTPUT_DIR=${PWD}/OUTPUT\n\ncp -r $INPUT_DIR/* $OUTPUT_DIR/\ncd $OUTPUT_DIR\n\nsrun gmx_mpi mdrun -v -deffnm em -cpi -v -ntomp 1 -pin on -tunepme -dlb yes -noappend\n</code></pre> <pre><code>#!/bin/bash -l\n# NOTE the -l (login) flag!\n#SBATCH -J gmx2021\n#SBATCH -o test.%x.%j.out\n#SBATCH -e test.%x.%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --time 72:00:00   # Max 3 days\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --cpus-per-task=2\n#SBATCH --account=$PI_ucid  # Replace PI_ucid with the UCID of PI\n\nmodule purge\nmodule load wulver\nmodule load foss/2025a GROMACS/2025.2\n\nINPUT_DIR=${PWD}/INPUT\nOUTPUT_DIR=${PWD}/OUTPUT\nOMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\ncp -r $INPUT_DIR/* $OUTPUT_DIR/\ncd $OUTPUT_DIR\n\nsrun gmx_mpi mdrun -v -deffnm em -cpi -v -ntomp $SLURM_CPUS_PER_TASK -pin on -tunepme -dlb yes -noappend\n</code></pre> <p>The tutorial in the above-mentioned job script can be found in <code>/apps/testjobs/gromacs</code></p>"},{"location":"Software/Molecular_Dynamics/gromacs/#related-applications","title":"Related Applications","text":"<ul> <li>LAMMPS</li> </ul>"},{"location":"Software/Molecular_Dynamics/gromacs/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/Molecular_Dynamics/lammps/","title":"LAMMPS","text":"<p>LAMMPS is a large scale classical molecular dynamics code, and stands for Large-scale Atomic/Molecular Massively Parallel Simulator.  LAMMPS has potentials for soft materials (biomolecules, polymers), solid-state materials (metals, semiconductors) and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, meso, or continuum scale.</p>"},{"location":"Software/Molecular_Dynamics/lammps/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command LAMMPS 29Aug2024_update2-kokkos foss/2024a <code>module load foss/2024a LAMMPS/29Aug2024_update2-kokkos</code> LAMMPS 29Aug2024_update2-kokkos-CUDA-12.6.0 foss/2024a <code>module load foss/2024a LAMMPS/29Aug2024_update2-kokkos-CUDA-12.6.0</code> LAMMPS 2Aug2023_update2-kokkos-CUDA-12.6.0 foss/2024a <code>module load foss/2024a LAMMPS/2Aug2023_update2-kokkos-CUDA-12.6.0</code> <p>Note</p> <p>To know the deatils about dependent toolchain please go to Toolchains</p>"},{"location":"Software/Molecular_Dynamics/lammps/#application-information-documentation-and-support","title":"Application Information, Documentation and Support","text":"<p>The official LAMMPS is available at LAMMPS Online Manual. LAMMPS has a large user base and a good user support. Question related to using LAMMPS can be posted to the LAMMPS User forum. Archived user mailing list are also useful to resolve some of the common user issues. </p> <p>Tip</p> <p>If after checking the above forum, if you believe that there is an issue with the module, please file a ticket with Service Now</p>"},{"location":"Software/Molecular_Dynamics/lammps/#using-lammps","title":"Using LAMMPS","text":"Sample Batch Script to Run LAMMPS CPUGPU <pre><code>#!/bin/bash\n#SBATCH -J test_lammps\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128\n#SBATCH --mem-per-cpu=4000M # Maximum allowable memory per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n###############################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load foss/2024a LAMMPS\n\nsrun lmp -in test.in\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH -J gpu_lammps\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=gpu \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128\n#SBATCH --mem-per-cpu=4000M # Maximum allowable memory per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --gres=gpu:2\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n###############################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load foss/2024a LAMMPS/29Aug2024_update2-kokkos-CUDA-12.6.0\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport OMP_PROC_BIND=spread\nexport OMP_PLACES=threads\n\n\nsrun lmp -in in.lj -k on g 2 -sf kk -pk kokkos newton off neigh full comm device\n</code></pre> <p>Then submit the job script using the sbatch command, e.g., assuming the job script name is <code>test_lammps.slurm</code>:</p> <pre><code>sbatch test_lammps.slurm\n</code></pre>"},{"location":"Software/Molecular_Dynamics/lammps/#building-lammps-from-source","title":"Building LAMMPS from source","text":"<p>Some users may be interested in building LAMMPS from source to enable more specific LAMMPS packages.  The source files for LAMMPS can be downloaded as either a tar file  or from the LAMMPS Github repository. </p> Building on Cluster <p>The following procedure was used to build LAMMPS on Wulver.  In the terminal:</p> Wulver <pre><code>module purge\nmodule load wulver\nmodule load foss\nmodule load CMake\n\ngit clone https://github.com/lammps/lammps.git\ncd lammps\nmkdir build\ncd build\n\ncmake -DCMAKE_INSTALL_PREFIX=$PWD/../install_hsw -DCMAKE_CXX_COMPILER=mpicxx \\\n            -DCMAKE_BUILD_TYPE=Release -D BUILD_MPI=yes -DKokkos_ENABLE_OPENMP=ON \\\n            -DKokkos_ARCH_HSW=ON -DCMAKE_CXX_STANDARD=17 -D PKG_MANYBODY=ON \\\n            -D PKG_MOLECULE=ON -D PKG_KSPACE=ON -D PKG_REPLICA=ON -D PKG_ASPHERE=ON \\\n            -D PKG_RIGID=ON -D PKG_KOKKOS=ON -D DOWNLOAD_KOKKOS=ON \\\n            -D CMAKE_POSITION_INDEPENDENT_CODE=ON -D CMAKE_EXE_FLAGS=\"-dynamic\" ../cmake\nmake -j16\nmake install\n</code></pre>"},{"location":"Software/Molecular_Dynamics/lammps/#related-applications","title":"Related Applications","text":"<ul> <li>GROMACS</li> </ul>"},{"location":"Software/Molecular_Dynamics/lammps/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/Molecular_Dynamics/plumed/","title":"PLUMED","text":""},{"location":"Software/Molecular_Dynamics/plumed/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command PLUMED 2.9.4 foss/2025a <code>module load foss/2025a PLUMED/2.9.4</code> PLUMED 2.9.3 foss/2024a <code>module load foss/2024a PLUMED/2.9.3</code>"},{"location":"Software/Molecular_Dynamics/plumed/#related-applications","title":"Related Applications","text":"<ul> <li>GROMACS, LAMMPS</li> </ul>"},{"location":"Software/Molecular_Dynamics/plumed/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/cp2k/","title":"CP2K","text":"<p>CP2K is a free and open-source quantum chemistry and solid-state physics software package that is designed to perform atomistic simulations of a wide range of materials. It is capable of carrying out a variety of quantum mechanical calculations, including density functional theory (DFT), time-dependent DFT, and many-body perturbation theory (MBPT).</p> <p>CP2K is optimized for running on massively parallel supercomputers, making it well-suited for large-scale simulations of complex systems. It includes a number of advanced algorithms and techniques for efficiently calculating electronic properties of materials, such as linear scaling algorithms for DFT and MBPT calculations.</p> <p>CP2K is highly modular and flexible, allowing users to customize and extend its functionality by writing their own input files or by modifying the source code. It also includes a graphical user interface (GUI) for setting up and running simulations, as well as a number of built-in tools for analyzing simulation output.</p> <p>CP2K is widely used in the fields of materials science, chemistry, and physics for studying a wide range of materials, including biomolecules, polymers, and solids.</p>"},{"location":"Software/chemistry/cp2k/#availability","title":"Availability","text":"Software Version Dependent Toolchain Module Load Command CP2K 2025.2 foss/2025a <code>module load foss/2025a CP2K/2025.2</code>"},{"location":"Software/chemistry/cp2k/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of CP2K is available at CP2K Documentation. For any issues CP2K simulation, users can contact at CP2K Forum. </p>"},{"location":"Software/chemistry/cp2k/#using-cp2k","title":"Using CP2K","text":"<p>CP2K MPI/OpenMP-hybrid Execution (PSMP), CP2K with Population Analysis capabilities- CP2K-popt</p> Sample Batch Script to Run CP2K : cp2k.submit.sh <pre><code>#!/bin/bash -l\n#SBATCH -J CP2K\n#SBATCH -o sn1-xtb_input.out\n#SBATCH -e sn1-xtb_input.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=16\n#SBATCH --mem-per-cpu=4G\n#SBATCH --qos=standard\n#SBATCH --partition=general\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH -t 72:00:00\n\n#module load command\n\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver\nmodule load foss/2025a CP2K\n\n#Run the program\n\ninputFile=sn1-xtb_input.inp\noutputFile=sn1-xtb_input.out\nmpirun -np $SLURM_NTASKS cp2k.popt -i $inputFile -o $outputFile\n</code></pre> <p>The sample input file <code>sn1-xtb_input.inp</code> can be found in <code>/apps/testjobs/CP2K</code></p> <p>Important</p> <p>Please don't run anything on <code>/apps/testjobs/CP2K</code> as users have only read-only permission. You need to copy the submit script and input file from <code>/apps/testjobs/CP2K</code> to <code>$HOME</code> or <code>/project</code>.</p>"},{"location":"Software/chemistry/cp2k/#related-applications","title":"Related Applications","text":"<ul> <li>Gaussian</li> <li>ORCA</li> </ul>"},{"location":"Software/chemistry/cp2k/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/gaussian/","title":"Gaussian","text":""},{"location":"Software/chemistry/gaussian/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command Gaussian 16.C.03-AVX2 - <code>module load Gaussian/16.C.03-AVX2</code> <p>Important</p> <p>Due to licensing restrictions, Gaussian is not automatically accessible to all HPC users. Students are required to contact hpc@njit.edu to request access to Gaussian.</p>"},{"location":"Software/chemistry/gaussian/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of Gaussian is available at Gaussian Documentation. For any issues Gaussian simulation, users can contact at Gaussian Support. </p>"},{"location":"Software/chemistry/gaussian/#using-gaussian","title":"Using Gaussian","text":"Sample Batch Script to Run Gaussian : g16.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH -J g16\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --mem-per-cpu=3G\n#SBATCH --time=10:00:00\n#SBATCH --partition=general\n#SBATCH --account=PI_UCID # Replace PI_UCID with the UCID of PI\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err\n#SBATCH --qos=standard\n\n#module load command\n\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver\nmodule load Gaussian\n\n#Run the program\n\ng16 test_g98.com\n</code></pre> <p>The sample input file <code>test_g98.com</code> can be found in <code>/apps/testjobs/Gaussian</code></p> <p>Important</p> <p>Please don't run anything on <code>/apps/testjobs/Gaussian</code> as users have only read-only permission. You need to copy the submit script and input file from <code>/apps/testjobs/Gaussian</code> to <code>$HOME</code> or <code>/project</code>.</p>"},{"location":"Software/chemistry/gaussian/#related-applications","title":"Related Applications","text":"<ul> <li>CP2K</li> <li>ORCA</li> </ul>"},{"location":"Software/chemistry/gaussian/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/orca/","title":"ORCA","text":""},{"location":"Software/chemistry/orca/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command ORCA 6.0.1-avx2 foss/2024a <code>module load foss/2024a ORCA/6.0.1-avx2</code>"},{"location":"Software/chemistry/orca/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of Gaussian is available at ORCA Documentation. For any issues Gaussian simulation, users can contact at ORCA Forum. </p>"},{"location":"Software/chemistry/orca/#using-gaussian","title":"Using Gaussian","text":"Sample Batch Script to Run ORCA : orca.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH --job-name=job_orca\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n#SBATCH --partition=general\n#SBATCH --qos=standard\n#SBATCH --nodes=1\n#SBATCH --account=PI_UCID # Replace PI_UCID with the UCID of PI\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=59:00  # D-HH:\n\n#module load command\n\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver\nmodule load foss/2024a ORCA\n\n#Run the program\n\nsrun orca test.inp &gt; geom.out\n</code></pre> <p>The sample input file <code>test.inp</code> can be found in <code>/apps/testjobs/ORCA</code></p> <p>Important</p> <p>Please don't run anything on <code>/apps/testjobs/ORCA</code> as users have only read-only permission. You need to copy the submit script and input file from <code>/apps/testjobs/ORCA</code> to <code>$HOME</code> or <code>/project</code>.</p>"},{"location":"Software/chemistry/orca/#related-applications","title":"Related Applications","text":"<ul> <li>Gaussian</li> <li>CP2K</li> </ul>"},{"location":"Software/chemistry/orca/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/qe/","title":"Quantum Espresso","text":""},{"location":"Software/chemistry/qe/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command QuantumESPRESSO 7.4 foss/2024a <code>module load foss/2024a QuantumESPRESSO/7.4</code>"},{"location":"Software/chemistry/qe/#related-applications","title":"Related Applications","text":""},{"location":"Software/chemistry/qe/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/siesta/","title":"SIESTA","text":"<p>SIESTA (Spanish Initiative for Electronic Simulations with Thousands of Atoms) is a free and open-source software package for performing electronic structure calculations of materials using density functional theory (DFT). It is particularly well-suited for simulating materials containing large numbers of atoms, such as nanomaterials and surfaces.</p> <p>SIESTA uses a localized basis set approach that reduces the computational cost of DFT calculations by approximating the electronic wavefunction using a small number of basis functions centered around each atom. This allows SIESTA to efficiently simulate systems containing thousands of atoms with reasonable accuracy.</p> <p>SIESTA is actively developed and maintained by a team of researchers at the Universidad Aut\u00f3noma de Madrid in Spain, and is available as a free download under an open-source license. It is widely used in the fields of material science, chemistry, and physics for studying a wide range of materials, including metals, semiconductors, and biological molecules.</p>"},{"location":"Software/chemistry/siesta/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command Siesta 5.4.0 foss/2024a <code>module load foss/2024a Siesta/5.4.0</code>"},{"location":"Software/chemistry/siesta/#related-applications","title":"Related Applications","text":""},{"location":"Software/chemistry/siesta/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/chemistry/vasp/","title":"VASP","text":"<p>VASP (Vienna Ab initio Simulation Package) is a commercial software package for performing first-principles quantum mechanical calculations in materials science, chemistry, and physics. It is based primarily on density functional theory (DFT), with extensions for time-dependent DFT and many-body perturbation theory (e.g., GW and RPA).</p> <p>VASP is particularly known for its efficient and accurate implementation of the projector augmented-wave (PAW) method, enabling precise electronic structure calculations. It supports geometry optimizations, molecular dynamics, phonon calculations, and band structure analysis, making it a powerful tool for studying molecules, solids, surfaces, and interfaces.</p> <p>The code is optimized for massively parallel supercomputers and includes advanced iterative diagonalization and charge-density mixing algorithms for large-scale simulations. Although VASP itself does not include a GUI, it integrates seamlessly with a wide range of external tools for input generation and visualization of results.</p>"},{"location":"Software/chemistry/vasp/#availability","title":"Availability","text":"<p>Since VASP is licensed software, access is restricted to the cluster. If your research group has a valid VASP license and wants to use it, please ask your advisor/PI to email hpc@njit.edu with the following details</p> <ol> <li>UCIDs of the students who need access to VASP </li> <li>Licensed VASP version (e.g., 6.x or 5.x)</li> <li>Proof of license (license confirmation/contract)</li> </ol>"},{"location":"Software/chemistry/vasp/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of CP2K is available at VASP Documentation. For any issues in VASP simulation, users can contact at VASP Forum. </p>"},{"location":"Software/chemistry/vasp/#using-vasp","title":"Using VASP","text":"<p>CP2K MPI/OpenMP-hybrid Execution (PSMP), CP2K with Population Analysis capabilities- CP2K-popt</p> Sample Batch Script to Run CP2K : cp2k.submit.sh <pre><code>#!/bin/bash -l\n#SBATCH -J VASP\n#SBATCH -o vasp.out\n#SBATCH -e vasp.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=16\n#SBATCH --mem-per-cpu=4G\n#SBATCH --qos=standard\n#SBATCH --partition=general\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH -t 72:00:00\n\n#module load command\n\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver\nmodule load intel/2025a HDF5 VASP\n\nsrun --mpi=pmix vasp_std &gt; vasp.log\n</code></pre>"},{"location":"Software/chemistry/vasp/#related-applications","title":"Related Applications","text":"<ul> <li>Gaussian</li> <li>ORCA</li> </ul>"},{"location":"Software/chemistry/vasp/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/programming/compilers/","title":"Compilers and Toolchains","text":""},{"location":"Software/programming/compilers/#gnu-and-intel-compilers","title":"GNU and Intel Compilers","text":"<p>We offer both GNU and Intel compilers. Here is the list of compilers you can find on our cluster.</p> Wulver Software Version Dependent Toolchain Module Load Command intel-compilers 2025.1.1 intel/2025a <code>module load intel-compilers/2025.1.1</code> intel-compilers 2024.2.0 intel/2024a <code>module load intel-compilers/2024.2.0</code> GCC 14.2.0 foss/2025a <code>module load GCC/14.2.0</code> GCC 13.3.0 foss/2024a <code>module load GCC/13.3.0</code>"},{"location":"Software/programming/compilers/#mpi-libraries","title":"MPI Libraries","text":"<p>MPI (Message Passing Interface) libraries are a set of software tools that allow for parallel computing on distributed memory systems, such as computer clusters. These libraries provide a standardized interface for communication between processes running on different nodes of the cluster. There are several implementations of MPI libraries available, such as OpenMPI, MPICH, and Intel MPI. Currently, the following MPI libraries on our cluster.</p> Wulver Software Version Dependent Toolchain Module Load Command impi 2021.15.0 intel/2025a <code>module load intel/2025a</code> impi 2021.13.0 intel/2024a <code>module load intel/2024a</code> OpenMPI 5.0.7 foss/2025a <code>module load foss/2025a</code> OpenMPI 5.0.3 foss/2024a <code>module load foss/2024a</code>"},{"location":"Software/programming/compilers/#toolchains","title":"Toolchains","text":"<p>We use EasyBuild to install the packages as modules and to avoid too many packages tol load as a module, we use pre-defined build environment modules called toolchains which include a combination of tools such as compilers, libraries etc. We use <code>foss</code> and <code>intel</code> toolchains in Wulver. The advantage of using toolchains is that user can load either <code>foss</code> or <code>intel</code> as base package and the additional libraries such as MPI, LAPACK and other math libraries will be automatically loaded. </p>"},{"location":"Software/programming/compilers/#free-open-source-software-foss","title":"Free Open Source Software (foss)","text":"<p>The <code>foss</code> toolchains are versioned with a yearletter scheme, e.g. <code>foss/2021b</code> is the second foss toolchain composed in 2021. The <code>foss</code> toolchain comprises the following </p> <pre><code>\ud83d\udcc1 foss\n\u251c\u2500\u2500 \ud83d\udcc1 gompi \n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 GCC\n\u2502   \u2502   \u251c\u2500\u2500  GCCcore\n\u2502   \u2502   \u251c\u2500\u2500  zlib\n\u2502   \u2502   \u2514\u2500\u2500  binutils\n\u2502   \u251c\u2500\u2500  OpenMPI\n\u2502   \u251c\u2500\u2500  numactl\n\u2502   \u251c\u2500\u2500  XZ\n\u2502   \u251c\u2500\u2500  libxml2\n\u2502   \u251c\u2500\u2500  libpciaccess\n\u2502   \u251c\u2500\u2500  hwloc\n\u2502   \u251c\u2500\u2500  OpenSSL\n\u2502   \u251c\u2500\u2500  libevent\n\u2502   \u251c\u2500\u2500  UCX\n\u2502   \u251c\u2500\u2500  libfabric\n\u2502   \u251c\u2500\u2500  PMIx\n\u2502   \u2514\u2500\u2500  UCC\n\u251c\u2500\u2500  FFTW\n\u251c\u2500\u2500  OpenBLAS\n\u251c\u2500\u2500  ScaLAPACK\n\u2514\u2500\u2500  FlexiBLAS\n</code></pre> Wulver Software Version Dependent Toolchain Module Load Command foss 2024a - <code>module load foss/2024a</code> foss 2025a - <code>module load foss/2025a</code> <p>To see GCC and OpenMPI versions details in each toolchain, see the list of compiler versions and OpenMPI versions.</p>"},{"location":"Software/programming/compilers/#intel","title":"Intel","text":"<p>Like <code>foss</code>, <code>intel</code> toolchains are versioned with yearletter scheme, e.g. <code>intel/2021b</code> is the second intel toolchain composed in 2021.</p> <pre><code>\ud83d\udcc1 intel\n\u251c\u2500\u2500 \ud83d\udcc1 intel-compilers\n\u2502   \u251c\u2500\u2500  GCCcore   \n\u2502   \u251c\u2500\u2500  zlib\n\u2502   \u2514\u2500\u2500  binutils\n\u251c\u2500\u2500 \ud83d\udcc1 iimpi\n\u2502   \u251c\u2500\u2500  iccifort\n\u2502   \u251c\u2500\u2500  Intel MPI\n\u2502   \u251c\u2500\u2500  numactl\n\u2502   \u2514\u2500\u2500  UCX\n\u2514\u2500\u2500  Intel Math Kernel Library\n</code></pre> Wulver Software Version Dependent Toolchain Module Load Command intel 2024a - <code>module load intel/2024a</code> intel 2025a - <code>module load intel/2025a</code> <p>The <code>intel-compilers</code> and <code>impi</code> versions in <code>intel</code> toolchains are tabulated in intel versions and impi versions. To see the versions of <code>GCCcore</code> and <code>mkl</code> libraries of <code>intel</code> toolchain, please load the intel toolchain module with yearletter version, e.g. <code>module load intel/2021b</code> and then use <code>module li</code>.</p>"},{"location":"Software/python/","title":"Python","text":"<p>Python is a high-level, general-purpose programming language. Python supports multiple programming paradigms, including structured, object-oriented, and functional programming. It is used in a wide range of applications such as machine learning, molecular dynamics, scientific computing, automation, image processing, etc.</p>"},{"location":"Software/python/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command Python 3.13.1 foss/2025a <code>module load foss/2025a Python/3.13.1</code> Python 3.12.3 foss/2024a <code>module load foss/2024a Python/3.12.3</code>"},{"location":"Software/python/#python-libraries","title":"Python libraries","text":"<p>Apart from Python\u2019s standard library, Python offers a wide range of additional libraries that need to be loaded as modules before users can use these. Here, we list these additional libraries. Please contact us to file a ticket with Service Now in case you do not find the libraries you want to use.</p> Software Version Dependent Toolchain Module Load Command matplotlib 3.10.3 foss/2025a <code>module load foss/2025a matplotlib/3.10.3</code> SciPy-bundle 2025.06 foss/2025a <code>module load foss/2025a SciPy-bundle/2025.06</code> matplotlib 3.9.2 foss/2024a <code>module load foss/2024a matplotlib/3.9.2</code> SciPy-bundle 2024.05 foss/2024a <code>module load foss/2024a SciPy-bundle/2024.05</code> <p>For using multiple libraries, you simply need to add the library name in <code>module load</code> command. For example, to load NumPy, Matplotlib, and SciPy together, you need to use the following command. </p> <pre><code>module load foss/2025a SciPy-bundle matplotlib\n</code></pre>"},{"location":"Software/python/#using-conda-or-pip-to-install-python-libraries","title":"Using Conda or <code>pip</code> to Install Python Libraries","text":"<p>Since sometimes users a specific version of Python libraries, it is advisable to use Conda so that users can create their own environment where they can install required packages based on their requirements. Conda is a cross-language package manager that excels at managing environments and dependencies, making it suitable for scientific computing and data science projects. It can handle non-Python libraries and binaries, offering a comprehensive solution. On the other hand, pip is the default Python package installer, known for its simplicity and compatibility with the Python Package Index (PyPI). While both tools serve the same purpose, it is generally recommended to choose one and remain consistent within a project to avoid potential conflicts. Please see Conda Documentation on how to install Python packages via Conda.</p>"},{"location":"Software/python/conda/","title":"Conda Environments","text":"<p>Since Python supports a wide range of additional libraries in machine learning or data science research, it is not always possible to install every package on HPC. Also, users sometimes need to use a specific version of Python or its libraries to conduct their research. Therefore, in that case, users can build their own Python version along with a specific library. One of the ways to accomplish this is to use Conda.</p>"},{"location":"Software/python/conda/#what-is-conda","title":"What is Conda","text":"<p>Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because conda is also an environment manager. </p>"},{"location":"Software/python/conda/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command Miniforge3 24.11.3-0 - <code>module load Miniforge3/24.11.3-0</code>"},{"location":"Software/python/conda/#conda-user-commands","title":"Conda User Commands","text":"Task Command Activate environment: <code>conda activate [environment_name]</code> Deactivate environment: <code>conda deactivate [environment_name]</code> Show the list of environments: <code>conda env list</code> Delete environment: <code>conda remove --name [environment_name] --all</code> Export environment: <code>conda env export &gt; [environment_name].yml</code> Import environment from YAML: <code>conda env create -f [environment_name].yml</code> Import environment to different location: <code>conda env create -f [environment_name].yml -p [PATH]</code>"},{"location":"Software/python/conda/#create-and-activate-a-conda-virtual-environment","title":"Create and Activate a Conda Virtual Environment","text":"<p>Tip</p> <p>The login node has limited memory and processing resources, which can slow down conda environment creation and package installation. It is recommended to start an interactive session on a compute node before creating or modifying your conda environment.</p> <p>Load the Miniforge3 Module</p> <pre><code>module load Miniforge3\n</code></pre>"},{"location":"Software/python/conda/#create-environment-with-conda","title":"Create Environment with <code>conda</code>","text":"<p>To create an environment use the <code>conda create</code> command. Once the environment is created, you need to use <code>conda activate</code> to activate the environment. To create an environment with a specific python version, use <code>conda create --name ENV python=3.9</code> where <code>ENV</code> is the name of the environment. You can choose any environment name of your choice.</p> <p>Info</p> <p>You don't need to specify the Python version when creating a Conda environment. If you don't specify it, Conda will use the latest available version by default. In that case, simply use <code>conda create --name ENV</code>.</p>"},{"location":"Software/python/conda/#activate-and-deactivate-conda-environment","title":"Activate and Deactivate Conda Environment","text":"<p>Once you create an environment, you need to activate the environment to install python packages Use <code>conda activate ENV</code> to activate the Conda environment (<code>ENV</code> is the name of the environment). Following the activation of the conda environment, the name of the environment appears at the left of the hostname in the terminal. </p> <pre><code>[ls565@n0058 ~]$ module load Miniforge3\n[ls565@n0058 ~]$ conda create --name ENV python=3.9\n[ls565@n0058 ~]$ conda activate ENV\n(ENV) [ls565@n0058 ~]$\n</code></pre> <p>Once you finish the installation of Python packages, deactivate the conda environment using <code>conda deactivate ENV</code>. </p> <p>Warning</p> <p>Please note that you may need to create multiple Conda environments, as some packages may not work in a single environment. For example, if you want to install PyTorch and TensorFlow, it's advisable to create separate environments as sometimes both packages in a single environment can cause errors. To create another environment make sure to deactivate the previous environment by using the <code>conda deactivate</code> command. </p>"},{"location":"Software/python/conda/#install-python-packages-via-conda","title":"Install Python Packages Via Conda","text":"<p>Once Conda environment is activated, you can install packages via <code>conda install package_name</code> command. For example, if you want to install <code>matplotlib</code>, you need to use</p> <p><pre><code>(ENV) [ls565@n0058 ~]$ conda install -c conda-forge matplotlib\n</code></pre> where <code>conda-forge</code> is the name of the conda channel. </p> <p>Warning</p> <p>Make sure to activate the conda environment prior to installing Python packages. </p>"},{"location":"Software/python/conda/#conda-channel","title":"Conda Channel","text":"<p>Conda Channel refers to a repository or collection of software packages that are available for installation using Conda. Conda Channels are used to organize and distribute packages, and they play a crucial role in the Conda ecosystem. Channels can be specified using the <code>--channel</code> or <code>-c</code> option with the conda install command i.e.  <code>conda install -c channel_name package_name</code>. </p> <p>Tip</p> <p>Since memory and CPU usage are limited, it's better to start an interactive session with the compute node whenever you are installing Python packages via Conda.</p> <p>In the above example, the command <code>conda install -c conda-forge matplotlib</code> will install <code>matplotlib</code> from <code>conda-forge</code> channel which is a community-maintained collection of Conda packages where a wide range of packages contributed by the community are available.  Users can prioritize channels by listing them in a specific order, so that Conda searches channels in the order they are listed, installing the first version of a package that it finds. To list the channels, create a file <code>.condarc</code> in the <code>$HOME</code> directory and add the following</p> <p><pre><code>auto_activate_base: false\nchannels:\n  - conda-forge\n</code></pre> The advantage of using <code>.condarc</code> is that you don't have to mention the channel name every time you install a package. However, please note that you still need to use the channel name if you want to install Python packages that require a specific channel other than the <code>conda-forge</code> channel.</p>"},{"location":"Software/python/conda/#examples-of-conda-environemnt","title":"Examples of Conda Environemnt","text":"<p>Here, we provide some examples of how to use <code>conda</code> to install applications. </p> <p>Note</p> <p>For the following examples please make sure to start an interactive session on GPU node.</p>"},{"location":"Software/python/conda/#install-tensorflow-with-gpu","title":"Install TensorFlow with GPU","text":"<ul> <li> <p>The following example will create a new conda environment and install TensorFlow in the environment. <pre><code>[ls565@n0058 ~]$ module load Miniforge3\n[ls565@n0058 ~]$ conda create --name tf \n</code></pre></p> </li> <li> <p>Activate the new 'tf' environment <pre><code>[ls565@n0058 ~]$ conda activate tf\n(tf) [ls565@n0058 ~]$\n</code></pre></p> </li> <li> <p>Install tensorflow-gpu <pre><code>(tf) [ls565@n0058 ~]$ conda install -c conda-forge tensorflow-gpu\n</code></pre></p> </li> <li> <p>Check if TensorFlow can be loaded <pre><code>(tf) [ls565@n0058 ~]$ python -c \"import tensorflow as tf; print( tf.__version__)\"\n</code></pre></p> </li> <li> <p>Check if TensorFlow is compiled with GPU <pre><code>(tf) [ls565@n0058 ~]$ python -c \"import tensorflow as tf; print(tf.test.is_built_with_gpu_support())\"\n</code></pre></p> </li> </ul> <p>You can also verify using this simple TensorFlow test program to make sure the virtual env can access a GPU. </p> tf.gpu.test.py <pre><code>import tensorflow as tf\n\nif tf.test.gpu_device_name():\n\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n\nelse:\n\n   print(\"Please install GPU version of TF\")\n</code></pre> Slurm script to submit the job Wulver <pre><code>#!/bin/bash -l\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Purge any module loaded by default\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver # Load slurm, easybuild\nmodule load Miniforge3\nconda activate tf\nsrun python tf.gpu.test.py\n</code></pre> <p>Next, deactivate the environment using <code>conda deactivate tf</code> command.</p>"},{"location":"Software/python/conda/#install-pytorch-with-gpu","title":"Install PyTorch with GPU","text":"<ul> <li>To install PyTorch with GPU, load the <code>Miniforge3</code> module as described above and then use the following</li> </ul> <pre><code>conda create --name torch-cuda\nconda activate torch-cuda\nconda install -c \"nvidia/label/cuda-12.2.0\" cuda-toolkit\nconda install -c pytorch -c nvidia pytorch torchvision torchaudio pytorch-cuda -y\n</code></pre> <p>Info</p> <p>In the example above, we mentioned the channel name as we intend to install PyTorch and PyTorch-CUDA from a specific channel. For the default channel please see Channels.</p> <ul> <li>Check the Torch version <pre><code>python -c \"import torch; print( torch.__version__)\"\n</code></pre></li> <li>Check the CUDA version <pre><code>python -c \"import torch; print(torch. version .cuda)\"\n</code></pre></li> <li>Check whether Torch is compiled with CUDA <pre><code>python -c \"import torch; print(torch.cuda. is_available())\"\n</code></pre></li> </ul> <p>Warning</p> <p>While checking the CUDA version or PyTorch compilation using the commands mentioned above, make sure to start an interactive session on a GPU node; otherwise, the command will not recognize CUDA or the GPU.</p> <ul> <li>A simple PyTorch test program is given below to check whether PyTorch has been installed properly. Program is called</li> </ul> torch_tensor.py <pre><code># -*- coding: utf-8 -*-\n\nimport torch\nimport math\n\n\ndtype = torch.float\n#device = torch.device(\"cpu\")   # Uncomment this to run on CPU\ndevice = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n\n# Create random input and output data\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Randomly initialize weights\na = torch.randn((), device=device, dtype=dtype)\nb = torch.randn((), device=device, dtype=dtype)\nc = torch.randn((), device=device, dtype=dtype)\nd = torch.randn((), device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    if t % 100 == 99:\n        print(t, loss)\n\n    # Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights using gradient descent\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n</code></pre> <p>User can use the following job script to run the script.</p> torch-cuda.submit.sh Wulver <pre><code>#!/bin/bash -l\n#SBATCH --job-name=torch_test\n#SBATCH --output=%x.%j.out # %x.%j expands to JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n\n# Purge any module loaded by default\nmodule purge &gt; /dev/null 2&gt;&amp;1\nmodule load wulver # Load slurm, easybuild\nmodule load Miniforge3\nconda activate torch-cuda\nsrun python touch_tensor.py\n</code></pre> <p>Warning</p> <p>When working with Python, it is generally advised to avoid mixing package management tools such as pip and conda within the same environment. Pip and Conda manage dependencies differently, and their conflict can lead to compatibility issues and unexpected behavior. Mixing the two can result in an environment where packages installed with one tool may not interact seamlessly with those installed using the other. </p>"},{"location":"Software/python/conda/#mamba-the-conda-alternative","title":"Mamba: The Conda Alternative","text":"<p>Mamba is a fast, robust, and cross-platform package manager and particularly useful for building complicated environments, where <code>conda</code> is unable to 'solve' the required set of packages within a reasonable amount of time. Users can install packages with <code>mamba</code> in the same way as with <code>conda</code>. <pre><code>module load Miniforge3\n\n# create new environment\nconda create --name env_name python numpy pandas \n# install a new package into an existing environment\nconda activate env_name\nmamba install scipy\n</code></pre></p>"},{"location":"Software/python/conda/#example-of-installing-pytorch-via-mamba","title":"Example of Installing PyTorch via mamba","text":"<pre><code>module load Miniforge3\nconda create --name torch-cuda\nconda activate torch-cuda\nmamba install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n</code></pre> <p>This will install pytorch in the <code>torch-cuda</code> environment.</p>"},{"location":"Software/python/conda/#export-and-import-conda-environment","title":"Export and Import Conda Environment","text":"<p>Exporting and importing Conda environments allows users to capture and reproduce the exact set of dependencies for a project. With Conda, a popular package and environment management system, users can export an environment, including all installed packages, into a YAML file. This file can then be shared or version-controlled. Importing the environment from the YAML file on another system ensures consistent dependencies, making it easier to recreate the development or execution environment. </p> <p>Tips</p> <p>When installing Python packages via Conda, ensure that you perform the installation on the compute node rather than the login node. The CPU and memory resources on login nodes are limited, and installing Python packages on the login node can be time-consuming. To avoid this, initiate an tnteractive session with compute node.</p>"},{"location":"Software/python/conda/#export-conda-environment","title":"Export Conda Environment","text":"<p>To export a conda environment to a new directory or a different machine, you need to activate the environment first that you intend to export. Please see Conda environment on how to activate the environment. Once your environment is activated, you can export it to a YAML file: <pre><code>conda env export &gt; my_environment.yml\n</code></pre> The YAML should look like this</p> <p><pre><code>name: my_env\nchannels:\n- defaults\ndependencies:\n- _libgcc_mutex=0.1=main\n- _openmp_mutex=5.1=1_gnu\n- blas=1.0=mkl\n\n&lt;ouput snipped&gt;\n\n#the last line is the path of the env\nprefix: /home/a/abc3/.conda/envs/my_env.\n</code></pre> Next, edit the <code>my_environment.yml</code> file to make sure it has the correct environment name and other settings. The last line of the file specifies the path of the environment.</p> <p>Once the YAML file is ready, you can transfer the <code>my_environment.yml</code> file to the new machine or directory where you want to replicate the environment. See cluster file transfer for details on transferring the files to clusters.</p>"},{"location":"Software/python/conda/#set-different-location-for-conda-environment-and-package","title":"Set Different Location for Conda Environment and Package","text":"<p>Since Conda, by default, downloads packages and creates environments in the <code>$HOME</code> directory, users might encounter disk quota errors if multiple environments are created. In such cases, please follow the above steps to move the existing environments from <code>$HOME</code> to <code>/project</code>. For future environment and package downloads, create a <code>.condarc</code> file in the <code>$HOME</code> directory and add the following: <pre><code>auto_activate_base: false\nenvs_dirs:\n  - /path/to/custom/conda/envs/directory\npkgs_dirs:\n  - /path/to/custom/conda/pkgs\n</code></pre></p> <p>Replace <code>/path/to/custom/conda/envs/directory</code> with the path you want to use. </p>"},{"location":"Software/python/conda/#import-environment-on-new-machine","title":"Import Environment on New Machine","text":"<p>On the new machine, first load Anaconda and initialize conda as before. Then, create the environment from the YAML file:</p> <p><pre><code>conda env create -f my_environment.yml\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n&lt;ouput snipped&gt;\n\nDownloading and Extracting Packages\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n# $ conda activate my_env\n#\n# To deactivate an active environment, use\n#\n# $ conda deactivate\n</code></pre> After running this command, Conda will set up the environment as it was on the original machine, including downloading and installing packages. To activate the New Environment use <code>conda activate my_env</code> where <code>my_env</code> is the environment name. You can check your current environments using <code>conda env list</code>.</p>"},{"location":"Software/python/conda/#importing-to-a-different-location","title":"Importing to a Different Location","text":"<p>If you want to import the conda environment to a different location, use the <code>--prefix</code> or <code>-p</code> option <pre><code>conda env create -f my_environment.yml -p /project/hpcadmins/abc3/conda_env/my_env\n</code></pre> This will create the environment in the specified directory instead of the default conda environment directory. Please note that in that case, you need to provide the full path of the environment to activate it.</p> <p><pre><code>conda activate /project/hpcadmins/abc3/conda_env/my_env\n(/project/hpcadmins/abc3/conda_env/my_env) abc3@login01:~$ conda env list\n# conda environments:\n#\nbase /apps/easybuild/software/Miniforge3/2023.09-0\n* /project/hpcadmins/abc3/conda_env/my_env\n</code></pre> By following these steps, you can successfully export a conda environment from one machine and import it to another, ensuring a consistent working environment across different machines or directories.</p> <p>Warning</p> <p>It is advisable to use the <code>/project</code> directory to store the Conda environment rather than using the <code>$HOME</code> directory. On Wulver, the storage space on <code>$HOME</code> is limited (50G) and cannot be increased. See Wulver Filesystems for details. </p>"},{"location":"Software/python/jupyter/","title":"Jupyter Notebook","text":"<p>The Jupyter Notebook is a web-based interactive computing platform. The notebook combines live code, equations, narrative text, and visualizations. In our cluster, we have JupyterLab which is the next-generation user interface for Project Jupyter offering all the familiar building blocks of the classic Jupyter Notebook (notebook, terminal, text editor, file browser, rich outputs, etc.) in a flexible and powerful user interface. </p>"},{"location":"Software/python/jupyter/#using-jupyter-notebook-on-wulver","title":"Using Jupyter Notebook on Wulver","text":"<p>Jupyter Notebook via slurm on Wulver is deprecated </p> <p>Since two-factor authentication has been implemented on Wulver, the use of Jupyter Notebook via SLURM scripts has been discontinued and is no longer supported. Users should use OnDemand to use Jupyter Notebook on Wulver. First, users need to install Jupyter Notebook in their Conda environment. Once the Conda Environment is activated, users can install Jupyter Notebook using the command <code>conda install -c conda-forge jupyter notebook</code>. Then, you need to specify the environment in OnDemand to start the Jupyter Notebook session. Check here for details.</p>"},{"location":"Software/python/jupyter/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/slurm/","title":"SLURM","text":"<p>Slurm (Simple Linux Utility for Resource Management) is an open-source workload manager and job scheduler designed for high-performance computing clusters. It is widely used in research, academia, and industry to efficiently manage and allocate computing resources such as CPUs, GPUs, memory, and storage for running various types of jobs and tasks. Slurm helps optimize resource utilization, minimizes job conflicts, and provides a flexible framework for distributing workloads across a cluster of machines. It offers features like job prioritization, fair sharing of resources, job dependencies, and real-time monitoring, making it an essential tool for orchestrating complex computational workflows in diverse fields.</p>"},{"location":"Software/slurm/#availability","title":"Availability","text":"Software Module Load Command slurm <code>module load wulver</code> <p>Please note that the module <code>wulver</code> is already loaded when a user logs in to the cluster. If you use <code>module purge</code> command, make sure to use <code>module load wulver</code> in the slurm script to load SLURM.</p>"},{"location":"Software/slurm/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of SLURM is available at SLURM manual. </p>"},{"location":"Software/slurm/#managing-and-monitoring-jobs","title":"Managing and Monitoring Jobs","text":"<p>Our website structure has changed. For more details on monitoring jobs, check Running Jobs. </p>"},{"location":"Software/utilities/apptainer/","title":"Apptainer","text":"<p>Apptainer (formerly Singularity) s a container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Apptainer on your laptop, and then run it on many of the largest HPC clusters in the world, local university or company clusters, a single server, in the cloud, or on a workstation down the hall. Your container is a single file, and you don\u2019t have to worry about how to install all the software you need on each different operating system.</p> <p>Apptainer was created to run complex applications on HPC clusters in a simple, portable, and reproducible way. First developed at Lawrence Berkeley National Laboratory, it quickly became popular at other HPC sites, academic sites, and beyond. Apptainer is an open-source project, with a friendly community of developers and users. The user base continues to expand, with Apptainer now used across industry and academia in many areas of work.</p> <p>Many container platforms are available, but Apptainer is focused on:</p> <ul> <li>Verifiable reproducibility and security, using cryptographic signatures, an immutable container image format, and in-memory decryption.</li> <li>Integration over isolation by default. Easily make use of GPUs, high-speed networks, parallel filesystems on a cluster or server by default.</li> <li>Mobility of computing. The single file SIF container format is easy to transport and share.</li> <li>A simple, effective security model. You are the same user inside a container as outside, and cannot gain additional privilege on the host system by default. Read more about Security in Apptainer.</li> </ul>"},{"location":"Software/utilities/apptainer/#use-cases","title":"Use Cases","text":"<ul> <li>BYOE: Bring Your Own Environment!</li> <li>Reproducible science</li> <li>Commercially supported code requiring a particular environment</li> <li>Static environments (software appliances)</li> <li>Legacy code on old operating systems</li> <li>Complicated software stacks that are very host specific</li> <li>Complicated work-flows that require custom installation and/or data</li> </ul>"},{"location":"Software/utilities/dmtcp/","title":"DMTCP","text":""},{"location":"Software/utilities/dmtcp/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command DMTCP 4.0.0 foss/2025a <code>module load foss/2025a DMTCP/4.0.0</code>"},{"location":"Software/utilities/dmtcp/#related-applications","title":"Related Applications","text":""},{"location":"Software/utilities/dmtcp/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/utilities/parallel/","title":"GNU Parallel","text":"<p>GNU Parallel is a shell tool for executing jobs in parallel using one or more computers. A job can be a single command or a small script that has to be run for each of the lines in the input. The typical input is a list of files, a list of hosts, a list of users, a list of URLs, or a list of tables. A job can also be a command that reads from a pipe. GNU parallel can then split the input and pipe it into commands in parallel.</p> <p>Tip</p> <p>You can use GNU Parallel on your Mac OSX laptop/desktop to run applications in parallel. Just install using either brew or Macports</p> <pre><code>sudo port install parallel\nbrew install parallel\n</code></pre>"},{"location":"Software/utilities/parallel/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command parallel 20240722 foss/2024a <code>module load foss/2024a parallel/20240722</code>"},{"location":"Software/utilities/parallel/#usage","title":"Usage","text":"Usage <pre><code>module load parallel/20200422\nparallel echo ::: 1 2 3 ::: 4 5 6\n</code></pre>"},{"location":"Software/utilities/parallel/#purpose","title":"Purpose","text":"<p>GNU Parallel is a tool for running a series of jobs, mostly serial jobs, in parallel. This tool is best suited for running a bunch of serial jobs that may not run for the same simulation time. For example, the most easiest way to run a series of serial jobs in parallel on n cpus within one submit script is as follows</p> <pre><code>./job_1 &amp;\n./job_2 &amp;\n...\n./job_n &amp;\nwait\n./job_(n+1) &amp;\n./job_(n+2) &amp;\n...\n./job_2n &amp;\nwait\n./job_(2n+1) &amp;\n./job_(2n+2) &amp;\n...\n./job_3n &amp;\nwait\n</code></pre> <p>This works most efficiently when all jobs have the same or almost the same run time. If run times for jobs are unequal, then n jobs are run simultaneously and the cpus remain idle until all n jobs are completed before looping through the next n jobs. This will lead to idle time and inefficient consumption of cpu time.</p> <p></p> <p>GNU Parallel solves this issue by first launching n jobs. When one job completes, then the next job in sequence is started. This permits efficient use of cpu time by reducing the wait time and letting a number of small jobs to run while some cpus work on longer jobs.</p> <pre><code>parallel job_{1} ::: $(seq 1 3n)\n</code></pre> <p></p>"},{"location":"Software/utilities/parallel/#single-node-example","title":"Single Node example","text":""},{"location":"Software/utilities/parallel/#multi-node-example","title":"Multi Node example","text":""},{"location":"Software/utilities/parallel/#related-links","title":"Related Links","text":"<ul> <li>https://www.gnu.org/software/parallel/parallel_tutorial.html</li> <li>https://www.biostars.org/p/63816/</li> <li>http://www.shakthimaan.com/posts/2014/11/27/gnu-parallel/news.html</li> <li>https://www.msi.umn.edu/support/faq/how-can-i-use-gnu-parallel-run-lot-commands-parallel</li> <li>https://github.com/LangilleLab/microbiome_helper/wiki/Quick-Introduction-to-GNU-Parallel</li> <li>https://davetang.org/muse/2013/11/18/using-gnu-parallel/</li> <li>https://sites.google.com/a/stanford.edu/rcpedia/parallel-processing/gnu-parallel-examples</li> <li>http://phili.pe/posts/free-concurrency-with-gnu-parallel/</li> </ul>"},{"location":"Software/utilities/parallel/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/ase/","title":"ASE","text":"<p>ASE (Atomic Simulation Environment) is an open-source Python library for performing atomic-scale simulations of materials. It provides a collection of tools and interfaces for setting up, running, and analyzing simulations of atoms, molecules, and solids using a variety of simulation packages.</p> <p>ASE is designed to be flexible and modular, allowing users to easily switch between different simulation packages and methods without having to modify their code. It currently supports a number of popular simulation packages, including Quantum ESPRESSO, LAMMPS, and GROMACS etc.</p> <p>ASE provides a wide range of functionality for working with atomic-scale simulations, including geometry optimization, molecular dynamics simulations, electronic structure calculations, and vibrational analysis. It also includes a number of built-in tools for analyzing simulation output, such as calculating radial distribution functions, computing surface area and volume, and visualizing simulation trajectories.</p> <p>ASE is actively developed and maintained by a community of researchers and developers from around the world, and is available as a free download under an open-source license.</p>"},{"location":"Software/visualization/ase/#availability","title":"Availability","text":"Wulver <p> Software Version Dependent Toolchain Module Load Command ASE 3.26.0 foss/2025a <code>module load foss/2025a ASE/3.26.0</code>  ```</p>"},{"location":"Software/visualization/ase/#related-applications","title":"Related Applications","text":""},{"location":"Software/visualization/ase/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/gnuplot/","title":"GNUPlot","text":""},{"location":"Software/visualization/gnuplot/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command gnuplot 6.0.1 foss/2024a <code>module load foss/2024a gnuplot/6.0.1</code>"},{"location":"Software/visualization/gnuplot/#related-applications","title":"Related Applications","text":""},{"location":"Software/visualization/gnuplot/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/ovito/","title":"OVITO","text":"<p>OVITO is a scientific visualization and analysis software tool designed for the analysis of atomistic simulation data. It is used primarily in the field of materials science and computational chemistry, and is particularly well-suited for the analysis of molecular dynamics simulations and other atomistic simulation techniques.</p> <p>OVITO provides a graphical user interface (GUI) that allows users to interactively visualize and analyze large datasets of atomistic simulation data. It supports a wide range of file formats commonly used in molecular dynamics simulations, including LAMMPS, GROMACS, and DL_POLY.</p> <p>In addition to its visualization capabilities, OVITO also provides a set of built-in analysis tools that allow users to compute various properties of the simulated systems. These include calculation of radial distribution functions, Voronoi tessellation, and common structural analysis metrics like bond order parameters and coordination numbers.</p>"},{"location":"Software/visualization/ovito/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command OVITO 3.12.3-basic foss/2024a <code>module load foss/2024a OVITO/3.12.3-basic</code>"},{"location":"Software/visualization/ovito/#related-applications","title":"Related Applications","text":""},{"location":"Software/visualization/ovito/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"Software/visualization/paraview/","title":"Paraview","text":"<p>ParaView is an open-source, cross-platform data visualization and analysis tool that allows users to create visualizations and analyze large datasets. It was developed to process and visualize scientific and engineering data, such as computational fluid dynamics (CFD) simulations, seismic data, medical imaging, and climate data.</p> <p>ParaView provides a graphical user interface (GUI) that enables users to interactively explore and visualize data. It supports a wide range of data formats and allows users to customize and manipulate visualizations to suit their needs. Additionally, ParaView provides a scripting interface that allows users to automate repetitive tasks and create custom analysis pipelines.</p> <p>ParaView is widely used in a variety of scientific and engineering fields, including aerospace, automotive, biomedical, energy, and geosciences, among others. It is actively developed and maintained by Kitware, a software company that specializes in open-source solutions for scientific computing and data analysis.</p>"},{"location":"Software/visualization/paraview/#availability","title":"Availability","text":"Wulver Software Version Dependent Toolchain Module Load Command ParaView 5.11.2-egl - <code>module load ParaView/5.11.2-egl</code> ParaView 5.11.2-osmesa - <code>module load ParaView/5.11.2-osmesa</code> ParaView 5.13.2 foss/2024a <code>module load foss/2024a ParaView/5.13.2</code>"},{"location":"Software/visualization/paraview/#application-information-documentation","title":"Application Information, Documentation","text":"<p>The documentation of ParaView is available at ParaView manual. To use ParaView on the cluster, users need to use the same version of ParaView on their local machine. You can download the ParaView from ParaView official download page</p>"},{"location":"Software/visualization/paraview/#using-paraview","title":"Using ParaView","text":"<p>ParaView supports GPU acceleration, which can significantly improve performance and reduce processing times for certain types of data and operations. GPU acceleration is particularly useful for large datasets with many points or cells, as well as for operations such as volume rendering and streamlines. You can use ParaView with GPU acceleration, but you need to use GPU nodes on our cluster. ParaView is also designed to work in parallel environments, and it supports the Message Passing Interface (MPI) standard for distributed computing. With MPI support, ParaView can be used to visualize and analyze large-scale datasets on clusters. </p> Sample Batch Script to Run ParaView with MPI support: pvserver_cpu.submit.sh <pre><code>#!/bin/bash -l\n#SBATCH --job-name=pvserver_cpu\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load the slurm, easybuild \nmodule load ParaView/5.11.2-osmesa \n################################################\n#\n# Open an ssh tunnel to the login node\n#\n################################################\n# Run on random port\nport=$(shuf -i 6000-9999 -n 1)\nHOST=$(hostname)\nif [ $(hostname) == $HOST ]; then\n  /usr/bin/ssh -N -f -R $port:localhost:$port login02.tartan.njit.edu\nfi\n################################################\ncat&lt;&lt;EOF\n\npvserver is running on: $(hostname)\nJob starts at: $(date)\n\nStep 1: Create SSH tunnel\n\nOpen new terminal window, and run:\n(If you are off campus you will need VPN running)\n\nssh -L $port:localhost:$port $USER@login02.tartan.njit.edu\nEOF\n################################################\n# Run MPI pvserver\n#\n################################################\nhost_list=$(srun hostname -s | sort | uniq | paste -s -d, -)\nmpiexec -np $SLURM_NTASKS -rmk slurm -hosts $host_list pvserver --server-port=$port --force-offscreen-rendering\n</code></pre> <p>To use ParaView with GPU, you need to use the following job script</p> Sample Batch Script to Run ParaView with GPU support: pvserver_gpu.submit.sh <pre><code>#!/bin/bash -l\n#SBATCH --job-name=pvserver_gpu\n#SBATCH --output=%x.%j.out # %x.%j expands to slurm JobName.JobID\n#SBATCH --error=%x.%j.err # prints the error message\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=16\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=4000M # Maximum allowable mempry per CPU 4G\n#SBATCH --qos=standard\n#SBATCH --account=PI_ucid # Replace PI_ucid which the NJIT UCID of PI\n#SBATCH --time=71:59:59  # D-HH:MM:SS\n################################################\n#\n# Purge and load modules needed for run\n#\n################################################\nmodule purge\nmodule load wulver # Load slurm, easybuild\nmodule load ParaView/5.11.2-egl\n################################################\n#\n# Open an ssh tunnel to the login node\n#\n################################################\nport=$(shuf -i 6000-9999 -n 1)\nHOST=$(hostname)\nif [ $(hostname) == $HOST ]; then\n     /usr/bin/ssh -N -f -R $port:localhost:$port login02\nfi\n################################################\ncat&lt;&lt;EOF\n\npvserver is running on: $(hostname)\nJob starts at: $(date)\n\nStep 1: Create SSH tunnel\n\nOpen a new terminal window on your local machine, and run:\n(If you are off campus you will need a VPN running)\n\nssh -L $port:localhost:$port $USER@login02.tartan.njit.edu\nEOF\n################################################\n#\n# Run MPI pvserver\n#\n################################################\n\nhost_list=$(srun hostname -s | sort | uniq | paste -s -d, -)\n\nmpiexec -np $SLURM_NTASKS -rmk slurm -hosts $host_list pvserver --server-port=$port --force-offscreen-rendering\n</code></pre> <p>Submit the job script using the sbatch command: <code>sbatch pvserver_gpu.submit.sh</code> or <code>sbatch pvserver_cpu.submit.sh</code>. Once you submit the job, please open the output file with <code>.out</code> extension, and get the port number from the output file. Once you open the output file (with <code>.out</code> extension) and go to the end of the file, you should see the following </p> <pre><code>Step 1: Create SSH tunnel\n\nOpen new terminal window, and run:\n(If you are off campus you will need VPN running)\n\nssh -L 1234:localhost:1234 user@login01.tartan.njit.edu\nWaiting for client...\nConnection URL: cs://n0003:1234\nAccepting connection(s): n0003:1234\n</code></pre> <p>Next, open a new terminal and type</p> <p><code>ssh -L $port:localhost:$port $USER@login02.tartan.njit.edu</code></p> <p>where <code>$port</code> corresponds to the port number. Once you open ParaView from your local machine go to <code>File --&gt; Connnect</code>, and you will see a dialogue box with the name <code>Choose Server Configuration</code>. You need to select Add Server option and there you need to use the following as shown below.</p>    Your browser does not support the video tag.  <p>Make sure to use the same port number in Port option. Once you add the server, you need to select Connect to connect ParaView to the cluster.</p> <p>Note</p> <p>The port number may change every time you submit the job. In that case, you need to modify the port number by selecting the Edit Server option. The step to modify the server is shown in the above tutorial.</p>"},{"location":"Software/visualization/paraview/#related-applications","title":"Related Applications","text":"<ul> <li>Tecplot</li> </ul>"},{"location":"Software/visualization/paraview/#user-contributed-information","title":"User Contributed Information","text":"<p>Please help us improve this page</p> <p>Users are invited to contribute helpful information and corrections through our Github repository.</p>"},{"location":"about/","title":"ARCS Leadership","text":"Name Title Phone Email Office Alex Pacheco Associate CIO - Advanced Research Computing (973) 596-2672 alex.pacheco@njit.edu GITC 5401 Kate Cahill Director, High Performance Research Computing (973) 596-2721 katharine.cahill@njit.edu GITC 2203 Kevin Walsh Assistant Director, Advanced Computing Infrastructure (973) 596-5747 walsh@njit.edu GITC 2202"},{"location":"about/#high-performance-research-computing","title":"High Performance Research Computing","text":"Name Title Phone Email Office Kate Cahill Director, High Performance Research Computing (973) 596-2721 katharine.cahill@njit.edu GITC 2203 Kevin Walsh Assistant Director, Advanced Computing Infrastructure (973) 596-5747 walsh@njit.edu GITC 2202 Hui(Julia) Zhao Research Computing Facilitator (973) 596-2727 hui.zhao@njit.edu GITC 2316B Abhishek Mukherjee Research Computing Facilitator (973) 642-4132 abhishek.mukherjee@njit.edu GITC 2316B"},{"location":"about/#research-and-academic-technologies","title":"Research and Academic Technologies","text":"Name Title Phone Email Office Rick Gaine Research Technology System Administrator (973) 596-5441 rgaine@njit.edu GITC 2404 Charles Yan Research Technology Support Specialist (973) 596-2907 cyan@njit.edu GITC 2301 <ul> <li>"},{"location":"about/#associate-cio-advanced-research-computing","title":"Associate CIO, Advanced Research Computing","text":"<p> Alex Pacheco <p>Alex Pacheco joined NJIT on August 2, 2022, as the Associate CIO for Advanced Research Computing. He brings over twenty years of combined experience as an HPC user, user support consultant, and manager of HPC resources. At Lehigh University, he helped develop a strategic vision for a shared HPC resource and served as a co-PI on an NSF Campus Cyberinfrastructure award.  </p> <p>-</p> <ul> <li>"},{"location":"about/#director-high-performance-research-computing","title":"Director, High Performance Research Computing","text":"<p> Kate Cahill <p>Kate started at NJIT in September 2023. Previously, she was the Education &amp; Training Specialist at the Ohio Supercomputer Center for 8 years, where she led the training programs for OSC as well as external education programs related to HPC and computational science for the XSEDE project as well as other grant-funded efforts.</p>"},{"location":"about/#assistant-director-advanced-computing-infrastructure","title":"Assistant Director, Advanced Computing Infrastructure","text":"<p>Kevin Walsh is the Assistant Director of Research Computing Infrastructure at New Jersey Institute of Technology. With an extensive experience in managing high-performance computing environments, Kevin leads the design, deployment, and support of computational services crucial to cutting-edge research. Known for his collaborative approach, he works closely with faculty, IT teams, and external partners to ensure reliable, scalable infrastructure that accelerates discovery and innovation. </p>"},{"location":"about/#hpc-consultation","title":"HPC consultation","text":"<ul> <li>"},{"location":"about/#research-computing-facilitator","title":"Research Computing Facilitator","text":"<p> Hui(Julia) Zhao <p>Hui(Julia) joined NJIT in December 2023, following twenty years in a similar role at Memorial Sloan Kettering Cancer Center. Julia's experience includes roles as a Bioinformatician, Unix System Administrator and High Performance Computing Engineer, with expertise in system and application configuration and user support. She helps user apply high-performance computing techniques to address diverse challenges in scientific computing.</p> <li>"},{"location":"about/#research-computing-facilitator_1","title":"Research Computing Facilitator","text":"<p> Abhishek Mukherjee <p>Abhishek Mukherjee is a computational scientist and has experience in multidisciplinary projects, providing engineering solutions for computational fluid dynamics problems. He has expertise in HPC software installation and is an active contributor to EasyBuild that allows to manage software on HPC systems in an efficient way.  </p> <p>You can schedule appointments with Abhishek from this link to consult on problems or questions you are encountering related to your work using the high-performance computing and big data resources at NJIT. Please, before making appointments with Abhishek Mukherjee, send your query to hpc@njit.edu, so that an incident number will be created which is required to schedule an appointment. </p>"},{"location":"about/#hpc-student-interns","title":"HPC Student Interns","text":"<ul> <li>"},{"location":"about/#_1","title":"ARCS Leadership","text":"<p> Lakshya Saharan <p>Lakshya Saharan began his Master\u2019s in Computer Science and joined NJIT\u2019s High Performance Computing (HPC) department as a student intern on September 24. He provides assistance to researchers and students with effective utilization of the cluster, contributes to essential technical documentation, and supports ongoing operational tasks to ensure the cluster's smooth and efficient performance. </p> <li>"},{"location":"about/#_2","title":"ARCS Leadership","text":"<p> Aakash Singh <p>Aakash Singh is a CKA certified DevOps and SRE professional with extensive experience in cloud infrastructure optimization (AWS, Azure) and Kubernetes administration. He has a strong record of enhancing system reliability and achieving significant operational cost reductions.. Aakash excels in designing and managing high-availability, scalable systems, including GPU-accelerated infrastructure for AI/ML workloads and MLOps. His expertise encompasses CI/CD automation, platform engineering, and performance benchmarking of GPUs (e.g., A100s). With deep Linux knowledge, Aakash currently works as an HPC Support Specialist, focusing on AI/ML platforms and contributing to the efficiency of distributed systems. He is keen to leverage his skills in cutting-edge AI and HPC environments. </p>"},{"location":"about/#former-staff-and-students","title":"Former staff and students","text":""},{"location":"about/#gedaliah-wolosh","title":"Gedaliah Wolosh","text":"<p>Dr. Wolosh recently retired after  25 years at NJIT  working in research computing. He was the lead architect for all the HPC resources at NJIT. He specialized in building scientific software stacks.  </p>"},{"location":"about/contact/","title":"Contact Us","text":"<p>Requests for assistance with HPC resources have been integrated into the campus-wide ServiceNow system. To have a service ticket created automatically and assigned to the HPC staff, email hpc@njit.edu. Before requesting assistance, we encourage you to take a look at the relevant documentation on this site. </p> <p>For prompt response, please consider the following when submitting a ticket. </p> <ul> <li> <p>Email Subject</p> <ul> <li>Very short summary of the issue as a subject, examples:<ul> <li>Lochness: Unable to submit jobs to public partition</li> <li>Lochness: Job error - no space on disk</li> <li>Lochness: Jobs are getting killed</li> </ul> </li> </ul> </li> <li> <p>Email Content</p> <ul> <li>JOB NUMBER</li> <li>System, Software and Partition where you are having an issue.</li> <li>Detailed description of your issue including modules loaded, and location of input, output and error files.</li> <li>If you have an error to report, paste the error message. Do not take a snapshot and upload the image.</li> <li>An image is not helpful when a long string of characters including directory path or file names needs to be copied.</li> <li>Anything else that you think may be necessary for us to understand and find a resolution to your issue.</li> </ul> </li> <li> <p>Software requests</p> <ul> <li>Submit a request for HPC Software Installation by visiting the Service Catalog </li> <li>Note: Users should consider installing software in their home directory, especially Python and R packages. See Building your own for instructions on using conda or mamba. If you encounter issues, consider asking for help above rather than requesting a software install.</li> <li>You will need to enter the following information<ul> <li>Name of software and URL to download.</li> <li>If software is behind a paywall or account, please download the software, copy it over to lochness and provide location from where we can copy it for installation.</li> </ul> </li> </ul> </li> <li> <p>Account requests</p> <ul> <li>Requests for an account must come from faculty advisors by sending an email to hpc@njit.edu including the UCIDs for the accounts to be created.</li> <li>Students making requests should add their faculty advisor to the request. Requests will only be approved after confirmation by faculty advisor.</li> <li>To use HPC in courses, the course instructor should submit a request and include UCID's for the accounts to be created.</li> <li>Guest Accounts: Faculty can use Service Now to request Guest Access on HPC resources only for their external collaborators for a period of 1 year (can be renewed annually). </li> </ul> </li> <li> <p>Storage Requests</p> <ul> <li>Requests for storage must come from faculty advisors by sending an email to hpc@njit.edu. The request should include the amount of storage needed.</li> </ul> </li> <li> <p>Consultations</p> <ul> <li>During the migration, HPC leadership will host office hours for faculty and PIs. These can be virtual or in person at GITC 2200. Please sign up here.</li> <li> <p>We also provide one-on-one consultations for HPC related issues. Schedule an appointment directly from below.</p> <p></p> </li> <li> <p>Please note that, when requesting assistance provide the information described above (where applicable), so we can most effectively assist you. Before requesting one-on-one consultations, please send you query to hpc@njit.edu, so that an incident number will be created which is required to schedule an appointment. </p> </li> </ul> </li> </ul>"},{"location":"about/research_advisory/","title":"Research Computing Advisory Board","text":""},{"location":"about/research_advisory/#purpose","title":"Purpose","text":"<p>The purpose of the Research Computing Advisory Board (RCAB) is to advise on the operation of Research IT resources at NJIT as a partnership between IST, Office of Research and NJIT Faculty. The committee will ensure that the use of Research IT resources, primarily HPC, aligns with the university\u2019s research priorities and promotes the advancement of knowledge across diverse disciplines.</p>"},{"location":"about/research_advisory/#advisory-board-charge","title":"Advisory Board Charge","text":""},{"location":"about/research_advisory/#advise-on-resource-management","title":"Advise on Resource Management","text":"<p>Provide recommendations on the management and enhancement of Research IT resources to support current and future research needs. Resources will include both hardware and software, and physical facilities such as colocation data centers to support the needs of faculty.</p>"},{"location":"about/research_advisory/#develop-policies-for-fair-distribution-of-hpc-resources","title":"Develop policies for fair distribution of HPC resources","text":"<ol> <li>Develop Allocation Policies: Establish clear and transparent policies for the submission, review, and approval of HPC resource requests.</li> <li>Review and Approve Allocation Proposals: Review and evaluate proposals for HPC resource allocation by NJIT faculty based on scientific merit, potential impact, and alignment with university research goals.</li> <li>Monitor Resource Usage: Track the utilization of HPC resources to ensure efficient use and identify areas for improvement.</li> </ol>"},{"location":"about/research_advisory/#promote-interdisciplinary-collaboration","title":"Promote Interdisciplinary Collaboration","text":"<p>Encourage and facilitate interdisciplinary research projects that leverage HPC resources.</p>"},{"location":"about/research_advisory/#enhance-research-cyberinfrastructure","title":"Enhance Research Cyberinfrastructure","text":"<p>Lead NJIT faculty and coordinate with IST and OVPR in submitting cyberinfrastructure grants to meet the future research needs. </p>"},{"location":"about/research_advisory/#outreach-and-education","title":"Outreach and Education","text":"<p>Organize workshops, seminars, and training sessions to educate the university community about HPC resources and best practices for their use.</p>"},{"location":"about/research_advisory/#meeting-frequency","title":"Meeting Frequency","text":"<p>The committee will meet quarterly or as needed to submit proposals to outside agencies, review allocation proposals and discuss matters related to resource management.</p>"},{"location":"about/research_advisory/#reporting","title":"Reporting","text":"<p>The RCAB will report to the Faculty Senate and provide annual summaries of resource usage, and policy updates.</p>"},{"location":"about/research_advisory/members/","title":"Members","text":"<p>The RCAB shall be composed of faculty members representing a cross-section of disciplines that utilize ARCS resources. Members will be appointed by the Deans of the five colleges and serve a term of two years, with the possibility of reappointment. Additionally, the  committee will comprise representatives from the Office of Vice Provost for Research (Associate Vice Provost for Research Collaboration and Partnerships) and the Information Services and Technology (Associate CIO - Advanced Research Computing). The committee will be co-chaired by the IST representative and a faculty member elected by the committee. </p> Name Title Email Alex Pacheco (co-chair) Associate CIO - Advanced Research Computing (IST) alex.pacheco@njit.edu Mike Siegel (co-chair) Professor (Math/HCSLA) michael.s.siegel@njit.edu Shawn Chester Associate Vice Provost Research Collaboration and Partnership (OVPR) shawn.a.chester@njit.edu Simon Garnier Professor (Biological Sciences/HCSLA) simon.j.garnier@njit.edu Dibakar Datta Associate Professor (MIE/NCE) dibakar.datta@njit.edu Gennady Gor Associate Professor (CME/NCE) gor@njit.edu Andrew Sohn Associate Professor (CS/YWCC) andrew.sohn@njit.edu Zhi Wei Professor (CS/YWCC) zhi.wei@njit.edu Dantong Yu Professor (MTSM) dantong.yu@njit.edu Mathew Schwartz Associate Professor (HCAD) mathew.schwartz@njit.edu Andrzej Zarzycki Associate Professor (HCAD) andrzej.zarzycki@njit.edu"},{"location":"archived/lochness_policies/","title":"Lochness User Policies","text":"<p>Access and usages policies for the lochness cluster</p> <ul> <li> <p>Access</p> <ul> <li>Faculty members can request access to lochness for themselves and their students by sending an email to hpc@njit.edu.  There is no charge for using lochness.</li> <li>For courses, send an email to hpc@njit.edu to discuss requirements</li> </ul> </li> <li> <p>Storage</p> <ul> <li>Users are given a quota of 100GB</li> <li>Shared research directory with 500 GB quota is available upon request</li> </ul> </li> <li> <p>Scheduling</p> <ul> <li>The SLURM scheduler uses a \u201cfair share\u201d algorithm to assign priority to jobs. The \u201cfair share\u201d algorithm takes into account factors such as user history, wall time request, etc \u2026 to assign priority. The \u201cfair share\u201d algorithm is configure to favor smaller jobs. </li> <li>Limits<ul> <li>Wall time: 30 Days. Note that shorter wall time receive higher priority.</li> </ul> </li> <li>Cores/RAM: No limit but we request users be considerate. There are limited resources for public access.</li> </ul> </li> <li> <p>Condo</p> <ul> <li>Lochness has a private pool ownership model. Users buy nodes which are then incorporated into the cluster as a private partition.</li> </ul> </li> </ul> <p>Warning</p> <p>As of Jan 16, no new Lochness accounts will be created and Lochness will be decommissioned once the Wulver migration is complete.</p>"},{"location":"archived/migration/","title":"Important Announcement: HPC Cluster Migration from Lochness to Wulver","text":"<p>We are excited to share important news about the upcoming migration of users from the old HPC cluster, Lochness, to the new and improved cluster, Wulver. This migration is scheduled to commence on 1/16/2024.</p>"},{"location":"archived/migration/#migration-details","title":"Migration Details:","text":"<ul> <li>Start Date: January 16, 2024</li> <li>Priority: Migration will be carried out in PI groups, with PIs who own nodes in Lochness being migrated first.</li> <li>Communication: PIs will receive prior communication from our team to discuss specific details tailored to their groups.</li> <li>Lochness Complete Shutdown: At the conclusion of the migration, Lochness will undergo a complete shutdown to facilitate its merger with Wulver. You will be advised of any necessary preparations needed on your end for a seamless transition.</li> <li>Restrictions on Lochness: As of the migration start date, no new user accounts or software installations will be permitted on Lochness. We appreciate your cooperation in adhering to these restrictions to ensure a smooth migration process.</li> <li>HPC Usage in Spring Courses: For coursework during the Spring semester, HPC usage will be exclusively on Wulver. Faculty members planning to integrate Wulver into their Spring courses are encouraged to contact us for testing and any necessary support. This proactive approach will help ensure a successful experience for both faculty and students.</li> </ul>"},{"location":"archived/migration/#action-required","title":"Action Required:","text":"<p>PIs: Expect communication from our team before the migration date, providing essential details and discussing the migration plan tailored to your group's needs. We will need a list of your current students and software in use by your group.</p>"},{"location":"archived/migration/#important-note-regarding-documentation","title":"Important Note Regarding Documentation:","text":"<p>To facilitate a smooth transition, please refer to FAQs, and other resources specific to the migration process.</p>"},{"location":"archived/migration/#benefits-of-migration","title":"Benefits of Migration:","text":"<p>Wulver is equipped with enhanced capabilities and improved performance, ensuring a more efficient and streamlined high-performance computing experience for all users.</p> <p>If you have immediate concerns or questions, please feel free to reach out to the group via Contact US.</p>"},{"location":"archived/migration/Lochness_node_owners/","title":"Lochness Node Owners Information","text":""},{"location":"archived/migration/Lochness_node_owners/#overview","title":"Overview","text":"<p>As a node owner on the Lochness high-performance computing (HPC) cluster, we would like to provide you with important information regarding the upcoming migration to the new cluster, Wulver.</p>"},{"location":"archived/migration/Lochness_node_owners/#node-migration-to-wulver","title":"Node Migration to Wulver","text":""},{"location":"archived/migration/Lochness_node_owners/#privately-owned-nodes","title":"Privately Owned Nodes","text":"<p>Most privately owned nodes on Lochness will be migrated to Wulver as part of the cluster upgrade. Owners of these nodes will be considered \"condo investors\" and will receive higher priority access on an equivalent amount of resources for a period of time to be determined. This ensures that your investment in HPC resources continues to be recognized and prioritized during the transition.</p>"},{"location":"archived/migration/Lochness_node_owners/#decommissioning-of-out-of-warranty-nodes","title":"Decommissioning of Out-of-Warranty Nodes","text":"<p>Nodes that are out of warranty and deemed too old to become part of Wulver's infrastructure will be decommissioned. We will contact owners of such nodes for further discussions and potential considerations.</p>"},{"location":"archived/migration/Lochness_node_owners/#node-exclusions-from-wulver-migration","title":"Node Exclusions from Wulver Migration","text":""},{"location":"archived/migration/Lochness_node_owners/#nodes-with-consumer-grade-gpus","title":"Nodes with Consumer Grade GPUs","text":"<p>Nodes equipped with consumer-grade GPUs, such as Titan, RTX, etc., will not be migrated to Wulver. Instead, these nodes will be relocated to the GITC 4320 datacenter and will be individually managed by ARCS.</p> <p>This decision is made to ensure the optimal performance and compatibility of the Wulver cluster while providing an alternative solution for nodes with specialized hardware configurations.</p>"},{"location":"archived/migration/Lochness_node_owners/#action-required","title":"Action Required","text":"<ol> <li>Privately Owned Node Owners: Await further communication regarding the migration process and priority access details.</li> <li>Owners of Out-of-Warranty Nodes: Await further communication for discussions on potential considerations and decommissioning.</li> <li>Owners of Nodes with Consumer Grade GPUs: Expect your nodes to be relocated to the GITC 4320 datacenter, managed individually by ARCS.</li> </ol>"},{"location":"archived/migration/Lochness_node_owners/#timeline","title":"Timeline","text":"<p>The migration timeline and the duration of higher priority access for condo investors will be communicated to you in advance.</p>"},{"location":"archived/migration/faqs/","title":"Faqs","text":""},{"location":"archived/migration/faqs/#the-following-are-questions-we-have-received-over-the-last-several-weeks-regarding-the-migration-from-lochness-to-wulver","title":"The following are questions we have received over the last several weeks regarding the migration from Lochness to Wulver.","text":""},{"location":"archived/migration/faqs/#what-is-the-timeline-for-the-migration-from-lochness-to-wulver","title":"What is the timeline for the migration from Lochness to Wulver?","text":"Answer <p>We anticipate the migration process to be complete by the end of February 2024. The migration will commence on January 16, 2024, and our team is dedicated to ensuring a smooth transition for all users. We will keep you informed about any updates or changes to the timeline as the migration progresses. Your cooperation and understanding during this period are greatly appreciated. If you have any specific concerns about the timeline, please feel free to reach out to our support team for further clarification.</p>"},{"location":"archived/migration/faqs/#how-will-the-migration-impact-my-existing-workflows-and-computations","title":"How will the migration impact my existing workflows and computations?","text":"Answer <p>The research facilitation team is committed to assisting you during the migration process. Our team will work closely with you to ensure that your existing workflows and computations are seamlessly transferred to Wulver. We understand the importance of minimizing disruptions to your research activities, and our experts will provide guidance and support to address any compatibility issues that may arise. You can expect personalized assistance to make the transition as smooth as possible. If you have specific concerns about your workflows, please don't hesitate to reach out to our team for tailored support.</p>"},{"location":"archived/migration/faqs/#what-specific-steps-should-i-take-to-prepare-for-the-migration","title":"What specific steps should I take to prepare for the migration?","text":"Answer <p>To prepare for the migration, there are a few crucial steps:</p> <ul> <li> <p>Provide a List of Current Students and Postdocs: </p> <ul> <li>Please share with us an updated list of current students, postdocs, and external collaborators who are actively using the HPC resources on Lochness. This information will ensure that user accounts are accurately migrated to Wulver, and access is maintained for the relevant individuals.</li> </ul> </li> <li> <p>List of Required Software: </p> <ul> <li>Compile a list of software applications that are essential for your research. This includes both commonly used software and any specialized tools unique to your work. Knowing your software requirements enables us to ensure that the necessary applications are available and properly configured on Wulver. You can find the list of software applications installed on Wulver in Software. If you don't find your applications in the list, submit a request for HPC Software Installation by visiting the Service Catalog.</li> </ul> </li> <li> <p>Planning for Former Students: </p> <ul> <li>If you have former students who may still have data or files on Lochness, it's essential to plan for their data migration or archival. We recommend reaching out to former students to coordinate any necessary data transfers or backups to avoid potential data loss.</li> </ul> </li> </ul> <p>These steps will contribute to a successful migration, allowing us to tailor the process to your specific needs. If you have any questions or need assistance with these preparations, please contact our support team.</p>"},{"location":"archived/migration/faqs/#will-there-be-any-downtime-during-the-migration-and-how-will-it-affect-my-research-activities","title":"Will there be any downtime during the migration, and how will it affect my research activities?","text":"Answer <p>Once the migration has started for your group, Lochness will be inaccessible. If access to Lochness is critical for specific tasks during this period, we strongly encourage you to reach out to us. We understand that some users may have time-sensitive activities or dependencies on Lochness, and we are committed to working with you to find solutions that meet your needs. Please contact our support team by sending an email to hpc@njit.edu to discuss your specific requirements, and we will do our best to accommodate your situation during the migration process. You can check Contact Us to see how to create tickets with us and what information is required to create tickets. </p>"},{"location":"archived/migration/faqs/#is-there-a-plan-for-backing-up-and-restoring-data-during-the-migration-process","title":"Is there a plan for backing up and restoring data during the migration process?","text":"Answer <p>Yes, there is a plan in place for data continuity. The data on Lochness resides on a shared filesystem, and these same filesystems will be mounted and available on Wulver post-migration. This approach ensures that your data remains accessible and seamlessly transfers to the new cluster.</p> <p>There is no need for a separate backup and restoration process as the shared filesystem continuity facilitates a smooth transition. If you have specific data-related concerns or requirements, please feel free to reach out to our support team (Contact Us) for further clarification and assistance. Your data integrity and accessibility are our top priorities throughout the migration process.</p>"},{"location":"archived/migration/faqs/#what-changes-will-occur-in-the-file-directory-structure-especially-regarding-the-research-and-home-directories","title":"What changes will occur in the file directory structure, especially regarding the <code>/research</code> and <code>/home</code> directories?","text":"Answer <p>With the migration to Wulver, there will be changes in the file directory structure:</p> <p>Filesystems on Wulver: Wulver will have three filesystems available for use: <code>/home</code>, <code>/project</code>, and <code>/scratch</code>.</p> <ul> <li> <p>Availability of <code>/research</code> Directory: The <code>/research</code> directory from Lochness will be mounted on Wulver and will be available for use. This ensures continuity for research-related files and data.</p> </li> <li> <p>Lochness <code>/home</code> Directory on Wulver: The Lochness <code>/home</code> directory will be mounted on the Wulver login node as <code>/oldhome</code>. Users will be able to read files in this directory and move files from this directory but will not be able to write files into this deirectory. This allows users to access their personal home directories from Lochness during the migration period. The Lochness files under <code>/oldhome</code> will be available until 1-Jan-2025. Users should move needed files into <code>/project/PI_UCID</code> directory (replace PI_UCID with the UCID of PI). </p> </li> </ul> <p>These changes are designed to optimize the file organization on Wulver while maintaining accessibility to critical research data. The research facilitation team will work closely with users to ensure a smooth transition of data and assist in adapting to the new file directory structure. If you have specific questions or require assistance with data migration, please don't hesitate to contact our support team.</p>"},{"location":"archived/migration/faqs/#how-will-the-migration-impact-access-to-specialized-software-or-applications-that-i-currently-use-on-lochness","title":"How will the migration impact access to specialized software or applications that I currently use on Lochness?","text":"Answer <p>The research facilitation team is dedicated to ensuring a smooth transition for users in terms of software and applications:</p> <ul> <li>Installation Support: <ul> <li>The research facilitation team will handle the installation of necessary software on Wulver, ensuring that essential tools and applications are available for your research. You can request for HPC Software Installation by visiting the Service Catalog. Please visit Software to see the list of applications isntalled on Wulver.</li> </ul> </li> <li>Code Compilation Assistance: <ul> <li>If your research involves custom code that needs compilation, the research facilitation team will provide assistance to ensure a successful compilation on Wulver. This support extends to helping users adapt their code to the new environment.</li> </ul> </li> </ul> <p>Our goal is to minimize any disruptions to your research activities and provide the necessary support for a seamless transition. If you have specific software requirements or need assistance with code compilation, please reach out to the research facilitation team, and they will be happy to assist you.</p>"},{"location":"archived/migration/faqs/#can-i-continue-using-lochness-for-specific-tasks-during-the-migration-or-will-it-be-completely-inaccessible","title":"Can I continue using Lochness for specific tasks during the migration, or will it be completely inaccessible?","text":"Answer <p>Once the migration has started for your group, Lochness will be inaccessible. If access to Lochness is critical for specific tasks during this period, we strongly encourage you to reach out to us. We understand that some users may have time-sensitive activities or dependencies on Lochness, and we are committed to working with you to find solutions that meet your needs. Please contact our support team to discuss your specific requirements, and we will do our best to accommodate your situation during the migration process.</p>"},{"location":"archived/migration/faqs/#what-happens-to-my-existing-job-submissions-and-queued-jobs-during-the-migration-process","title":"What happens to my existing job submissions and queued jobs during the migration process?","text":"Answer <p>For the most part, the migration will start after all running jobs have been completed. We understand the importance of job completion for ongoing research activities. Accommodations will be made for long-running jobs that cannot be checkpointed.   Our aim is to minimize disruptions to your computational tasks and ensure a smooth transition. If you have specific concerns about job submissions or if you anticipate long-running jobs during the migration period, please communicate with our support team. We are here to work collaboratively and make necessary accommodations to facilitate the completion of your jobs during the migration process.</p>"},{"location":"archived/migration/faqs/#are-there-any-adjustments-needed-for-code-or-scripts-to-ensure-compatibility-with-wulver","title":"Are there any adjustments needed for code or scripts to ensure compatibility with Wulver?","text":"Answer <p>Yes, adjustments will be required for code or submission scripts to ensure compatibility with Wulver:</p> <ul> <li>Submit Script Changes: <ul> <li>Submit scripts will need to be modified to accommodate changes in partitions, hardware configurations, policies, and filesystems on Wulver. The research facilitation team will provide guidance and support in updating your submit scripts for seamless job submissions. Check the sample submit scipts for Wulver in SLURM.</li> </ul> </li> <li>Code Recompilation: <ul> <li>Due to differences in hardware, code may need to be recompiled to ensure optimal performance on Wulver. The research facilitation team is ready to assist you in this process, offering support to recompile code and address any related issues.</li> <li>If you code is compiled based on FOSS Toolchain (GCC and OpenMPI), you need to compile the code the same way you did in Lochness. Just make sure all the dependency libraries are installed on Wulver. Please visit Software to check the list of applications installed on Wulver.</li> <li>If your code is based on the Intel toolchain, you need to add the following while configuring your code.      <pre><code>./configure CFLAGS=\"-march=core-avx2\"\n</code></pre></li> <li>When installing codes, ensure that you perform the installation on the compute node rather than the login node. Since the hardware architecture is different on the login node, it's best practice to compile your code on the compute node. You need to initiate an interactive session with compute node before compiling your code.</li> </ul> </li> </ul> <p>Assistance will be provided to help you adapt your code and scripts to the new environment on Wulver. If you have specific concerns or require support in making these adjustments, please reach out to our research facilitation team, and they will work with you to ensure a smooth transition.</p>"},{"location":"archived/migration/faqs/#will-there-be-training-sessions-or-documentation-available-to-help-faculty-and-researchers-transition-smoothly-to-wulver","title":"Will there be training sessions or documentation available to help faculty and researchers transition smoothly to Wulver?","text":"Answer <p>While there are no official training sessions scheduled at this point, comprehensive documentation is available at NJIT HPC Documentation to assist faculty and researchers during the transition to Wulver.   In addition to documentation, the research facilitation team is committed to providing personal assistance to faculty and researchers. If you have specific questions, require hands-on support, or need guidance on using Wulver effectively for your research, please do not hesitate to reach out to the research facilitation team. We are here to ensure that you receive the assistance you need for a successful transition.</p>"},{"location":"archived/migration/faqs/#how-can-i-request-additional-resources-or-discuss-specific-requirements-for-my-research-projects-on-wulver","title":"How can I request additional resources or discuss specific requirements for my research projects on Wulver?","text":"Answer <p>To request additional resources or discuss specific requirements for your research projects on Wulver, please reach out to us at hpc@njit.edu. Our team is ready to assist you with any inquiries related to resource allocation, project needs, or any other aspects that can enhance your experience on the Wulver cluster. Your requests will be promptly addressed, and we are committed to providing the support necessary for the success of your research endeavors.</p>"},{"location":"archived/migration/faqs/#what-support-services-will-be-available-during-and-after-the-migration-to-address-any-issues-or-concerns","title":"What support services will be available during and after the migration to address any issues or concerns?","text":"Answer <p>The research facilitation team is committed to providing personalized assistance and support services during and after the migration:</p> <ul> <li> <ul> <li>The research facilitation team is dedicated to offering personal assistance to each user. Whether you need help with data migration, code adjustments, or understanding the new environment, our team is here to provide tailored support.</li> </ul> <p>Personal Assistance:</p> <ul> <li>Issue Resolution:</li> <li> <p>Any issues or concerns that arise during or after the migration will be promptly addressed by the research facilitation team. We aim to ensure a smooth transition for all users and are ready to tackle any challenges that may arise.</p> </li> <li> <p>Ongoing Support:</p> </li> <li>Support services will continue to be available after the migration to address ongoing needs, answer questions, and assist with any further optimizations or adjustments required for your research projects.</li> </ul> </li> </ul> <p>Your success is our priority, and the research facilitation team is here to guide you through the migration process and beyond. If you encounter any issues or have specific concerns, please reach out to the team for personalized assistance.</p>"},{"location":"archived/migration/faqs/#can-i-test-my-applications-or-simulations-on-wulver-before-the-migration-to-ensure-compatibility","title":"Can I test my applications or simulations on Wulver before the migration to ensure compatibility?","text":"Answer <p>Yes, absolutely! We encourage users to proactively test their applications or simulations on Wulver before the migration to ensure compatibility and identify any potential issues. This testing phase allows you to familiarize yourself with the new environment and address any concerns in advance.</p> <p>If you encounter challenges or have questions during the testing process, please don't hesitate to reach out to us. Our team is here to provide guidance, answer queries, and assist you in ensuring a smooth transition for your research activities on Wulver. Your proactive testing will contribute to a successful migration experience.</p>"},{"location":"archived/migration/faqs/#how-long-will-the-oldhome-directory-on-lochness-be-accessible-after-the-migration-and-what-actions-should-i-take-regarding-data-stored-there","title":"How long will the <code>/oldhome</code> directory on Lochness be accessible after the migration, and what actions should I take regarding data stored there?","text":"Answer <p>The <code>/oldhome</code> directory on Lochness will be accessible for 6 months after the migration is complete. During this period, users are advised to review and move their data to other locations on Wulver or archive it as needed. This timeframe provides a reasonable window for users to organize and transfer their data while ensuring a smooth transition.</p> <p>If you have specific questions about data migration or need assistance during this post-migration period, please reach out to our support team. We are here to help you with any further steps or considerations related to your data on Lochness.</p>"},{"location":"archived/migration/faqs/#will-afs-be-available-on-wulver","title":"Will AFS be available on Wulver?","text":"Answer <p>The AFS will not be available on Wulver. However we will setup a self-serve procedure for researchers to move AFS files to <code>/research</code>. We can do this as part of the full migration process. Please reach out to us at hpc@njit.edu for any questions.</p>"},{"location":"archived/migration/faqs/#i-have-several-conda-environments-on-lochness-should-i-perform-a-fresh-installation-or-copy-the-environment-to-wulver","title":"I have several Conda environments on Lochness. Should I perform a fresh installation or copy the environment to Wulver?","text":"Answer <p>We recommend a fresh installation since the hardware architecture is different on Wulver. However, you can export the existing Conda environment to a YAML file, transfer it to Wulver, and then install the environment using this YAML file. See Conda Export Environment for details.</p>"},{"location":"archived/migration/lochness_filesystem/","title":"Lochness to Wulver Migration: File Directory Changes","text":""},{"location":"archived/migration/lochness_filesystem/#overview","title":"Overview","text":"<p>As part of the HPC cluster migration from Lochness to Wulver, there will be changes in the file directory structure to ensure a smooth transition. This documentation provides details on how the <code>/research</code> and home directories will be handled on the new Wulver cluster.</p>"},{"location":"archived/migration/lochness_filesystem/#research-directory","title":"<code>/research</code> Directory","text":"<p>The <code>/research</code> directory from Lochness will be mounted on Wulver as <code>/research</code>. This ensures that users can seamlessly access their research-related files and data in the familiar directory structure. All content, including subdirectories and files, within <code>/research</code> on Lochness will be avaialble on the same location on Wulver.</p>"},{"location":"archived/migration/lochness_filesystem/#home-directories","title":"Home Directories","text":""},{"location":"archived/migration/lochness_filesystem/#lochness-home-directory","title":"Lochness Home Directory","text":"<p>The home directories from Lochness will be available on Wulver under the directory <code>/oldhome</code>. Users will find their personal home directories within <code>/oldhome</code>, allowing them to access their files and configurations.</p>"},{"location":"archived/migration/lochness_filesystem/#locking-of-oldhome-directory","title":"Locking of <code>/oldhome</code> Directory","text":"<p>After the migration is completed, the <code>/oldhome</code> directory on Lochness will be locked. This means that no additional files or directories can be created in <code>/oldhome</code>. The locking of this directory is a crucial step to maintain the integrity of the migrated data and to ensure a stable environment on both clusters.</p> <p>Users are advised to review and transfer any necessary files from <code>/oldhome</code> on Lochness to their new home directories on Wulver during the migration period.</p>"},{"location":"archived/migration/lochness_filesystem/#important-notes","title":"Important Notes","text":"<ul> <li>Migration Date: The migration process is scheduled to start on January 16, 2024.</li> <li>Locking Date for <code>/oldhome</code>: After the migration, the <code>/oldhome</code> directory on Lochness will be locked to prevent further modifications.</li> <li>File Access: Users will have continued access to their research files under <code>/research</code> on Wulver and their home directories under <code>/oldhome</code> on Lochness until the specified locking date.</li> </ul> <p>Please make the necessary adjustments to your workflows and ensure a smooth transition by reviewing and moving your files as needed. If you have any questions or concerns, feel free to reach out to our support team.</p>"},{"location":"clusters/","title":"Current cluster","text":"<ul> <li>Wulver is NJIT's newest High Performance Cluster made available to users in Jan 2024.</li> </ul> <ul> <li> </li> </ul>"},{"location":"clusters/#virtual-tour-of-njit-data-center","title":"Virtual Tour of NJIT Data Center","text":"<p>Wulver is built through a partnership with DataBank, which is live in DataBank\u2019s Piscataway, N.J. data center (EWR2) and will support NJIT\u2019s research efforts. This infrastructure will bolster NJIT\u2019s research initiatives. You can access the 3D virtual tour of HPC data center below:</p> <p></p>"},{"location":"clusters/Wulver_filesystems/","title":"Wulver Filesystems","text":"<p>The Wulver environment is quite a bit like Lochness, but there are some key differences, especially in filesystems and SLURM partitions and priorities.</p> <p>Wulver Filesystems are deployed with more attention to PI ownership / group efforts:</p> <ol> <li>The <code>$HOME</code> directory is not intended for primary storage and has a 50GB quota. The main location for storing files is the group project directory which has 2TB of storage per PI group. To run the simulations, compilations, etc., users need to use a project directory which has 2TB of storage per PI group. Students can store their files under their corresponding PI\u2019s UCID in the <code>/project</code> directory.  For example, if PI\u2019s UCID is <code>doctorx</code>, then students need to use the <code>/project/doctorx/</code> directory. </li> <li>Users can also store temporary files under the <code>/scratch</code> directory, likewise under a PI-group directory. For example, PI\u2019s UCID is <code>doctorx</code>, so students need to use the <code>/scratch/doctorx/</code> directory.  Please note that the files under <code>/scratch</code> will be periodically deleted. To store files for longer than computations, please use the <code>/project</code> directory.  Files under <code>/scratch</code> are not backed up. For best performance simulations should be performed in the <code>/scratch</code> directory. Once the simulation is complete, the results should be copied into the <code>$HOME</code> or <code>/project</code> directory.  Files are deleted from <code>/scratch</code> after they are 30 days old. Users will get notified prior deletion so that they can review the files and move them to <code>/project</code> if required.</li> </ol> Filesystem Purpose Characterics Backup policy Total Size Default Quota Deletion Policy Cost per TB <code>/home</code> Non-research user files, such as profile, history, etc., files not intended for sharing.  Not for actual research files. Kalray Pixstor / GPFS (expensive) Daily ~1 PB 50GB per user (locked) One year after owner leaves NJIT Not possible to increase size, use <code>/project</code> or <code>/research</code> instead <code>/project</code> Active research by groups. Deployed as <code>/project/$PI_UCID/$LOGIN/</code> Kalray Pixstor / GPFS (expensive) Daily ~1 PB 2TB per group TBD $200 per TB for a duration of five years, minimum storage allocation of 5TB is required <code>/scratch</code> Temporary space for intermediate results, downloads, checkpoints, and such. MOVE YOUR RESULTS &amp; IMPORTANT FILES TO <code>/project</code> or <code>/research</code> Nvme  (very expensive) NEVER ~ 150 TB 10TB per group Files deleted after 30 days or sooner if 80% full No charge, but files are automatically deleted <code>/local</code> Very high speed temporary storage, can be accessed while running jobs on the node via <code>$TEMP_DIR</code> Node-local SSD or NVME (very expensive) NEVER 800 GB per node shared by all users NONE Files deleted after job completion No charge, but files are automatically deleted <code>/research</code> Long term archive. Users can buy as much as they need. The users need to buy this space.  Existing purchases/quotas will be kept over from Lochness. NFS (inexpensive) Daily 8 TB Whatever the PI purchases. TBD $100 per TB per five years"},{"location":"clusters/cluster_access/","title":"Access to NJIT Clusters","text":"<p>The following instructions are provided to access NJIT HPC clusters from a local computer.</p>"},{"location":"clusters/cluster_access/#getting-a-login","title":"Getting a Login","text":"<p>Faculty can obtain a login to NJIT's HPC by sending an email to hpc@njit.edu. Students can obtain a login either by taking a class that uses one of the systems or by asking their faculty adviser to contact on their behalf. Your login and password are the same as for any NJIT AFS system.</p>"},{"location":"clusters/cluster_access/#access-to-clusters","title":"Access to Clusters","text":"<p>Make sure the user is connected to <code>NJITsecure</code> if the user is on campus. If working off campus, NJIT VPN is required. Please find the details here. Here we will provide instructions for connecting to NJIT HPC on Mac/Linux and Windows OS.</p> <p>Update</p> <p>In the recent update (Sep 10<sup>th</sup> 2024) Cisco two-factor authentication (TFA) is deployed, similar to what is already deployed on NJIT websites and VPN. </p> Mac/LinuxWindows <p>Open terminal from Launchpad and select terminal. Type the following in the terminal and replace <code>$UCID</code> with your NJIT UCID.. <pre><code>  localhost&gt; ssh -X -Y $UCID@wulver.njit.edu  \n</code></pre></p> <p>Users will be prompted for your password. Enter your NJIT UCID password. Users can omit the <code>-X -Y</code> if you are not using a graphic interface. Once the password is provided, User will authenticate via Cisco two-factor authentication (TFA)</p> <pre><code>(guest@wulver.njit.edu) Duo two-factor login for guest\n\nEnter a passcode or select one of the following options:\n\n1. Duo Push to XXX-XXX-2332\n2. Phone call to XXX-XXX-2332\n3. SMS passcodes to XXX-XXX-2332 (next code starts with: 1)\n\nPasscode or option (1-3):\n</code></pre> <p>Based on the option provided, user will be logged in after succesfull authetication.  </p> <p>Download the MobaXterm from this link.  Open MobaXterm after installation is completed.  Select Start local terminal to open the terminal.</p> <p></p> <p>Type <code>ssh $UCID@wulver.njit.edu</code>. Replace <code>$UCID</code> with your NJIT UCID.</p> <p>. </p> <p>User will be prompted to type the password. The password is NJIT UCID password. After successful login, user will see the following in the terminal, which means the user is now connected to HPC.</p> <pre><code>Last login: Tue Oct 25 12:37:31 2022 from 10.192.228.138\nStarting /home/a/user/.bash_profile ... standard AFS bash profile\n\n========================\nHome directory : /home/u/user is not in AFS -- skipping quota check\n========================\n\nOn host login-1 :\n         12:38:05 up 315 days, 22:36, 35 users,  load average: 10.32, 10.10, 10.63\n\n=== === === Your Kerberos ticket and AFS token status === === ===\nKerberos :  Renew until 10/26/22 12:38:01, Flags: FRIA Renew until 10/26/22 12:38:01, Flags: FRA\nAFS      :  User's (AFS ID 105631) tokens for afs@cad.njit.edu [Expires Oct 25 22:38]\n\nRunning your /home/u/user/.modules file:\n\nTo see your aliases, enter \"alias\"\n\nlogin-1-41 ~ &gt;:\n</code></pre>"},{"location":"clusters/cluster_access/#transfer-the-data-from-the-local-machine-to-clusters-or-vice-versa","title":"Transfer the Data from the Local Machine to Clusters or vice versa","text":"Mac/LinuxWindows <p>Users need to use the command in the terminal to transfer in and out the data.  </p> <p>User needs to select the <code>follow terminal folder</code> of the left pane of the MobaXterm terminal. </p> <p></p> <p>Next, to transfer the data from the local machine to Wulver user needs to select the <code>Upload to current folder</code> option, as shown below. Selecting this option will open a dialogue box where user needs to select the files to upload.</p> <p></p> <p>For transferring the data from Wulver to the local machine user needs to select the directory or the data from the left pane and then select <code>Download selected files</code>.</p>"},{"location":"clusters/cluster_access/#rsync","title":"<code>rsync</code>:","text":"<ul> <li> <p>Transfer the data from local machine to HPC cluster <pre><code>rsync -avzP /path/to/local/machine $UCID@wulver.njit.edu:/path/to/destination\n</code></pre></p> </li> <li> <p>To transfer the data from HPC cluster to local machine use <pre><code>rsync -avzP $UCID@wulver.njit.edu:/path/to/source /path/to/local/machine\n</code></pre></p> </li> </ul>"},{"location":"clusters/cluster_access/#scp","title":"<code>scp</code>:","text":"<ul> <li> <p>Copy files from remote machine to local machine <pre><code>scp [option] [$UCID@wulver.njit.edu:path/to/source/file] [target/path]\n</code></pre></p> </li> <li> <p>Copy files from local machine to remote machine <pre><code>scp [option] [path/to/source/file] [$UCID@wulver.njit.edu:target/path] \n</code></pre></p> </li> <li> <p>Example of scp: <pre><code>scp -r example $UCID@wulver.njit.edu:/home/dir \n</code></pre> Copy the \u201cexample\u201d folder recursively to <code>/home/dir</code></p> </li> </ul>"},{"location":"clusters/get_started_on_Wulver/","title":"Getting Started on Wulver","text":"<p>Wulver is a high performance computing (HPC) cluster \u2013 a collection of computers and data storage connected with high-speed low-latency networks. We refer to individual computers in this network as nodes. Wulver is only accessible to researchers remotely; your gateways to the cluster are the login nodes. From these nodes, you can view and edit files and dispatch jobs to computers configured for computation, called compute nodes. The tool we use to manage these jobs is called a job scheduler. All compute nodes on a cluster mount several shared filesystems; a file server or set of servers store files on a large array of disks. This allows your jobs to access and edit your data from any compute node. </p> <p></p>"},{"location":"clusters/get_started_on_Wulver/#being-a-good-hpc-citizen","title":"Being a Good HPC Citizen","text":"<p>While using HPC resources, here are some important things to remember:</p> <ul> <li>Do not run jobs or computation on a login node, instead submit jobs to compute nodes.  You should be using <code>sbatch</code>, <code>srun</code>, or OnDemand to run your jobs.  </li> <li>Never give your password to anyone else.</li> <li>Do not run larger numbers of very short (less than a minute) jobs Use of the clusters is also governed by our official guidelines. Violating the guidelines might result in having your access to Wulver revoked, but more often the result is your jobs will run painfully slower.</li> </ul>"},{"location":"clusters/get_started_on_Wulver/#remote-access","title":"Remote Access","text":"<p>All users access the Wulver cluster remotely, either through ssh or a browser using the Open OnDemand portal. See these detailed login instructions. NB: If you want to access the clusters from outside NJIT\u2019s network, you must use the VPN.</p>"},{"location":"clusters/get_started_on_Wulver/#schedule-a-job","title":"Schedule a Job","text":"<p>On our cluster, you control your jobs using a job scheduling system called Slurm that allocates and manages compute resources for you. You can submit your jobs in one of two ways. For testing and small jobs, you may want to run a job interactively. This way you can directly interact with the compute node(s) in real time. The other way, which is the preferred way for multiple jobs or long-running jobs, involves writing your job commands in a script and submitting that to the job scheduler. Please see our Running Jobs or review our training materials for more details.  </p>"},{"location":"clusters/get_started_on_Wulver/#use-software","title":"Use Software","text":"<p>To best serve the diverse needs of all our researchers, we use software modules to make multiple versions of popular software available. Modules allow you to swap between different applications and versions of those applications. The software can be loaded via <code>module load</code> command. You see the following modules are loaded once you log in to the Wulver. Use the <code>module li</code> command to see the modules.  <pre><code>   1) easybuild   2) slurm/wulver   3) null\n</code></pre> If you cannot find certain software or libraries on the Wulver cluster, please submit a request for HPC Software Installation by visiting the Service Catalog. The list of installed software or packages on the Wulver HPC cluster can be found in the Software List.</p>"},{"location":"clusters/get_started_on_Wulver/#shared-filesystems","title":"Shared Filesystems","text":"<p>A critical component of Wulver is its shared filesystem, which facilitates the efficient storage, retrieval, and sharing of data among the various compute nodes. It enables multiple users and applications to read from and write to a common storage pool, ensuring data consistency and accessibility across the entire system. See Wulver Filesystems for a different type of shared filesystems.</p>"},{"location":"clusters/get_started_on_Wulver/#transfer-your-files","title":"Transfer Your Files","text":"<p>As part of setting up and running jobs and collecting results, you will want to copy files between your computer and the clusters. We have a few options, and the best for each situation usually depends on the size and number of files you would like to transfer. For most situations, uploading a small number of smaller files through Open OnDemand's upload interface is the best option. This can be done directly through the file viewer interface by clicking the Upload button and dragging and dropping your files into the upload window. Check the Ondemand file transfer for more details. For more information on other upload methods, see our transferring data instructions. </p>"},{"location":"clusters/get_started_on_Wulver/#workshop-and-training-videos","title":"Workshop and Training Videos","text":"<p>Each semester, we host webinars and training sessions. Please check the list of events and register from HPC Events. You can also check the recordings of previously hosted webinars at HPC Training.</p>"},{"location":"clusters/get_started_on_Wulver/#linux","title":"Linux","text":"<p>Our cluster runs Red Hat Enterprise Linux 8, utilizing the bash (or zsh set via https://myucid.njit.edu) command line interface (CLI).  A basic familiarity with Linux commands is required for interacting with the clusters. We have a list of commonly used commands here. We periodically run an Intro to Linux Training to get you started, see our Events for upcoming training. There are also many excellent beginner tutorials available for free online, including the following:</p> <ul> <li>Unix Tutorial for Beginners</li> <li>Cornell Virtual Workshop: An Introduction to Linux</li> </ul> <p>In the table below, you can find the basic linux commands required to use the cluster. For more details on linux commands, please see the Linux commands cheat sheets.</p> Linux Commands Description <code>cd [directory]</code> Change directory <code>cd ..</code> Change to one directory up <code>mkdir [directory]</code> create a directory <code>mkdir -p [directory]</code> create directories as necessary, if the directories exist, no error is specified <code>pwd</code> Print current working directory <code>ls</code> lists directory contents of files and directories <code>ls -ltra</code> all the files in long format with the newest one at the bottom <code>cp /path/to/source/file1 /path/to/destination/file2</code> Copy files from source to destination <code>cp \u2013r /path/to/source/dir1 /path/to/destination/dir2</code> Copy directory from source to destination <code>mv /source/file1 /source/file2</code> move directories or files  and rename them <code>cat filename1</code> display the content of text files <code>rm -rf /path/to/dir</code> used to delete files and directories"},{"location":"clusters/get_started_on_Wulver/#get-help","title":"Get Help","text":"<p>If you have additional questions, please email us at hpc@njit.edu. If you are having a problem with <code>sbatch</code> or <code>srun</code>, please include the following information:</p> <ul> <li>Job ID#(s)</li> <li>Error messages</li> <li>Command used to submit the job(s)</li> <li>Path(s) to scripts called by the submission command</li> <li>Path(s) to output files from your jobs</li> </ul> <p>Here are some tips to get help faster:</p> <ul> <li>The fastest way to get help is by sending an email to HPC@NJIT.EDU, this is routed right to us at ARCS HPC and will be answered by the most knowledgeable team member based on your question.</li> <li>Never reply to an earlier HPC email incident as the ticketing system does not make a new ticket when you respond to an already closed ticket.</li> </ul>"},{"location":"clusters/wulver/","title":"Wulver","text":"<p>Wulver is NJIT's newest cluster, which went into production in early 2024. To get started with Wulver, please check the details in get started on Wulver</p>"},{"location":"clusters/wulver/#specifications","title":"Specifications:","text":"General Bigmem GPU Partition <code>general</code> <code>bigmem</code> <code>gpu</code> Nodes 100 2 25 CPU Type AMD EPYC 7753 AMD EPYC 7753 AMD EPYC 7713 CPU Speed (GHZ) 2.45 2.45 2.0 CPUs/Socket 64 64 64 Sockets/Node 2 2 2 RAM/Node (GB) 512 2048 512 GPU Type - - nVIDIA A100 GPUs/Node - - 4 GPU Memory (GB) - - 80 Total CPUs 12800 256 3200 Total RAM (TB) 50 4 12.5 Total SUs/year 112128000 2242560 28032000 Peak Performance (TFLOPs) 501.76 10.035 1072.4 HPL (TFLOPs/Node) - - - <ul> <li>100 GB High throughput/low latency Infiniband network</li> <li>Arcastream Storage<ul> <li>Parallel filesystem allowing multiple read/write operations</li> <li>Max performance 20 GB/s</li> <li>Hierarchical storage; NVME scratch, SAS, Cloud</li> <li>User transparent transfer from high-speed to medium to archive storage</li> <li>1 PB total storage</li> </ul> </li> <li>Virtualized Support and login nodes</li> <li>Management and deployment overseen by NJIT:<ul> <li>Nvidia Bright Cluster Manager</li> <li>X-ISS system administration</li> <li>SLURM scheduler with full support</li> <li>InfiniBand support via Dell or directly from Mellanox</li> <li>Will be deployed in the Databank data center in Piscataway</li> </ul> </li> </ul>"},{"location":"clusters/decommissioned/lochness/","title":"Lochness","text":"<p>This very heterogeneous cluster is a mix of manufacturers, components, and capacities as it was built up in incremental purchases spanning several years. </p> <p>Lochness was decommissioned on March 2024. Many lochness nodes were incorporated into Wulver cluster 2Q 2025 for HPC course purposes.</p>"},{"location":"clusters/decommissioned/lochness/#specifications","title":"Specifications:","text":"Current Partitions Number of Nodes Processor Family Cores Per Node Total Cores Number of GPU per node Total GPUs Model of GPU bader 1 Sky Lake 20 20 4 4 Titan V cld 11 Sandy Bridge 20 220 0 9 cld-gpu 2 Sandy Bridge 20 40 2 4 K20Xm datasci 7 Sandy Bridge 20 140 2 14 P100 datasci 1 Sky Lake 20 20 4 4 Titan RTX datasci3 1 Sky Lake 20 20 4 4 Titan RTX datasci4 1 Sky Lake 20 20 4 4 Titan RTX davidsw 1 Cascade Lake 32 32 2 2 A100 ddlab 1 Sandy Bridge 20 20 0 0 ddlab 1 Cascade Lake 20 20 2 2 V100 esratoy 2 Cascade Lake 32 64 0 0 fahmadpo 14 Cascade Lake 64 896 0 0 fahmadpo-gpu 1 Cascade Lake 24 24 4 4 Titan RTX gor 28 Broadwell 20 560 0 0 gperry 3 Cascade Lake 36 108 0 0 hrjin 1 Sky Lake 32 32 4 3 Titan XP jyoung 3 Cascade Lake 96 288 0 0 phan 1 Cascade Lake 20 20 4 4 1 Titan X,3 Ge Force project 1 Cascade Lake 32 32 0 0 public 37 Cascade Lake 32 1184 0 0 samaneh 16 Cascade Lake 40 640 0 0 shakib 20 Cascade Lake 24 480 0 0 singhp 1 Ice Lake 56 56 0 0 smarras 4 Cascade Lake 48 192 0 0 smarras 2 Cascade Lake 48 96 2 2 A100 solarlab 1 Cascade Lake 48 48 2 2 A100 solarlab 4 Cascade Lake 48 192 0 0 solarlab 1 Ice Lake 48 48 1 1 A100 solarlab 2 Ice Lake 48 96 0 0 solarlab 1 Ice Lake 48 48 0 0 xt3 11 Cascade Lake 32 352 0 0 xye 1 Cascade Lake 16 16 5 5 3 types of GPUs zhiwei 1 Ice Lake 56 56 4 4 A40 183 nan 6080 nan 72 <ul> <li>All nodes have:<ul> <li>Ethernet network interface (1GigE or 10GigE)</li> <li>Infiniband network interface (mix of HDR100, EDR, and FDR speeds)</li> <li>1TB local storage (mostly SSD but a few HD)</li> </ul> </li> <li>All nodes have network accessible storage:<ul> <li><code>/home/</code>: 26 TB</li> <li><code>/research/</code>: 97 TB</li> <li><code>/afs/cad/</code>: 50 TB </li> </ul> </li> </ul> <p>The cluster also features</p> <ul> <li>\"CentOS Linux 7 (Core)\" operating system</li> <li>Virtualized login and control nodes</li> <li>SLURM job scheduler</li> <li>Warewulf stateless node provisioning</li> <li>Managed entirely by NJIT personnel</li> </ul> <p>Roughly half under warranty; rest are time and materials</p>"},{"location":"clusters/decommissioned/stheno/","title":"Stheno","text":"<p>Stheno was decommissioned on October 6<sup>th</sup>, 2023. Stheno users can access their data from Lochness when Lochness is returned to service.</p>"},{"location":"clusters/decommissioned/stheno/#specifications","title":"Specifications","text":"<ul> <li>GPU nodes: 2</li> <li>Total GPUs: 4</li> <li>GPU models:</li> <li>2x  node(s) w/ 2x K20m GPU(s)</li> <li>Total Nodes: 22</li> <li>Total Cores: 1224</li> <li>Total GB RAM: 2481667</li> <li>Nodes:  GB:<ul> <li>15x     128</li> <li>7x      96</li> </ul> </li> <li>Processor models:<ul> <li>15x Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz</li> <li>7x Intel(R) Xeon(R) CPU E5649  @ 2.53GHz</li> </ul> </li> </ul>"},{"location":"facilities/facilities/","title":"Facilities","text":"<p>As New Jersey\u2019s Science and Technology University, New Jersey Institute of Technology (NJIT) has developed a local cyberinfrastructure well positioned to allow NJIT faculty and students to collaborate at local, national, and global levels on many issues at the forefront of science and engineering research.</p>"},{"location":"facilities/facilities/#cyberinfrastructure","title":"Cyberinfrastructure","text":"<p>NJIT\u2019s Information Services and Technology (IST) resources provide members of the university community with universal access to a wealth of resources and services available over the NJIT network. NJIT's multi-100 gigabit backbone and multi-gigabit user network provides access to classrooms, laboratories, residence halls, faculty and staff offices, the library, student organization offices and others. 50% of these locations are provided with speeds of 5Gb/s or more. The campus wireless network 2,900+ access points blanket the university\u2019s public areas, classrooms, offices, collaboration spaces and outdoor areas. Over 60% of the Wi-Fi network is Wi-fi 6 capable, enabling NJIT\u2019s mobile users connectivity with multiple devices at increasing speeds. NJIT\u2019s connectivity to NJEdge, NJ\u2019s state-wide higher education network, provides access to the Internet and Internet 2. Students have the opportunity to work closely with faculty and researchers as new families of advanced applications are developed for an increasingly networked and information-based society. NJIT is also directly connected to cloud service providers such as AWS (Amazon Web Services) to provide low latency high speed access to cloud resources. A redundant diverse 100Gb network connection is being provisioned to support NJIT\u2019s new HPC co-location facility in Piscataway NJ.</p>"},{"location":"facilities/facilities/#compute-resources","title":"Compute Resources","text":"<p>NJIT\u2019s Advanced Research Computing Services (ARCS) group presently maintains a 127-node heterogeneous condominium cluster at the new HPC co-location facility, Databank in Piscataway, NJ, which went into production for general use in early 2024. The previous 224-node heterogeneous condominium cluster, Lochness, was decommissioned; however, a few nodes from Lochness will soon be integrated into the Wulver for academic use.</p> Category Wulver Total Nodes 127 Total Cores 6256 Total RAM / GB 68096 Total GPUs 100 Total GPU RAM / GB 8000 Access General Network (Infiniband; Ethernet) HDR100 and 10Gig Theoretical Peak Performance (TFLOPs) 1584.19"},{"location":"facilities/facilities/#storage-resources","title":"Storage Resources","text":"<p>Storage on Wulver will be provided by a 1PB arcastream high-performance storage that combines flash, disk, tape, and cloud storage into a unified single namespace architecture. Data moves seamlessly through various tiers of storage - from fast flash to cost-effective, high capacity object storage, all the way out to the cloud.</p> <p>The ARCS team provides 50TB of research and academic storage via the Andrew File System (AFS) distributed file system. AFS is backed up daily via IST\u2019s enterprise backup system. The current AFS implementation, OpenAFS, which is open source, will be replaced with a commercial implementation, AuriStor, during the 2022-2023 academic year, providing important enhancements in performance, security,capacities, authorization, permissions, and administration as well as bug fixes and technical support.</p>"},{"location":"facilities/facilities/#afs-storage","title":"AFS Storage","text":"<p>AFS is a distributed file system. Its principal components are :</p> <ul> <li>Database servers : provide information on authorization, and directory and file locations on file servers </li> <li>File servers : store all data in discrete \"volumes\", with associated quotas </li> <li>Client software : connects AFS clients to database servers and file servers over a network. Every client has access to every file in AFS, subject to the permissions attached to the identity of the user logged into the client. Client software is available for Linux, MacOS, and Windows. Single global name space for all clients. See the identical path names and permissions.</li> </ul> <p>An AFS \"cell\" is a group of database servers and file servers sharing the same cell name.</p> <p>AFS was designed for highly-distributed wide area network (WAN) use. A cell can be concentrated in one physical location, or be widely geographically dispersed. A client in one cell can be given fine-grained levels of access to the data in other cells.</p> <p>The NJIT cell name is <code>cad.njit.edu</code>; all file and directory paths begin with <code>/afs/cad.njit.edu/</code>(abbreviated to <code>/afs/cad/</code>). This cell currently contains about 27TB of research data, 4TB of user data,and 1.4TB of applications data, in about 47,700 volumes.</p> <p>The current AFS implementation, OpenAFS, which is open source, will be replaced with a commercial implementation during the 2022-2023 academic year, providing important enhancements in performance, security,capacities, authorization, permissions, and administration as well as bug fixes and technical support.</p> <p>All of <code>/afs/cad/</code> is backed up daily via IST enterprise backup.</p>"},{"location":"facilities/facilities/#cloud-computing","title":"Cloud Computing","text":"<p>Access to Cloud Computing is provided via Rescale, a cloud computing platform combining scientific software with high performance computing. Rescale takes advantage of commercial cloud computing vendors such as AWS, Azure and Google Cloud to provide compute cycles as well as storage. The Rescale services also include applications setup, and billing and provides a pay-as-you-go method for researchers to use commercial cloud services - e.g., Amazon Web Services, Azure, Google Cloud Platform.</p>"},{"location":"facilities/facilities/#data-center-space-power-and-cooling","title":"Data Center: Space, Power and Cooling","text":"<p>NJIT\u2019s recent purchase of the Wulver cluster exceeds the capacity of NJIT\u2019s current computing facilities in terms of power and cooling. To accommodate Wulver and future expansion, NJIT has partnered with Databank, a leader in collocation facilities. Databank has more than 65 datacenters in over 27 metropolitan areas, supporting many industries including very large HPC deployments. The Databank location in Piscataway NJ will provide NJIT with 100% uptime SLA due to redundant power, cooling, and network facilities. The facility also provides water-cooling instead of traditional air-conditioned cooling in order to support far denser equipment needed for modern HPC. </p>"},{"location":"facilities/facilities/#services","title":"Services","text":"<p>NJIT employs nine FTE staff members to administer and support research computing resources. Services available to the user community include system design, installation, and administration of research computing resources, application support, assistance with software purchasing and consulting services to faculty members, their research associates, and students.</p>"},{"location":"faq/faq/","title":"FAQs","text":"<p>Welcome to our most frequently asked questions. If you cannot find an entry related to your question, please contact us, and we will add it.</p>"},{"location":"faq/faq/#login-issues-access","title":"Login Issues / Access","text":"How do I get access to the Wulver HPC cluster? <ul> <li>If you are a student or researcher, your research/faculty advisor will need to request an account on your behalf. </li> <li>If you are a faculty member, then you can directly email us at hpc@njit.edu for an account.</li> <li>For individuals non affiliated with NJIT have to contact a faculty member in NJIT to sponsor you a guest account</li> <li>If you are taking a course that requires computation on Wulver please ask your course instructor to email hpc@njit.edu to request access for the students.</li> <li>For detail information please click here.</li> </ul> How do I connect to the Wulver HPC cluster? <ul> <li>For detail information please click here.</li> </ul> Why can\u2019t I log in to the HPC? <ul> <li>It\u2019s possible that your account is not activated on Wulver. Make sure to follow the instructions in How do I get access to the Wulver HPC cluster?. Once your account is created, you will receive a confirmation in your NJIT email.</li> <li>It\u2019s possible that your account is not activated on Wulver. Make sure to follow the instructions in How do I get access to the Wulver HPC cluster?. Once your account is created, you will receive a confirmation in your NJIT email.</li> <li>Ensure that your UCID and password are correct.</li> <li>If you are working off-campus, make sure to connect to NJIT VPN before logging into Wulver. Visit https://ist.njit.edu/vpn for more information on VPN installation.</li> </ul> Why is my password not showing in the terminal when I type? <ul> <li>The Command Line Interface hides passwords as you type to protect against visual exposure and enhance security.</li> </ul> Why am I getting \u201cpermission denied\u201d ? <ul> <li>You might not have access to the HPC yet. For requesting access, please check our user access.</li> <li>Your instructor/PI might not have added you into the group yet.</li> <li>Cluster might be having downtime due to maintenance or other reasons.</li> </ul> How do I transfer data to and from the HPC cluster? <ul> <li>Detailed instructions are given  here.</li> </ul> What security measures should I be aware of when using the HPC cluster? <ul> <li>Do not share your login information with anyone else or allow anyone to login with your account.</li> </ul> Which directory will I land on when I log in? <ul> <li>You will enter into your home directory named under your UCID.</li> <li>Please note that you are on the login node of Wulver. Do not run any computations on the login nodes, as CPU and memory usage are limited per user on these nodes. To perform computations, you need to request resources from the compute nodes via SLURM.</li> </ul>"},{"location":"faq/faq/#file-systems-storage","title":"File Systems / Storage","text":"What are the different file storage systems available on Wulver? <ul> <li>Please visit our file system page for detailed information. </li> </ul> How can I get more storage? <ul> <li>When your account is active on Wulver, you will have access to following file systems<ul> <li>$HOME directory - Default quota of 50GB per user and cannot be increased</li> <li>/project - Default quota of 2TB per PI group and can be increased by purchasing additional storage at $200/TB for 5 years.</li> <li>/scratch - No quota but files will be deleted after 30 days or sooner if the directory reaches 80% capacity. Users will be notified prior to deletion so they can review and move important files to <code>/project</code> if necessary.</li> </ul> </li> <li>For more details, visit file system.</li> </ul> I have an error \u201cdisk quota exceeded\u201d while running a job, how can I solve this error? <ul> <li>First, check which filesystem is causing the error. If it\u2019s in <code>$HOME</code>, it means the 50GB quota has been exceeded. You can either delete files, compress them, or move them to <code>/project</code>. If the error is in <code>/project</code>, you need to compress or delete some files. Alternatively, you can ask your PI to purchase an increase  in the <code>/project</code> quota by emailing us at hpc@njit.edu. You can also run your code in <code>/scratch</code> and, after the simulation, transfer the important files to <code>/project</code>. </li> <li>Use <code>quota_info</code> command to check the filesystem quota. </li> <li>If you are encountering this error while running Python or Jupyter Notebook via Conda, the issue is likely due to Conda packages or cache files stored in <code>$HOME</code>. Use the <code>sn p $HOME</code> command to view a detailed breakdown of each directory in $HOME, showing how much space it is consuming. If you notice that the <code>.cache</code> directory is consuming a significant amount of space, you should remove it. If the <code>.conda</code> directory is taking up too much space, you need to move the Conda environment to <code>/project</code>. Refer to this guide for detailed steps. </li> </ul> What is <code>/scratch</code> used for? <ul> <li>Run your code in <code>/scratch</code> and, after the simulation, transfer the important files to <code>/project</code>. </li> <li>NB : This space is not backed up; files will be deleted after 30 days.</li> </ul> How can I remove files from my project directory when a student leaves? <ul> <li>When a student leaves NJIT or a research group, their files and directories are the responsibility of the Principal Investigator (PI). PIs are the owners of these directories and should review, retain, or delete the data as needed. HPC support does not automatically delete user data; PIs should manage directory cleanup unless assistance is required due to permission or exceptional issues.</li> </ul>"},{"location":"faq/faq/#jobs-and-scheduling","title":"Jobs and scheduling","text":"How do I submit and manage jobs on the Wulver HPC cluster? <ul> <li>We use the SLURM resource manager and scheduler on Wulver for submitting and managing jobs on the compute nodes. </li> <li>Check our Running Jobs page on the website for more detailed guidance.</li> </ul> What is Walltime? <ul> <li>Walltime refers to the maximum amount of real-world time a job is allowed to run on the cluster. The actual job may finish earlier. </li> <li>To set the walltime, you'll typically specify it in the job submission script:  <pre><code>#SBATCH --time=01:00:00      # Request 1 hour of walltime\n</code></pre></li> </ul> How can I monitor the status of my jobs? <ul> <li>For checking and monitoring the status of your job please use this guide for detailed information.</li> </ul> Where will my output appear after I submit a job? <ul> <li>Your output will appear in the file which you initialized in your job script which look like below: <pre><code>#SBATCH --output=file_name.%j.out # %j expands to slurm JobID\n</code></pre></li> <li>By default, this file is in the same directory as the job script, unless you specify the working directory via <code>sbatch</code></li> </ul> How can I see the status of the cluster? <ul> <li>For checking the cluster load use this link</li> </ul>"},{"location":"faq/faq/#maintenance","title":"Maintenance","text":"When does maintenance occur? <ul> <li>Second Tuesday 9AM - 9PM monthly.</li> </ul> I submitted my job before maintenance, but why did it go to pending status as \"ReqNodeNotAvail, Reserved for maintenance\" ? <ul> <li>If your job walltime request indicates the job will not finish before the maintenance starts then the scheduler will hold the job.  </li> <li>If you resubmit the job with a shorter walltime, it will not be held. </li> <li>For example: If you submit a job on Monday with a walltime of 2 days and maintenance is scheduled on Tuesday then your job overlaps with the maintenance schedule. Hence, SLURM will immediately put it on hold until the maintenance is completed and start it later.</li> </ul> How will maintenance affect me? <ul> <li>During the maintenance downtime, logins will be disabled, users will not have access to their stored data in <code>/project</code>, <code>/home</code> and <code>/scratch</code>.</li> <li>All jobs that do not end before 9AM will be held by the scheduler until the downtime is complete and the systems are returned to service.</li> </ul> Will I receive a notification before maintenance? <ul> <li>Our regular monthly maintenance cycle is every 2<sup>nd</sup> Tuesday. If there is a change to this cycle, all users will receive notification.</li> </ul> What happens to my jobs during maintenance? <ul> <li>Jobs queued just before the maintenance will be held in Pending State and then later gets continued.</li> </ul>"},{"location":"faq/faq/#software-and-hardware-specifications","title":"Software and Hardware Specifications","text":"Is there any installed software? <ul> <li>We have a variety of software packages already installed on Wulver.</li> <li>For more information, please visit our Software guide.</li> </ul> I want to install new software? <ul> <li>Please first check our already installed software list, and if you still don\u2019t find it then visit our guide for detailed guidance on software installation.</li> </ul> What are the hardware specifications of the Wulver HPC cluster? <ul> <li>Please check out our Wulver page for complete details on hardware specifications.</li> </ul> What programming languages are commonly used on the HPC cluster? <ul> <li>Most common ones are C, C++, Fortran, Python, R</li> </ul> What are the tools and compilers available on HPC cluster? <ul> <li>Common programming tools include Intel and GNU compilers as well as MPI for multi-node jobs.</li> <li>For GPU acceleration we have CUDA and OpenACC.</li> <li>Details on these resources are available here.</li> </ul> How can I optimize my code for parallel processing? <ul> <li>To optimize your code, you can use different parallelization techniques depending on your setup:<ul> <li>MPI and OpenMP for parallelizing code and then specifying the cores.</li> <li>CUDA and OpenACC for GPU acceleration, especially for compute-heavy tasks.</li> </ul> </li> <li>Focus on optimizing CPU, I/O, memory, and parallelism.</li> <li>If you have specific questions about this, please email HPC Help at hpc@njit.edu to request support.</li> </ul> Can I request additional resources or customization for my projects? <ul> <li>Please see Wulver Usage and Condo Policy for resource allocation details. </li> <li>The Research Computing Advisory Board is working on establishing policies and procedures for requesting additional computing time beyond the standard 300K SU/year.</li> </ul> My software requires a license, how should I proceed? <ul> <li>License is purchased by the user and his/her department.</li> <li>Please visit the software\u2019s documentation for your specific software.</li> </ul>"},{"location":"faq/faq/#miscellaneous","title":"Miscellaneous","text":"What documentation and support resources are available for Wulver users? <ul> <li>Please visit the Education and Training tab on our website.</li> <li>If you have any specific questions which are not covered, please contact us at hpc@njit.edu</li> </ul>"},{"location":"faq/rhel_9_faq/","title":"FAQ: RHEL8 to RHEL9 OS Upgrade","text":"<p>We are upgrading Wulver's operating system from RHEL8 to RHEL9 during the September maintenance cycle. If you have software installed for your group, complete this form to request access to the test cluster.  Please review these frequently asked questions. Please contact us if you have further questions.</p> What is changing on Wulver with the OS upgrade from RHEL8 to RHEL9? <ul> <li>The cluster\u2019s operating system is being upgraded from Red Hat Enterprise Linux 8 (RHEL8) to Red Hat Enterprise Linux 9 (RHEL9).</li> <li>This affects system libraries, compilers, security tools, and the default environment for all users.</li> <li>A new software stack is being deployed, replacing or upgrading many applications and modules.</li> </ul> What is included in the new software stack? <ul> <li>Key libraries (e.g., GCC, OpenMPI, Python, R, etc.) and applications have been rebuilt or upgraded for RHEL9 compatibility.</li> <li>Software compiled under RHEL8 will still be available to use at your own risk by loading the older version of foss or Intel modules However, the default path will point to the software compiled under RHEL 9. If users intend to use the older version of the software, the path will be shared upon request.</li> <li>Up-to-date documentation and module lists will be provided\u2014check the cluster\u2019s software page or module system for specifics.</li> <li>The default toolchain for newer RHEL 9 applications is <code>foss/2025a</code> and <code>foss/2024a</code> </li> </ul> How do I access the new modules or software? <ul> <li>Use the <code>module avail</code> and <code>module load</code> commands to explore new and updated software on RHEL9.</li> <li>Some modules or software stacks may have different names, versions, or paths\u2014review documentation.</li> </ul> How will the upgrade affect my jobs, software, and workflows? <ul> <li>Jobs started on RHEL8 nodes may not continue without modification on RHEL9 nodes, especially if they rely on system libraries or modules that have changed.</li> <li>Paths, environment variables, or application behavior may differ. Review/retest your scripts.</li> <li>Newer versions of libraries and tools may impact software compatibility and performance.</li> <li>If you were using software installed by the admins and available via modules, those software versions might have changed, as we have built new versions of the software against the updated compilers.</li> <li>Users are advised to create or modify their input scripts based on the new version of the software, if required. However, if you intend to use the old version of the software (compiled under RHEL 8), please contact us and we will share the path..</li> <li>Please note that we will not provide any support for the old versions of the software, as they have not been tested. Use them at your own risk. We will only provide support for the new software that was built with the updated compilers for RHEL 9.</li> </ul> How can I use software that I built or compiled on RHEL8? <ul> <li>Direct execution is not recommended: Binaries or environments built for RHEL8 may run into compatibility issues on RHEL9 (missing libraries, ABI mismatches).</li> <li>Best Practice: Rebuild or reinstall your software on RHEL9 when possible.</li> <li>If you have software installed for your group, complete this form to request access to the test cluster.</li> <li>For critical cases:<ul> <li>Containers: Use Singularity/Apptainer to encapsulate your RHEL8 application and its dependencies.</li> <li>Compatibility Libraries: If available, modules with compatibility libraries for RHEL8 runtime can be loaded (contact support for availability).</li> <li>Long-running jobs: Jobs checkpointed under RHEL8 should ideally resume on RHEL8, or require validation/testing if resuming on RHEL9.</li> </ul> </li> </ul> What about jobs that were running for a long time and need to restart after upgrade? <ul> <li>Checkpoint/Restart: If using application-level checkpointing, confirm that your checkpoint files are forward-compatible with the new software/libraries.</li> <li>Software upgrades may invalidate previous job states or results. Test recovery procedures in a development environment before production.</li> <li>For critical, long-running simulations or legacy applications dependent on the RHEL8 stack, consult with the cluster support team about feasible solutions like containers or legacy compatibility modules.</li> </ul>"},{"location":"faq/rhel_9_faq/#quick-tips-for-a-smooth-transition","title":"Quick tips for a smooth transition","text":"<ul> <li>Test: Validate scripts and workflows on RHEL9 nodes before large-scale jobs.</li> <li>Rebuild: Prefer fresh builds/reinstalls of your applications and conda environments.</li> <li>Stay Informed: Monitor cluster announcements, documentation, and user forums for updates.</li> </ul>"},{"location":"news/","title":"Cluster Maintenance Updates and News","text":""},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/","title":"Relocation of Lochness to Databank Datacenter","text":"<p>Dear Lochness Users,</p> <p>We hope this message finds you well. We want to inform you of an upcoming significant event regarding our HPC (High-Performance Computing) cluster, lochness.njit.edu. The GITC datacenter is scheduled for demolition on November 1, 2023. In order to maintain the operation of our computing infrastructure, we will be relocating the cluster to the Databank colocation facility in Piscataway.</p>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#key-details","title":"Key Details:","text":"<ul> <li>Cluster Shutdown Date: October 6<sup>th</sup>, 2023 at Noon</li> <li>Anticipated Duration: Up to Seven Days</li> <li>Operational Continuity: After the move the cluster will remain operational until the end of the semester.</li> <li>User Migration: We are actively working on migrating all users to the new cluster, wulver.njit.edu, before the end of the semester.</li> </ul>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#cluster-relocation-details","title":"Cluster Relocation Details:","text":"<p>The scheduled shutdown of the lochness.njit.edu cluster will take place on October 6<sup>th</sup>, 2023. We have estimated that the relocation process will require no longer than five days to complete. During this time, the cluster will not be accessible. We understand the importance of uninterrupted access to computational resources, and we will make every effort to minimize downtime.</p>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#operational-continuity","title":"Operational Continuity:","text":"<p>Rest assured that we are committed to maintaining cluster availability for your research and academic needs. The lochness.njit.edu cluster will remain operational until the end of the current semester. This means that you will have access to its computing power throughout your ongoing projects and coursework.</p>"},{"location":"news/2023/10/06/relocation-of-lochness-to-databank-datacenter/#user-migration","title":"User Migration:","text":"<p>Our team is actively working on the migration process to ensure a smooth transition for all users. We plan to migrate all users to the new cluster, wulver.njit.edu, well before the end of the semester. Detailed instructions and support will be provided to facilitate this transition, and we will keep you updated on the migration progress.</p> <p>We understand that this relocation may raise questions or concerns, and we are here to address them. Please feel free to reach out to hpc@njit.edu you have any specific inquiries or require further information.</p> <p>We appreciate your understanding and cooperation during this transitional period. The relocation of our HPC cluster is aimed at providing you with an improved and more reliable computing environment.</p> <p>Thank you for your ongoing support and contributions to our research community.</p>"},{"location":"news/2023/10/14/wulver-maintenance/","title":"Wulver Maintenance","text":"<p>GPFS Fileset Changes</p> <p>Wulver will be out of service Wed Oct 18<sup>th</sup> between 9:00 am-11:00 am for updates and configuration changes. The maintence will be conducted to fix the <code>stale file handle</code> error on <code>/scratch</code> while accessing files from login node.</p>"},{"location":"news/2023/10/14/wulver-maintenance/#maintencance-plans","title":"Maintencance Plans","text":""},{"location":"news/2023/10/14/wulver-maintenance/#recommendation","title":"Recommendation:","text":"<ul> <li>Each fileset gets it\u2019s own inode namespace</li> <li>Fileset names to automatically inherit pool policies</li> <li>Additional fileset settings for chmod to not conflict with ACLs</li> </ul>"},{"location":"news/2023/10/14/wulver-maintenance/#migration-plan","title":"Migration Plan:","text":"<ul> <li>Create new filesets and link under /mmfs1/Scratch and /mmfs1/Project<ul> <li>New fileset names with sata1-project_xx and nvme1-scratch_xx (no bearing on FS path)</li> <li>New fileset have own inode spaces</li> </ul> </li> <li>Rsync data from old to new location</li> <li>Job outage for final copy and change</li> <li>Final rsyncs<ul> <li>Remove symlink for /mmfs1/scratch and create /mmfs1/scratch</li> <li>Unlink/relink filesets in new location</li> <li>Resolve any links remaining on nodes/images</li> </ul> </li> </ul>"},{"location":"news/2023/10/18/lochness-maintenance-updates/","title":"Lochness Maintenance Updates","text":"<p>Lochness is Back Online!</p> <p>Lochness is mostly back up after being moved to a new facility.  The move required complete disassembly and reassembly of the entire cluster. There are 8 nodes down as they were damaged in the move, repairs forthcoming. Infiniband network issues affect 50 nodes, these are in \"drain\" state. Currently 120 nodes are fully functional. You can use <code>sinfo</code> to see the exact states of nodes accessible to you. Please email hpc@njit.edu for assistance.</p>"},{"location":"news/2024/01/22/wulver-maintenance/","title":"Wulver Maintenance","text":"<p>Wulver Monthly Maintenance</p> <p>Beginning Feb 1, 2024, ARCS HPC will be instituting a monthly maintenance downtime on all HPC systems on the second Tuesday from 9AM - 9PM. Wulver and the associated GPFS storage will be taken out of service for maintenance, repairs, patches and upgrades. During the maintenance downtime, logins will be disabled, users will not have access to their stored data in <code>/project</code>, <code>/home</code> and <code>/scratch</code>. All jobs that do not end before 9AM will be held by the scheduler until the downtime is complete and the systems are returned to service.</p> <p>We anticipate maintenance to be completed by the scheduled time. However, occasionally the maintenance may be completed earlier than scheduled or could be extended to the following days. A notification will be sent to the user mailing list when the systems are returned to service or the maintenance window is extended. Additionally, users will encounter the cluster service information upon logging in to Wulver during maintenance. Please pay attention to the Message of the Day when logging in, as it will serve as a reminder for upcoming downtimes or other crucial cluster-related information. Users should take into account the maintenance window when scheduling jobs and developing plans to meet various deadlines. Please do not contact the help desk, HPC staff or open SNOW tickets for access to the cluster or data during the maintenance downtime.</p>"},{"location":"news/2024/06/18/hpc-summer-events/","title":"HPC Summer Events","text":"<p>ARCS HPC invites you to our upcoming events. Please register for the events you plan to attend.</p>"},{"location":"news/2024/06/18/hpc-summer-events/#nvidia-workshop-fundamentals-of-accelerated-data-science","title":"NVIDIA Workshop \u2014 Fundamentals of Accelerated Data Science","text":"<p>Save the Date</p> <ul> <li>Date: July 15, 2024</li> <li>Location: GITC 3700</li> <li>Time: 9 a.m. - 5 p.m.</li> </ul> <p>Learn to use GPU-accelerated resources to analyze data. This is an intermediate level workshop that is intended for those who have some familiarity with Python, especially NumPy and SciPy libraries. See more detail about the workshop here.</p> <p>Registration</p>"},{"location":"news/2024/06/18/hpc-summer-events/#hpc-research-symposium","title":"HPC Research Symposium","text":"<p>Save the Date</p> <ul> <li>Date: July 16, 2024</li> <li>Location: Student Center Atrium</li> <li>Time: 9 a.m. - 5 p.m.</li> </ul> <p>This past year has been transformative for HPC Research at NJIT. The introduction of our new shared HPC cluster, Wulver, has expanded our computational capacity and made the research into vital areas more accessible to our faculty. Please join us to highlight the work of researchers using the HPC resources and connect with the NJIT HPC community. </p> <p>Please register for the symposium here, you can also sign up to present your HPC research as a lightning talk or poster presentation:  </p>"},{"location":"news/2024/06/18/hpc-summer-events/#slurm-workload-manager-workshop","title":"SLURM Workload Manager Workshop","text":"<p>Save the Date</p> <ul> <li>Date: August 13 &amp; 14, 2024</li> <li>Location: GITC 3700</li> <li>Time: 9 a.m. - 5 p.m.</li> </ul> <p>This immersive 2-day experience will take you through comprehensive technical scenarios with lectures, demos, and workshop lab environments. The Slurm trainer will assist in identifying commonalities between previously used resources and schedulers, offering increased understanding and adoption of SLURM job scheduling, resource management, and troubleshooting techniques. </p> <p>Registration is now closed.</p>"},{"location":"news/2024/09/11/hpc-fall-events/","title":"HPC Fall Events","text":"<p>ARCS HPC invites you to our upcoming events. Please register for the events you plan to attend.</p>"},{"location":"news/2024/09/11/hpc-fall-events/#slurm-batch-system-basics","title":"SLURM Batch System Basics","text":"<p>Save the Date</p> <ul> <li>Date: Sep 18<sup>th</sup>, 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul> <p>Join us for an informative webinar designed to introduce researchers, scientists, and HPC users to the fundamentals of the SLURM (Simple Linux Utility for Resource Management) workload manager. This virtual  session will equip you with essential skills to effectively utilize HPC resources through SLURM.</p> <p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"news/2024/09/11/hpc-fall-events/#introduction-to-containers-on-wulver","title":"Introduction to Containers on Wulver","text":"<p>Save the Date</p> <ul> <li>Date: Oct 16<sup>th</sup>, 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul> <p>The HPC training event on using Singularity containers provides participants with a comprehensive introduction to container technology and its advantages in high-performance computing environments. Attendees will learn the fundamentals of Singularity, including installation, basic commands, and workflow, as well as how to create and build containers using definition files and existing Docker images.</p> <p>Registration is now closed. Check the HPC training for the webinar recording and slides. </p>"},{"location":"news/2024/09/11/hpc-fall-events/#job-arrays-and-advanced-submission-techniques-for-hpc","title":"Job Arrays and Advanced Submission Techniques for HPC","text":"<p>Save the Date</p> <ul> <li>Date: Nov 20<sup>th</sup>, 2024</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul> <p>Elevate your High-Performance Computing skills with our advanced SLURM webinar! This session is designed for HPC users who are familiar with basic SLURM commands and are ready to dive into more sophisticated job management techniques.</p> <p>Registration is now closed. Check the HPC training for the webinar recording and slides.</p>"},{"location":"news/2025/01/14/hpc-2025-spring-events/","title":"HPC 2025 Spring Events","text":"<p>ARCS HPC invites you to our upcoming events. Please register for the events you plan to attend.</p>"},{"location":"news/2025/01/14/hpc-2025-spring-events/#introduction-to-wulver-getting-started","title":"Introduction to Wulver: Getting Started","text":"<p>Save the Date</p> <ul> <li>Date: Jan 22<sup>nd</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul> <p>Join us for an informative webinar designed to introduce NJIT's HPC environment, Wulver. This virtual session will provide essential information about the Wulver cluster, how to get an account, and allocation details.</p> <p>Registration is now closed.</p>"},{"location":"news/2025/01/14/hpc-2025-spring-events/#introduction-to-wulver-accessing-system-running-jobs","title":"Introduction to Wulver: Accessing System &amp; Running Jobs","text":"<p>Save the Date</p> <ul> <li>Date: Jan 29<sup>th</sup> 2025</li> <li>Location: Virtual</li> <li>Time: 2:30 PM - 3:30 PM</li> </ul> <p>This HPC training event focuses on providing the fundamentals of SLURM (Simple Linux Utility for Resource Management), a workload manager. This virtual session will equip you with the essential skills needed to effectively utilize HPC resources using SLURM.</p> <p>Registration is now closed.</p>"},{"location":"news/2025/07/29/wulver-maintenance/","title":"Wulver Maintenance","text":"<p>Wulver will be out of service on Tuesday, September 9<sup>th</sup>, for OS and SLURM updates.</p>"},{"location":"news/2025/07/29/wulver-maintenance/#maintenance-plans","title":"Maintenance Plans","text":"<ul> <li>Upgrade the OS: Upgrade from RHEL 8 to RHEL 9: This will resolve the glibc error users encounter when compiling the latest packages. For details, see FAQ.</li> <li>Upgrade SLURM version.</li> <li>Implement new SU calculation: This will allow users to consume fewer SUs when using a single GPU instead of all 4 GPUs on a node.</li> <li>Implement MIG.</li> <li>Add Lochness nodes for course-related usage.</li> </ul>"},{"location":"news/2025/08/22/wulver-outage/","title":"Wulver Outage","text":"<p>As part of NJIT's migration to a new Virtual Machine (VM) platform, Nutanix from (now ultra expensive) VMWare, Wulver will undergo an unplanned but required downtime to migrate critical virtual infrastructure hosting head, login, Open OnDemand and Slurm nodes starting at 8:00 AM on Friday, August 29. The cluster will be unavailable until all migration work is completed.</p> <ul> <li>Expected duration: Up to 12 hours (work may finish sooner)</li> <li>Reason: Migration to the Nutanix VM platform</li> </ul>"},{"location":"news/2025/08/22/wulver-outage/#important-information","title":"Important Information:","text":"<ul> <li>Any jobs submitted before the outage that would not finish in time will be held in the queue and will resume after the cluster is back online. Please plan your usage and submissions accordingly.</li> <li>There is a minor risk that queued jobs will be lost during migration. We will monitor this and inform affected users, if necessary.</li> <li>Updates will be provided if there is any change to the expected downtime window.</li> </ul> <p>We apologize for any inconvenience and appreciate your understanding as we make this important upgrade.</p>"},{"location":"news/2025/08/29/mig-gpu-testing-on-wulver-now-available/","title":"MIG GPU Testing on Wulver Now Available","text":"<p>We\u2019re excited to announce that MIG-enabled GPUs are now available on Wulver for workflow testing!</p> <p>We currently have 4 GPUs configured with MIG (Multi-Instance GPU) profiles as follows:</p> <ul> <li>40 GB profile \u2013 1 MIG instance per GPU</li> <li>20 GB profile \u2013 1 MIG instance per GPU</li> <li>10 GB profile \u2013 2 MIG instances per GPU</li> </ul> <p>This effectively allows the 4 GPUs to perform as 16 GPUs of varying RAM sizes. These MIG instances allow you to run multiple workloads in parallel with dedicated GPU resources, improving efficiency for smaller jobs and testing scenarios.</p> Who can use them? <p>All Wulver users are welcome to test their workflows on these new MIG profiles. Using the <code>debug_gpu</code> partition no Service Units (SUs) will be charged for these jobs. Modify your batch scripts to include these directives:</p> <pre><code>#SBATCH --partition=debug_gpu\n#SBATCH --qos=debug\n#SBATCH --gres=gpu:a100_10g:1      # Change to 20g or 40g as needed\n#SBATCH --time=59:00 # Debug_gpu partition has a 12 hour walltime limit\n</code></pre> What should you do? <ul> <li>Read through the MIG documentation on the HPC website.</li> <li>Test your GPU-enabled workflows with these MIG resources.</li> <li>Verify that your job scripts and containers handle MIG devices correctly.</li> <li>Share feedback on performance and any issues you encounter.</li> </ul> <p>This is a testing phase, so configurations may change based on usage and feedback. For more details, check the MIG documentation.</p>"},{"location":"news/2025/10/04/office-hours/","title":"Office Hours","text":"<p>We currently offer drop-in office hours every Tuesday and Friday from 2:00\u20134:00 p.m. Stop by to meet with our student consultants and ask any questions you have about using HPC resources. Whether you\u2019re just getting started or need help with a specific issue, feel free to bring your laptop to walk us through any problems you're facing. There's no need to create a ticket in advance; if follow-up is needed, the student consultants will open a ticket on your behalf, and you'll receive further instructions. </p> <p>Consulting Hours</p> <ul> <li>Date: Every Tuesday and Friday</li> <li>Location: GITC 2404</li> <li>Time: 2:00 PM - 4:00 PM</li> </ul>"},{"location":"news/2026/01/07/office-hours-for-2026-spring/","title":"Office Hours for 2026 Spring","text":"<p>For this spring, we offer drop-in office hours every Monday and Wednesday from 2:00\u20134:00 p.m. Stop by to meet with our student consultants and ask any questions you have about using HPC resources. Whether you\u2019re just getting started or need help with a specific issue, feel free to bring your laptop to walk us through any problems you're facing. There's no need to create a ticket in advance; if follow-up is needed, the student consultants will open a ticket on your behalf, and you'll receive further instructions. </p> <p>Consulting Hours</p> <ul> <li>Date: Every Monday and Wednesday</li> <li>Location: GITC 5302N</li> <li>Time: 2:00 PM - 4:00 PM</li> </ul>"},{"location":"news/archive/2026/","title":"2026","text":""},{"location":"news/archive/2025/","title":"2025","text":""},{"location":"news/archive/2024/","title":"2024","text":""},{"location":"news/archive/2023/","title":"2023","text":""},{"location":"news/page/2/","title":"Cluster Maintenance Updates and News","text":""}]}